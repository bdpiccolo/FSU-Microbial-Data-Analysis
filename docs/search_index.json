[
["index.html", "Florida State University Microbiota Workshop 2019 Introduction to Data Analysis of 16S rRNA Sequencing in R", " Florida State University Microbiota Workshop 2019 Brian Piccolo 2019-09-19 Introduction to Data Analysis of 16S rRNA Sequencing in R "],
["intro.html", "Chapter 1 Introduction 1.1 Types of microbiota data 1.2 Microbial composition data from sequencing technologies 1.3 Statistical analysis of microbial composition data", " Chapter 1 Introduction 1.1 Types of microbiota data Next-generation sequencing has greatly increased our ability to survey the gut microbiota over the past 10-15 years. The most widely used technologies to assess microbial community data are 16S rRNA amplicon sequencing and shotgun metagenomics sequencing. Both have been globaly referred to as “metagenomics”, but we will make sure to differentiate the technologies. 1.1.1 16S rRNA amplicon sequencing Sequences a single gene, the 16S ribosomal RNA (rRNA) gene. The 16S rRNA gene is ubiquitous in the bacterial kingdom, but is highly variable among different types of bacteria. About 80% of bacterial RNA is made up of rRNA, so this technology can identify rare bacteria groups at a high sensitivity (Li, 2015). The advantage of 16S rRNA sequencing is that it is fairly inexpensive, routinely done, and has good sensitivey at the community level. However, there are 2 major limitations associated with this technology: It is not sensitive at species level, and It does not provide any information regarding genetic potential or bacterial functions. 16S rRNA sequencing sometimes gets a bad rap because of these limitations, but there good reasons to use this technology. Again, its much less expensive than shotgun metagenomics and is still very sensitive at what it can read. It is a good option if you have many samples, a limited budget, and if you just want to identify whether there is an alteration in the composition of the microbiota. 1.1.2 Shotgun metagenomic sequencing Sequences all microbial genomic DNA. Provides species level compositional data and bacterial genetic data. The compositional data is exactly similar to 16S rRNA data, but superior because it gives species level depth. However, it is much more expensive on a per sample basis and requires significantly greater computational power to process. 1.2 Microbial composition data from sequencing technologies The overall structure of 16S rRNA and shotgun metagenomic sequencing data are similar. Lets quickly describe how the data is aquired to understand why this is. Note, this workshop is set up for statistical analysis, so we will keep this section at a very, very basic level. For both technologies, bacterial DNA must be isolated from the sample (e.g., feces, intestinal content, skin, soil, water, dog hair, etc.). Once the bacterial DNA is isolated, a subsample of the DNA is extracted for PCR, a subset of the resulting amplicon is pooled into a library, and a subset of the library is then sequenced (Morton et al., 2019). Main difference between the two technologies is the libraries. Again, 16S rRNA libraries are focused on the 16S rRNA gene, while shotgun metagenomics is focused on all bacterial genetic material. In both cases, you end up with raw sequences that have to be mapped to referenced phylogenetic tree. Generally, the sequences are aligned to reference gene until a set of sequences fill a percentage of the gene at a certain threshold. Once this threshold or probability is reached, then these clusters of sequences are assigned to the phylogeny. This is now considered a single “read” in your data. The accumulation of these read counts provide us the data that can then be statistically analyzed. There are some considerations you should consider when working with next-generation microbial sequencing data. It is essential to reach a minimum sampling depth. It is semi-quantitative data, i.e., counts of sequencing reads are not actually measuring a single bacteria. Data is commonly referred to as “relative abundance.” It does not take into consideration the microbial load of each sample. 1.3 Statistical analysis of microbial composition data Microbial composition data is multi-level and high-dimensional. What do I mean by these terms? It is multi-level because it can be analyzed at various taxonomy levels, e.g., species or phylum level. It is high-dimensional because you will likely have hundreds to tens of thousands of variables (i.e., bacterial taxa) to analyze. Spreadsheet or GUI (graphical user interface) based statistical software may be cumbersome or difficult to work with for these reasons. Therefore, it is highly recommended to use language based stastical software, like R, that can be tailored to meet the needs of these factors. R is a freely available statistical language and environment that is built around an extremely large and active user group. In fact, this web-book was created using R. While there are many, many benefits to using R, those without progamming experience may have a steep learning curve. The next chapter will be dedicated to detailing the basic knowledge required to analyze microbial sequencing data in R. "],
["rbasics.html", "Chapter 2 Basics of R 2.1 R 2.2 Downloading R 2.3 The R console 2.4 The R script 2.5 R packages 2.6 Basic operations, data types, missing values, and functions 2.7 Importing data 2.8 Working with your data 2.9 Stats", " Chapter 2 Basics of R 2.1 R R is an object-oriented programming language that was developed for statistical computing and visualization. It is freely available under the GNU General Public License and has a large and active user community. Figure 2.1 describes the most visited languages comparable to the R user base at Stack Overflow the largest question and answer programming site. Note the sustained linear growth since 2012. Figure 2.1: From stackoverflow blog: The Impressive Growth of R. By David Robinson, 10/10/2017. I bolded object-oriented because it highlights a major advantage of reproducibility in R and other programming-based langauge. Everything that is done in R uses named objects as either functions or data. Understanding how functions manipulate data types and data structures is the foundation of working with R. For example, if you want to obtain the mean over a set of numbers, the mean function is used to determine the mean of numbers within a vector. These bolded words and phrases may seem abstract at this point, but this chapter will expand these concepts. This chapter is dedicated to basic concepts needed to complete this course. Googling your question will more often than not provide an answer to your question, but there are several online tutorials for those interested in a deeper understanding of R. Several of these courses are listed below. DataCamp Coursera statistics.com codecademy Many of these excellent courses were not available when I learned R. I extensively used the Quick-R website by Robert Kabacoff as I learned how to use R. I will be providing links periodically to Quick-R for additional information not covered in this chapter. 2.2 Downloading R First step is to downloading R. R is available at the The Comprehensive R Archive Network (CRAN) website. Figure 2.2 shows the website and note the links needed for different systems are in a box at the top of the website. The default downloading prompts will result in aquiring 32 and 64 bit version for Windows users. You will likely be running a 64-bit Windows, but please refer here if you want to check your Windows version. Figure 2.2: CRAN website 2.3 The R console Figure 2.3 shows the R console after opening. Our color scheme differ because I customized my colors, but the information should be very similar. This is the R console and it is where you will type in code or pass code to run, causing R to work. Figure 2.3: R console Let’s try some basic operations. Type in 2 + 2, or copy the inline code (R code within the text) below, paste it into your R console, and press enter. Note, since this web-book was written in R, there will typically be an output box after provided code. # Operations after hashtag will not be processed by R. Good way to write notes! 2 + 2 ## [1] 4 You should see the output is ‘4’ like the output box above this text. Now let’s make a data object! Cut and paste the code below to make an object that contains the results of 2 + 2 # The &#39;&lt;-&#39; is referred to as an assignment operator. Use this when defining the name of an object. object1 &lt;- 2 + 2 object1 ## [1] 4 The output is 4, which is what the object contains. Try the next line of code to verify that ‘object1’ is the number 4. # Output should be 400. object1 * 100 ## [1] 400 The name of the object can be anything you want as long as it doesn’t start with a number or special characters. It can be as short as a single letter or as long as you want. Recommend making object names as short and succinct as possible. R is also case sensitive. Obj2 and obj2 are 2 different objects. Periods (.) and underscores are allowable, but special characters are not. Try to make use of periods or underscores for readability, e.g., Obj1_MEAN. You can add more numbers using the c function, for example. # the c() function means either combine or concatenate. obj2 &lt;- c(5, 6.3, 3, 0, -1, 13, 7) obj2 ## [1] 5.0 6.3 3.0 0.0 -1.0 13.0 7.0 We can now take the mean of obj2 using the mean function. Note, the mean function is a specialized object that performs and action on a data object. We will cover this in more detail. # In almost every case, an object will go inside the function&#39;s parantheses. mean(obj2) ## [1] 4.757143 2.4 The R script So far, we’ve been typing directly into the console. When you close R, you will normally lose everything you’ve done during the session. You can, however, write and save your code on a specialized text file (script). This will allow you to replicate your results every time you open a new R session. There is an intrinsic R script option in your R environment. Click File and then select ‘New Script’. You will see a blank window titled, ‘Untitled - R Editor.’ You can then type R commands in this window. Copy and paste the following code to the R Editor. To run this code, highlight all of the code with your mouse and then press Ctrl+R (hold down control and press the R key on your keyboard). You can also run the script by pressing Ctrl+R line by line. obj3 &lt;- c(5, -1, 3, -10, -2, -12, 15, 0) # The &lt;= means &#39;less than or equal to&#39; # &gt;= means &#39;greater than or equal to&#39; obj3 &lt;= 0 ## [1] FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE A very popular modular script editor is RStudio. RStudio is open-source and free! It nicely packages the R console, script(s), help pages, and graphical outputs in a convenient program (Figure 2.4). Many new users find RStudio extremely helpful. There are other text/script editors, but very few will pass your code directly to the R console like RStudio. 2.5 R packages You have been introduced to a couple functions that appear to be pre-loaded in R. Functions are what really drive R and makes it such a flexible tool. Anyone can write a function, and many, many people have. This is how people contribute to the growth of R, by releasing packages of R functions that perform specific tasks to their fields. In fact, the mean and c functions we used in 2.3 and 2.4 are contained in a preloaded ‘base’ package. Most of the preloaded packages contain core functions and widely used statistical and plotting functions. We will eventually be loading packages specific to ecology and microbial sequencing data, but let’s first learn how to identify and download packages. After you download R, the only packages you have are the core packages. You will have to identify packages you need and download them. The R community has 3 major repositories for R packages: CRAN Bioconductor Github You only have to install a package once; however, downloaded packages do not automatically load. You will have to load these add-on packages using the library function. Additional information about R packages, including installing packages using the R menu, can be viewed here 2.5.1 CRAN CRAN is the major repository of R packages. If you refer to their website (linked in 2.2) and select the ‘Package’ link under the Software header, you will see that there are almost 15,000 R packages available. You can view a table of all 15,000 packages, but this seems incredibly cumbersome way to identify a certain package. Fortunately, CRAN has a Task View option in the side-panel under CRAN. If you click the ‘Task View’ link you will see a list of topics that may be relevant to your field (Figure 2.6). Find ‘Phylogenetics’ and press that link. In this Task View, there are several options with a very brief description of each function’s functionality. We will actually be using functions from the ape and vegan package. Brief description of both can be found here. Most times, you will know exactly which package you need to install, so browsing random packages is not typically done. There are a couple ways to install packages and the easiest way is to use the install.packages function. However, before we actually install a function, let’s talk about Mirrors. Mirrors are host locations that house all of the available CRAN packages. A lot of redundancy, but choosing a Mirror site closest to your location will help with bandwith and latency and allows various locations to absorp the overall user traffic. You will be forced to choose a Mirror when you initially download a package. Will only have to do this once during a single session. Now, let’s download the vegan package. Choose the closest Mirror location when you’re prompted. # The quotations differentiate a &quot;character string&quot; from a data object. # Multiple packages can be downloaded at once, using the c() function. #install.packages(&quot;vegan&quot;) 2.5.2 Bioconductor Bioconductor provides R packages developed specifically for a wide variety of high-throughput data analysis (e.g., -omics data). Like CRAN, it is open source and free to use. It is fairly straight forward to download packages from the Bioconductor website. Each package on Bioconductor has a dedicated website with installation instructions. We will use the phyloseq package as an example (Figure 2.4). The phyloseq package was developed by Paul McMurdie and Susan Holmes and is a very important package for handling microbial sequencing data in R. We will be utilizing this package in subsequent chapters. Figure 2.4: phyloseq website at Bioconductor All you have to do is copy and paste the inline code under the Installation header into your R console and then press Enter. Simple as that. More detailed information can be found at the Bioconductor website. 2.5.3 Github Github is arguably the most popular hosting site for developing and disseminating coding. In regards to R packages, it serves 2 major functions: 1) Many CRAN and Bioconductor packages provide their underlying code at Github, and 2) packages in early stages of develoment are available at Github prior to release on CRAN or Bioconductor. We will not need to download any packages from Github for this course, but it is a valuable resource to consider if you decide to use R in the future. Github always tracks any changes made to files it hosts. So it a great way to track versions of your workflow and greatly helps with transperancy and reproducibility. 2.6 Basic operations, data types, missing values, and functions 2.6.1 Basic operations Although these operators may seem fairly low level, they are extremely important for coding. Here is a table containing basic arithmetic operators. Table 2.1: Arithmetic Operators Operator Use + addition - subtract * multiply / division ^ exponential Another table containing logical operators. Logical operators will return TRUE or FALSE. Table 2.2: Logical Operators Operator Use &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greather than or equal to == exactly equal to != not equal to R has some redundancy issues with special characters that can be quite confusing. The assignment operator, &lt;-, was shown in 2.3. Looks very close to the less than logical operators &lt; or &lt;=. You may also ask, why not just use = to designate equal to rather than ==. The single equal has multiple functions in R, 1) it is also an assignment operator like &lt;- and 2) it assigns arguments (parameters) in functions. Please see below # These will result in the same output. arrowAssign &lt;- 100/4 equalAssign = 100/4 ## We can check to see if they are equal using &#39;==&#39; arrowAssign == equalAssign ## [1] TRUE 2.6.2 Data types It is important to understand how R handles different data types. For example, how R handles categorical data that could be character strings (e.g., Obese or Lean), numerical data (e.g., 5, 8, 10567156.2, -400), etc. Most of this should be fairly straight forward, but there are some nuances to be aware of. numeric: Can get very technical, but generally any real number including decimals. integer: Will appear if all data are whole numbers. R will automatically determine this designation, primarily when importing data. Operations on numeric and integer data are similar. character: Generally, any non-numeric or non-logical values (programmers call these, “string values”). Default setting for data that R cannot determine as numbers/logic. Can almost force anything to be a character. character values will always be surrounded by quotations. factor: This type of data looks like a character, but is technically a special type of integer. It has a special attribute that assigns integers to a corresponding set of characters. The underlying integer assignment allows R to order the characters in plots, in addition to some other special features. Widely used in R, but can be confusing and lead to some odd outputs if not aware of their presence. logic: Either TRUE or FALSE. Very, very important. Many of the most basic functionality of R is based on logical operators. 2.6.3 Data structure R has various ways that it structures similar and different data types. The most basic of which is called a vector. 2.6.3.1 Vector A vector is a single value or combination of values within a particular data type. You create vectors using the c function, introduced in 2.3. Here are examples of each data type. # Example of a vector with a single value singlevalue_Vector &lt;- 50 singlevalue_Vector ## [1] 50 # Example of a numeric vector with a combination of values numeric_Vector &lt;- c(1.5, 20, -5348964.165141, 6) numeric_Vector ## [1] 1.5 20.0 -5348964.2 6.0 # Example of a character vector with a combination of values character_Vector &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;character string&quot;, &quot;FSU workshop 10-9-2019&quot;, &quot;bdpiccolo@gmail.com&quot;) character_Vector ## [1] &quot;cat&quot; &quot;dog&quot; ## [3] &quot;character string&quot; &quot;FSU workshop 10-9-2019&quot; ## [5] &quot;bdpiccolo@gmail.com&quot; # Example of a logical vector with a combination of values logical_Vector &lt;- c(TRUE, TRUE, FALSE, FALSE, TRUE, FALSE) logical_Vector ## [1] TRUE TRUE FALSE FALSE TRUE FALSE # Example of a logical vector with a combination of values factor_Vector &lt;- as.factor(c(&quot;Control&quot;, &quot;HighFat&quot;, &quot;Control&quot;, &quot;HighFat&quot;, &quot;Control&quot;, &quot;HighFat&quot;)) factor_Vector ## [1] Control HighFat Control HighFat Control HighFat ## Levels: Control HighFat Note the differences between the character and factor outputs. The characters are surrounded by quotations, but the factors are not. In addition, the factor output has a level attribute that shows the unique characters within the vector. 2.6.3.2 Matrix A matrix is a 2-dimensional vector. Since vectors can only contain a single data type, a matrix can then only contain a single data type. Here is an example of a matrix. # Example of a matrix with 2 rows and 3 columns matrix_example1 &lt;- matrix(data = c(2,5,7,3,7,3), nrow = 2, ncol = 3) matrix_example1 ## [,1] [,2] [,3] ## [1,] 2 7 7 ## [2,] 5 3 3 2.6.3.3 List So far, we’ve only been introduced to data structures that can handle a single data type. Lists are a way that you can package different data types in a single object. We will combine each of the vectors we created in 2.6.3.1. # Example of a list with 5 elements list_example &lt;- list(singlevalue_Vector, numeric_Vector, character_Vector, logical_Vector, factor_Vector) list_example ## [[1]] ## [1] 50 ## ## [[2]] ## [1] 1.5 20.0 -5348964.2 6.0 ## ## [[3]] ## [1] &quot;cat&quot; &quot;dog&quot; ## [3] &quot;character string&quot; &quot;FSU workshop 10-9-2019&quot; ## [5] &quot;bdpiccolo@gmail.com&quot; ## ## [[4]] ## [1] TRUE TRUE FALSE FALSE TRUE FALSE ## ## [[5]] ## [1] Control HighFat Control HighFat Control HighFat ## Levels: Control HighFat Note the double brackets above each element within the list. This designates the position of each element within the list. Each element acts independently of the other elements, which allows a list to hold different data types within a list. There is no limit to the number of elements that you can define. 2.6.3.4 Data frame A data frame will look very much like a matrix in your console, but is technically a constrained list. Each element within a data frame must be a vector of equal length. This allows it to have a 2-dimensional shape with different data types. This is the most common data structure you will find when uploading a .csv file. Here is an example of a data frame. # Example of a data frame with 3 columns and 4 rows. # First create vectors num_vec &lt;- c(1, 2.234, 0, 34) num_vec_2 &lt;- c(1000, 100, 10, 1) char_vec &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;mouse&quot;, &quot;cow&quot;) # Create data frame dataframe_example &lt;- data.frame(Num1 = num_vec, Num2 = num_vec_2, Char1 = char_vec) dataframe_example ## Num1 Num2 Char1 ## 1 1.000 1000 cat ## 2 2.234 100 dog ## 3 0.000 10 mouse ## 4 34.000 1 cow Note that the column names are labeled in the data frame. It is actually required that column and row names are labeled. You can access these names by using the colnames and rownames function. You can also label the rows and columns of matrices, but it is not required. 2.6.4 Missing values R handles missing values by replacing the missing value with a NA, no matter the data type. R does not know what to do with missing values because it assumes it could be anything. # Example of vectors with NA num_vec_withNA &lt;- c(1, 2.234, 0, NA, 5) num_vec_withNA ## [1] 1.000 2.234 0.000 NA 5.000 char_vec_withNA &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;mouse&quot;, &quot;cow&quot;, NA) char_vec_withNA ## [1] &quot;cat&quot; &quot;dog&quot; &quot;mouse&quot; &quot;cow&quot; NA Sometimes, package authors have accounted for NAs in their coding, but generally, you need to account for NA values in core functions. # mean() function will not work with an NA mean(num_vec_withNA) ## [1] NA Note that the output is also NA! That is because R does not know what to do with the missing value, so the function is undefined. Can correct this by using the ‘na.rm’ argument in the mean function. # This is not a standard term, so each function will have its own terminology. mean(num_vec_withNA, na.rm=TRUE) ## [1] 2.0585 More information can be found here 2.6.5 Functions This can be considered a more intermediate to advanced topic. You will come across some user-defined functions in this workshop, so it is good to understand some of the concepts. Another reason for R’s flexibility is that the user can create functions that can help and/or streamline their analyses. We will need to define our own functions for microbial sequencing analysis, so it will be advantages to go over some key concepts regarding functions. There are 4 main components of a function as shown below. Rfunction &lt;- # Name assignment function(arg1, arg2) { # Defining arguments for function inputs sum_args &lt;- arg1 + arg2 # Body of function with coding statements using defined arguments. return(sum_args) # Return object } 2.6.5.1 Name assignment This is what you name the function. Make sure it is a unique name. R will mask functions from other packages with the same names. If we named this function ‘mean’ it would mask the mean function from the ‘basic’ package. You can force a function from a specific package by using the coding below. This can be used if a function is masked or if you do not want to load an entire package. # You will not be able to import your data unless you set your working directory! base::mean(num_vec) It is not always necessary to name a function. This would occur in very specific functions that have an argument to use a user-defined function. Think of a situation where you want to identify the median over every column in a data frame (e.g., every genera in microbial sequencing data). There is a function that specifically runs a defined function over columns within a data frame, called sapply. Now let’s say you want to define your own function across these columns, you could set your own defined function, minus the name, within the fucntion argument within sapply. An example will be provided when we discuss loops and loop related functions. 2.6.5.2 Arguments Think of arguments as objects that will be passed to the coding within the body of the function. They can be made to accept any input or force you to select a defined set of parameters. We will only be working with the former type of arguments. Look how we place an input in the function created above. Rfunction(arg1 = 10, arg2 = 50) You don’t have to assign the arguments directly, but R will automatically try to order non-defined arguments. Arguments do not need to be defined in order. # Here, R will assign 10 to the first non-defined argument, arg1 Rfunction(10, arg2 = 50) # Here, R will assign 10 to the first non-defined argument, arg1 Rfunction(arg2 = 500, 10) # Here, R will assign 500 to the first non-defined argument, arg1; # then, R will define 10 to the next non-defined argument, arg2. Rfunction(500, 10) 2.6.5.3 Body The body of the function is what give the functionality to a function. It takes the definition of the argument, passes it to the coding in the body, then computes the coding operation. In this case, it takes the arg1 and arg2, sums them, and assigns the summation to an object. An interesting aspect of a function is that the body of the function does not interact with the R session. Meaning, that objects that you define in a function will only be remembered within the function and not outside it, so they will not affect your analysis. Objects within your analysis environment will be recognized in the body of a function though! Note that the body is nested within curly brackets, { and }. These define the body of the function and allow functions to contain more than a single line of code. You do not need to use curly brackets if you have a single line of code. 2.6.5.4 Return This defines the function’s output. Can be a single output or multiple outputs. A list is typically used if there are multiple objects that are being returned. Let’s make another function based on the one above, but use a list to output multiple objects. In this case, we will see that the arguments contain the inputted numbers. Rfunction_list &lt;- # Name assignment function(arg1, arg2) { # Defining arguments for function inputs sum_args &lt;- arg1 + arg2 # Body of function with coding statements using defined arguments. return(list(sum_args, arg1, arg2)) # Return object } Now let’s see the output. Rfunction_list(500, 1) ## [[1]] ## [1] 501 ## ## [[2]] ## [1] 500 ## ## [[3]] ## [1] 1 Note that the first element in the list is the summation of 500 and 1, the second element is the input of 500, and the third element is the input of 1. W It is good habit to define the output using the return function; however, it is not necessary. If you do not use return, then the last line of code will be the output. A single line function will sometimes be very minimal, for example # Function to determine squared residuals # The first line of code in the body is the last line too! residualFUN &lt;- function(x) (x/mean(x))^2 # Create data using the rnorm function normalDATA &lt;- rnorm(20, mean=100, sd=10) # Output list(normalDATA, residualFUN(normalDATA)) ## [[1]] ## [1] 92.37514 95.92941 96.00827 95.61768 107.10486 90.41717 87.29642 ## [8] 87.49584 96.59541 101.28829 86.55709 109.39325 102.91839 95.64197 ## [15] 104.32695 92.47921 92.87841 102.60314 98.44410 100.15962 ## ## [[2]] ## [1] 0.9111085 0.9825698 0.9841859 0.9761944 1.2248368 0.8728945 0.8136783 ## [8] 0.8174001 0.9962605 1.0954142 0.7999544 1.2777355 1.1309564 0.9766905 ## [15] 1.1621253 0.9131625 0.9210632 1.1240386 1.0347592 1.0711374 The first element of the list is the output of rnorm and the second element in the list is the output of residualFUN function. Look how flexible a list is! It allowed a function to define its second element! 2.7 Importing data R can handle a wide range of file types. For example, we will import a biome file later in this workshop. However, spreadsheet structured data is most commonly imported as a text delimited or csv file. More recent developed packages have have made it easier to import directly from an Excel file. However, we need to understand what a working directory is before we can demostrate importing files. 2.7.1 Working directory Quite simply, you have to tell R where to locate the file you want to import. To do this, you have to specify the directory where your files are located at. You can do that by choosing ‘File - Change dir’ and then selecting the directory with your files. Or you can use the setwd function. For example, # You will not be able to import your data unless you set your working directory! setwd(&quot;C:\\\\users\\\\brian\\\\documents\\\\FSU Workshop\\\\&quot;) 2.7.2 Comma delimited text file (csv) For our purposes, the read.csv function will suffice. There are other functions that can # The head() function will show the first 6 rows of a data frame. CSVimport &lt;- read.csv(&quot;CSVexample.csv&quot;) head(CSVimport) ## Subject Treatment Gender Age Height Wt BMI Glucose ## 1 579 Control F 38 63.00 82.27 32.1 99 ## 2 586 Control F 37 62.00 94.20 37.9 108 ## 3 502 Control M 62 68.60 112.50 37.1 97 ## 4 508 Control F 49 64.25 107.70 40.4 86 ## 5 569 Control F 46 63.80 80.91 29.9 102 ## 6 568 Control M 47 69.25 127.23 41.0 104 Use the read.table function to import a space delimited text file (.txt) 2.7.3 Excel file There are now several packages that provide functions to import Excel files. We will use the read_xlsx function from the readxl package. There are many arguments that can be inputted in this function, but we will focus on the first two. The ‘path’ argument defines the file and the ‘sheet’ argument defines the the sheet name to be read. If the ‘sheet’ argument is not defined, then the function will read the first sheet. # Need to load the package. library(readxl) XLSXimport &lt;- read_xlsx(&quot;XLSXexample.xlsx&quot;) XLSXimport ## # A tibble: 22 x 8 ## Subject Treatment Gender Age Height Wt BMI Glucose ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 579 Control F 38 63 82.3 32.1 99 ## 2 586 Control F 37 62 94.2 37.9 108 ## 3 502 Control M 62 68.6 112. 37.1 97 ## 4 508 Control F 49 64.2 108. 40.4 86 ## 5 569 Control F 46 63.8 80.9 29.9 102 ## 6 568 Control M 47 69.2 127. 41 104 ## 7 556 Control F 35 60.2 83.3 36 100 ## 8 581 Control F 49 64.5 92.1 34.6 126 ## 9 577 Control F 45 63 94.1 36.2 90 ## 10 516 Control F 25 68 94.7 31.5 87 ## # ... with 12 more rows Notice that the difference between this output and the csv output. The Excel output says that it is a tibble. This is a newer data structure based on a family of packages globally referred to as the tidyverse. We will not have time to learn the tidyverse syntax, but it is a highly popular package and and area of active development. We can coerce the tibble into a regular data frame, similar to the csv import. # You can also coerce data types into different data types. XLSXimport_df &lt;- as.data.frame(XLSXimport) head(XLSXimport_df) ## Subject Treatment Gender Age Height Wt BMI Glucose ## 1 579 Control F 38 63.00 82.27 32.1 99 ## 2 586 Control F 37 62.00 94.20 37.9 108 ## 3 502 Control M 62 68.60 112.50 37.1 97 ## 4 508 Control F 49 64.25 107.70 40.4 86 ## 5 569 Control F 46 63.80 80.91 29.9 102 ## 6 568 Control M 47 69.25 127.23 41.0 104 2.8 Working with your data We will now go over some basic data wrangling concepts and perform some basic univariate statistics on our imported data. 2.8.1 Viewing your data Its always a good idea to see what your data looks like after you import it. We used head earlier to see the first 6 rows # You can use the &#39;n&#39; argument to set how many rows are returned. head(CSVimport) ## Subject Treatment Gender Age Height Wt BMI Glucose ## 1 579 Control F 38 63.00 82.27 32.1 99 ## 2 586 Control F 37 62.00 94.20 37.9 108 ## 3 502 Control M 62 68.60 112.50 37.1 97 ## 4 508 Control F 49 64.25 107.70 40.4 86 ## 5 569 Control F 46 63.80 80.91 29.9 102 ## 6 568 Control M 47 69.25 127.23 41.0 104 There are columns that have characters and others that have numbers. That would indicate that we have a data frame. We can look at the structure of the object using the str function. # You can use the &#39;n&#39; argument to set how many rows are returned. str(CSVimport) ## &#39;data.frame&#39;: 22 obs. of 8 variables: ## $ Subject : int 579 586 502 508 569 568 556 581 577 516 ... ## $ Treatment: Factor w/ 2 levels &quot;Control&quot;,&quot;Experiment&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Gender : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 2 1 1 2 1 1 1 1 ... ## $ Age : int 38 37 62 49 46 47 35 49 45 25 ... ## $ Height : num 63 62 68.6 64.2 63.8 ... ## $ Wt : num 82.3 94.2 112.5 107.7 80.9 ... ## $ BMI : num 32.1 37.9 37.1 40.4 29.9 41 36 34.6 36.2 31.5 ... ## $ Glucose : int 99 108 97 86 102 104 100 126 90 87 ... Note that this return tells us that the object is indeed a data frame. It also shows us that R assigned the columns with strings as factors and also defined the columns with numbers as either integer or numeric. 2.8.2 Extracting elements Square brackets [ and ] are the most basic way to extract elements from various data structures. The most basic way is to define the position within a data structure. Let’s look at the ‘num_vec’ object from 2.6.3.4. There are 4 elements within the vector, 1, 2.234, 0, and 34. Let’s say we want to extract the 0 from this vector. We can use square brackets to extract this position. # Here is the original vector num_vec ## [1] 1.000 2.234 0.000 34.000 # 0 is the third element. num_vec[3] ## [1] 0 Now let’s say we want to extract the 2nd through the 4th element. We can use : if numbers are sequential. # Will work as long as the numbers are sequential and whole numbers num_vec[2:4] ## [1] 2.234 0.000 34.000 What if we want to extract elements 1 and 3? We can use the c function and define positions out of order. # There are multiple ways to do this. num_vec[c(1,3)] ## [1] 1 0 # This would also work, why? positions &lt;- c(1,3) num_vec[positions] # or num_vec[c(3,1)] We can also use logical statements to extract elements. # Under the hood, R is keeping the positions that are TRUE. num_vec[num_vec &gt; 2] ## [1] 2.234 34.000 Let’s look at a list output to see what is going on here. # Under the hood, R is keeping the positions that are TRUE. list(num_vec, num_vec &gt; 2) ## [[1]] ## [1] 1.000 2.234 0.000 34.000 ## ## [[2]] ## [1] FALSE TRUE FALSE TRUE The TRUE statements are equivelant to telling R which position to return. You have to be very careful when using positional bracketing. Square brackets can also work with 2-dimensional data structures. With matrices and data frames, a comma is added to differentiate rows and columns. Let’s look at the ‘matrix_object’ from 2.6.3.2. # Here is the original matrix matrix_example1 ## [,1] [,2] [,3] ## [1,] 2 7 7 ## [2,] 5 3 3 # Statements to the right of the comma refer to column position matrix_example1[,2:3] ## [,1] [,2] ## [1,] 7 7 ## [2,] 3 3 # Statements to the left of the comma refer to column position matrix_example1[1,] ## [1] 2 7 7 We can designate positions for both columns and rows. Let’s use the data frame from 2.6.3.4 # Here is the original matrix dataframe_example ## Num1 Num2 Char1 ## 1 1.000 1000 cat ## 2 2.234 100 dog ## 3 0.000 10 mouse ## 4 34.000 1 cow # can embed functions as shown above. dataframe_example[1:3,c(3,1)] ## Char1 Num1 ## 1 cat 1.000 ## 2 dog 2.234 ## 3 mouse 0.000 Note, that you can re-order the columns and rows using positional bracketing. Double brackets [[ and ]] are used with lists and data frames and are typically used to extract a single element by position or name. A the dollar sign, $, has the same functionality. For example, # These are all equivalent dataframe_example[[2]] ## [1] 1000 100 10 1 dataframe_example[[&quot;Num2&quot;]] ## [1] 1000 100 10 1 dataframe_example$Num2 ## [1] 1000 100 10 1 The $ will generally only work if the label has no spaces or special characters. Otherwise, use the double brackets with a character string. 2.8.3 Subsetting We can see from the str output in 2.8.1 that the ‘Treatment’ column has two factor levels, “Treatment” and “Control”. Let’s say we want to extract the data related to the Control group and make a new object with it. We could determine what row positions within the Treatment column contain the string “Control.” Fortunately, this dataset has them nicely ordered, but what if they’re not nicely ordered. Or what if you have 20,000 rows. A better way to do this is to use logical operators. Let see what happens when we use == on the ‘Treatment’ column. # &#39;==&#39; will ask if the vector is exact to something. CSVimport$Treatment == &quot;Control&quot; ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE It returns a logical vector of equal length to CSVimport$Treatment where TRUE replaces “Control” and FALSE replaces “Treatment”. We can use this with positional bracketing to subset the data. # Data structure, followed by brackets. # For 2-dim, rows left of comma, columns right of comma. CSV_ctrl &lt;- CSVimport[CSVimport$Treatment == &quot;Control&quot;, ] CSV_ctrl ## Subject Treatment Gender Age Height Wt BMI Glucose ## 1 579 Control F 38 63.00 82.27 32.1 99 ## 2 586 Control F 37 62.00 94.20 37.9 108 ## 3 502 Control M 62 68.60 112.50 37.1 97 ## 4 508 Control F 49 64.25 107.70 40.4 86 ## 5 569 Control F 46 63.80 80.91 29.9 102 ## 6 568 Control M 47 69.25 127.23 41.0 104 ## 7 556 Control F 35 60.15 83.32 36.0 100 ## 8 581 Control F 49 64.50 92.09 34.6 126 ## 9 577 Control F 45 63.00 94.09 36.2 90 ## 10 516 Control F 25 68.00 94.68 31.5 87 ## 11 565 Control F 34 61.50 108.18 44.3 88 Now, what if we have a vector containing Subject numbers that we want to filter on. Let’s see what happens when we use ==. # We can make an object containing the subject IDs first. ID_filter &lt;- c(&quot;579&quot;,&quot;586&quot;,&quot;502&quot;,&quot;508&quot;,&quot;569&quot;,&quot;568&quot;,&quot;556&quot;) CSVimport$Subject == ID_filter ## Warning in CSVimport$Subject == ID_filter: longer object length is not a ## multiple of shorter object length A weird error returns. I will not get into the reason for the error, but there is a more optimal way to do this using the match operator, %in%. The %in% operator will return TRUE if anything on the left matches on the right. Let’s see it in action. # Use &#39;==&#39; if you&#39;re matching on a single element # use &#39;%in% if you&#39;re filtering on more than 1 element. CSVimport$Subject %in% ID_filter ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE ## [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # Now subset the data CSV_IDfilter &lt;- CSVimport[CSVimport$Subject %in% ID_filter, ] CSV_IDfilter ## Subject Treatment Gender Age Height Wt BMI Glucose ## 1 579 Control F 38 63.00 82.27 32.1 99 ## 2 586 Control F 37 62.00 94.20 37.9 108 ## 3 502 Control M 62 68.60 112.50 37.1 97 ## 4 508 Control F 49 64.25 107.70 40.4 86 ## 5 569 Control F 46 63.80 80.91 29.9 102 ## 6 568 Control M 47 69.25 127.23 41.0 104 ## 7 556 Control F 35 60.15 83.32 36.0 100 Finally, let’s filter ‘Experiment’ subjects that are less than 40 years old. We also only want the ‘Age’, ‘Wt’, ‘BMI’, and ‘Glucose’ columns. # Use &#39;&amp;&#39; to evaluate logical combiners. There are other specialized logical combiners. CSV_exp40 &lt;- CSVimport[CSVimport$Treatment == &quot;Experiment&quot; &amp; CSVimport$Age &lt; 40, ] # Create vector containing the column names that we want to filter. # Remember, R is case sensitive. column_filter &lt;- c(&quot;Age&quot;, &quot;Wt&quot;, &quot;BMI&quot;, &quot;Glucose&quot;) CSV_exp40_filter &lt;- CSV_exp40[ ,colnames(CSV_exp40) %in% column_filter] CSV_exp40_filter ## Age Wt BMI Glucose ## 12 37 95.36 34.2 101 ## 15 18 105.70 36.8 99 ## 17 37 99.80 35.4 97 ## 18 39 86.50 37.8 99 ## 19 29 139.40 43.1 104 ## 22 31 109.82 38.4 99 There are many other ways to filter and subset your data. There is actually a subset function that does the same things we covered here, but within a function. tidyverse has a more linear approach to filtering, which has become extremely popular. Some workflows are better if you have millions of data points, but this way minimizes the need for add on functions. 2.8.4 Reshaping data This can be considered a more intermediate to advanced topic. Well, I told myself that I was not going to introduce functions from the tidyverse, but the reshaping functions are SO VERY convenient for high-throughput data analysis. Before we get into these functions, let me introduce you to the t (transpose) function first. # Will make a matrix first using the rnorm() function. randomDATA &lt;- rnorm(n = 18, mean = 10, sd = 2) Matrix_example2 &lt;- matrix(randomDATA, ncol=6, nrow=3) Matrix_example2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 7.773096 12.273325 11.13885 10.10186 9.349558 8.192193 ## [2,] 14.218296 11.114960 13.72156 11.15117 11.562190 9.955560 ## [3,] 6.833486 7.703311 11.77362 12.35612 11.185806 10.314718 # Transpose t(Matrix_example2) ## [,1] [,2] [,3] ## [1,] 7.773096 14.21830 6.833486 ## [2,] 12.273325 11.11496 7.703311 ## [3,] 11.138851 13.72156 11.773624 ## [4,] 10.101863 11.15117 12.356115 ## [5,] 9.349558 11.56219 11.185806 ## [6,] 8.192193 9.95556 10.314718 Now, a couple things to know about the tidyverse. It is a cluster of packages built primarily by Hadley Wickham, who is arguably the closest you can get to rock star status in the R programming world. The primary reason for tidyverse was to make the coding grammar uniform and linear in structure, as opposed to the nesting structure that R was originally developed in. One way to do this was to use an operator, %&gt;% (referred to as a pipe), that forwards a value into the next function or expression. Let me provide an example first: Let’s say we want to import our data, subset subjects &lt; 50, and then calculate mean and standard deviation across Control and Experimental groups for all continuous variables. Let’s see how this would like # Remember to load package library(tidyverse) ## Warning: package &#39;ggplot2&#39; was built under R version 3.6.1 ## Warning: package &#39;readr&#39; was built under R version 3.6.1 # Note that this syntax no longer requires quotation marks for characters. CSVfilter &lt;- filter(CSVimport, Age &lt; 50) CSVfilter_byTxt &lt;- group_by(CSVfilter, Treatment) summarise(CSVfilter_byTxt, Avg_Age = mean(Age), sd_Age = sd(Age)) ## # A tibble: 2 x 3 ## Treatment Avg_Age sd_Age ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 40.5 7.95 ## 2 Experiment 37.1 9.16 summarise(CSVfilter_byTxt, Avg_Height = mean(Height), sd_Height = sd(Height)) ## # A tibble: 2 x 3 ## Treatment Avg_Height sd_Height ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 63.9 2.81 ## 2 Experiment 65.7 2.86 summarise(CSVfilter_byTxt, Avg_Wt = mean(Wt), sd_Wt = sd(Wt)) ## # A tibble: 2 x 3 ## Treatment Avg_Wt sd_Wt ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 96.5 14.4 ## 2 Experiment 102. 16.1 summarise(CSVfilter_byTxt, Avg_BMI = mean(BMI), sd_BMI = sd(BMI)) ## # A tibble: 2 x 3 ## Treatment Avg_BMI sd_BMI ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 36.4 4.59 ## 2 Experiment 36.5 3.34 summarise(CSVfilter_byTxt, Avg_Glucose = mean(Glucose), sd_Glucose = sd(Glucose)) ## # A tibble: 2 x 3 ## Treatment Avg_Glucose sd_Glucose ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 99 12.3 ## 2 Experiment 99.4 4.74 We did this line by line and take notice that we had to make a couple objects, CSVfilter and CSVfilter_byTxt. Then we had to copy and paste the summarise function for each continuous variable. Lots of potential to make some typing erros here. Let’s try it again, but we’ll use the %&gt;% operator to avoid making several objects and we’ll use the gather function to reshape the data into a tall form. This will allow us to use the continuous variable as a grouping variable and make a single value column. Let’s look at an example and piece it together. # Do not have to load the library unless not previously loaded. # library(tidyverse) # We initially set the object and the pipes will transfer each function to each other. CSVimport %&gt;% filter(Age &lt; 50) %&gt;% gather(key = Variables, value = Measurement, -(Subject:Gender)) %&gt;% group_by(Variables, Treatment) %&gt;% summarise(mean = mean(Measurement), sd = sd(Measurement)) ## # A tibble: 10 x 4 ## # Groups: Variables [5] ## Variables Treatment mean sd ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Age Control 40.5 7.95 ## 2 Age Experiment 37.1 9.16 ## 3 BMI Control 36.4 4.59 ## 4 BMI Experiment 36.5 3.34 ## 5 Glucose Control 99 12.3 ## 6 Glucose Experiment 99.4 4.74 ## 7 Height Control 63.9 2.81 ## 8 Height Experiment 65.7 2.86 ## 9 Wt Control 96.5 14.4 ## 10 Wt Experiment 102. 16.1 Okay, so a lot to take in here. Let’s start at the beginning. Remember, the pipe operator allows data to be transfered to the next function. The pipe operator actually assigns it to the first argument in the function, thus, your object to the left of the pipe should be what is required in the first argument of the function. Let’s look at a very simple use of a pipe. # Let&#39;s make the same matrix that we did at the beginning of this section (2.9.3) randomDATA %&gt;% matrix(ncol=6, nrow=3) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 7.773096 12.273325 11.13885 10.10186 9.349558 8.192193 ## [2,] 14.218296 11.114960 13.72156 11.15117 11.562190 9.955560 ## [3,] 6.833486 7.703311 11.77362 12.35612 11.185806 10.314718 See how the pipe operator can be used in other ways. Here the pipe deposited the numeric vector into the first argument and we supplied the rest of the arguments to match the prior output. Now let’s look talk about gather. gather reshapes your column names and data points into a two new columns. One that we defined as ‘Variables’ and the other as ‘Measurement’. Then, it takes the rest of the data and repeats it evenly along each column name. Now can use the ‘Measurement’ column as a grouping variable. Let’s see the output. # Do not have to load the library unless not previously loaded. # library(tidyverse) # We initially set the object and the pipes will transfer each function to each other. CSVimport %&gt;% filter(Age &lt; 50) %&gt;% gather(key = Variables, value = Measurement, -(Subject:Gender)) %&gt;% head() ## Subject Treatment Gender Variables Measurement ## 1 579 Control F Age 38 ## 2 586 Control F Age 37 ## 3 508 Control F Age 49 ## 4 569 Control F Age 46 ## 5 568 Control M Age 47 ## 6 556 Control F Age 35 Next, we group by ‘Variables’ and ‘Treatment’ in the group_by function. Then we define which outputs we want to summarise. You may ask, if this is so straight-forward and linear, why do we have to learn all the other more complicated syntax. Well, pipes are good when you have an established workflow and are working with single output; however, it gets just as complicated when you require multiple inputs (e.g., t-test, ANOVA, regression, etc.). A new version of a major tidyverse package was released 9/14/2019 which released new functions to replace gather and spread. Still trying to determine if this new version will affect this workflow. This may affect those who have installed tidyverse or tidyr packages after 9/14/2019. 2.8.5 Merging data Just like with most tasks in R, there are several ways to do things. Let’s look at a couple functions to merge data by rows or columns. We will first split the data by Gender and then re-combine each by rows using the rbind function. # We can use brackets to subset data CSVmale &lt;- CSVimport[CSVimport$Gender %in% &quot;M&quot;,] CSVfemale &lt;- CSVimport[CSVimport$Gender %in% &quot;F&quot;,] # column labels must match for rbind() to work rbind(CSVmale, CSVfemale) ## Subject Treatment Gender Age Height Wt BMI Glucose ## 3 502 Control M 62 68.600 112.50 37.1 97 ## 6 568 Control M 47 69.250 127.23 41.0 104 ## 16 576 Experiment M 53 72.750 102.18 30.5 105 ## 19 548 Experiment M 29 70.750 139.40 43.1 104 ## 1 579 Control F 38 63.000 82.27 32.1 99 ## 2 586 Control F 37 62.000 94.20 37.9 108 ## 4 508 Control F 49 64.250 107.70 40.4 86 ## 5 569 Control F 46 63.800 80.91 29.9 102 ## 7 556 Control F 35 60.150 83.32 36.0 100 ## 8 581 Control F 49 64.500 92.09 34.6 126 ## 9 577 Control F 45 63.000 94.09 36.2 90 ## 10 516 Control F 25 68.000 94.68 31.5 87 ## 11 565 Control F 34 61.500 108.18 44.3 88 ## 12 598 Experiment F 37 65.750 95.36 34.2 101 ## 13 515 Experiment F 49 66.000 109.70 39.1 110 ## 14 535 Experiment F 41 67.250 99.60 34.1 96 ## 15 510 Experiment F 18 66.625 105.70 36.8 99 ## 17 550 Experiment F 37 66.000 99.80 35.4 97 ## 18 507 Experiment F 39 59.500 86.50 37.8 99 ## 20 517 Experiment F 44 64.750 92.80 34.7 94 ## 21 506 Experiment F 46 63.560 81.20 31.1 95 ## 22 530 Experiment F 31 66.500 109.82 38.4 99 There will be cases when we use one set of codes to determine P- and FDR-values from comparison tests for individual bacterial taxa, and then separately assess medians and IQRs for the same taxa. For convenience and table output, we will join those outputs. The cbind function combines data by columns. This will make data wider. We will add CSVimport to itself as an arbitrary example. # Looks odd, but data in CSVimport object is stored in memory, # so it doesn&#39;t matter how many times you use the object. head(cbind(CSVimport, CSVimport)) ## Subject Treatment Gender Age Height Wt BMI Glucose Subject ## 1 579 Control F 38 63.00 82.27 32.1 99 579 ## 2 586 Control F 37 62.00 94.20 37.9 108 586 ## 3 502 Control M 62 68.60 112.50 37.1 97 502 ## 4 508 Control F 49 64.25 107.70 40.4 86 508 ## 5 569 Control F 46 63.80 80.91 29.9 102 569 ## 6 568 Control M 47 69.25 127.23 41.0 104 568 ## Treatment Gender Age Height Wt BMI Glucose ## 1 Control F 38 63.00 82.27 32.1 99 ## 2 Control F 37 62.00 94.20 37.9 108 ## 3 Control M 62 68.60 112.50 37.1 97 ## 4 Control F 49 64.25 107.70 40.4 86 ## 5 Control F 46 63.80 80.91 29.9 102 ## 6 Control M 47 69.25 127.23 41.0 104 These low level functions are very dangerous, especially cbind if not used correctly. An alternative to cbind is the _join family of functions in the ‘tidyverse’. We will consider a couple of these, inner_join and full_join. The major difference between the two is that inner_join will filter rows that do not match between both data frames and full_join will add NA in rows that do not match. How do these functions know which rows match? Both data frames must have a matching column name that is specified in the function. Let’s see an example. # Can access these functions from tidyverse package, but they&#39;re technically in tidyr package # library (tidyverse) # Make 2 random data frames. DF1 &lt;- data.frame(ID = c(&quot;S1&quot;, &quot;S2&quot;, &quot;S3&quot;, &quot;S4&quot;), BMI = c(34.2, 36.1, 29.6, 33.4), Wt = c(88.5, 94.4, 96.6, 100.0)) DF1 ## ID BMI Wt ## 1 S1 34.2 88.5 ## 2 S2 36.1 94.4 ## 3 S3 29.6 96.6 ## 4 S4 33.4 100.0 # DF1 and DF2 rows differ. DF1 includes ID - S4, and DF2 includes ID - S5. DF2 &lt;- data.frame(ID = c(&quot;S1&quot;, &quot;S2&quot;, &quot;S3&quot;, &quot;S5&quot;), VitD = c(56.4, 80.3, 49.6, 46.1), SunHabit = c(&quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;)) DF2 ## ID VitD SunHabit ## 1 S1 56.4 Yes ## 2 S2 80.3 Yes ## 3 S3 49.6 No ## 4 S5 46.1 No # inner_join() - should only merge matching rows # the &#39;by&#39; argument tells inner_join() what to match on. It will only read a character input. inner_join(DF1, DF2, by=&quot;ID&quot;) ## ID BMI Wt VitD SunHabit ## 1 S1 34.2 88.5 56.4 Yes ## 2 S2 36.1 94.4 80.3 Yes ## 3 S3 29.6 96.6 49.6 No # full_join() - will merge all rows with both ID columns full_join(DF1, DF2, by=&quot;ID&quot;) ## ID BMI Wt VitD SunHabit ## 1 S1 34.2 88.5 56.4 Yes ## 2 S2 36.1 94.4 80.3 Yes ## 3 S3 29.6 96.6 49.6 No ## 4 S4 33.4 100.0 NA &lt;NA&gt; ## 5 S5 NA NA 46.1 No Note the NA values within the rows for “S4” and “S5”. 2.8.6 apply family of functions This is definitely an intermediate to advanced topic. But it is unavoidable for high-throughput data analysis. These functions are also what really sets R apart from more traditional statistical software. Similar to other programming languages, R utilizes loops to repeatedly performs a specific task over a set of defined conditions. Fortunately for us, we won’t have to use the loop system in R! However, R has a another system that can iteratively run a task over vectors and matrices, which is generally executed through the apply family of functions. What really sets the apply functions apart from a loop, is that they were built to have outputs for different purposes. We will only use apply, lapply, and sapply. Let’s start with the core apply function, apply. 2.8.6.1 apply If you look at the help page for apply, the description states: “Returns a vector or array or list of values obtained by applying a function to margins of an array or matrix.” Couple new concepts here. An array is another data structure that stores a single data type in greater than 2 dimensions. We won’t use array’s in this workshop so disregard this concept. A margin refers to the dimension in your data structure where 1 indicates rows and 2 indicates columns. Now the description states that apply applies a function to margins of a matrix, thus apply runs a function iteratively across either rows or columns in a matrix and returns the value. Let’s see it in action. # apply() is under the hood of all other apply related functions # Extract the continuous data. CSVapply &lt;- CSVimport[, colnames(CSVimport) %in% c(&quot;Age&quot;,&quot;Height&quot;,&quot;Wt&quot;,&quot;BMI&quot;,&quot;Glucose&quot;)] # Determine median for each column apply(X = CSVapply, MARGIN = 2, FUN = median) ## Age Height Wt BMI Glucose ## 40.00 65.25 97.48 36.10 99.00 At minimum, we need 3 arguments: the matrix, margin, and function. Let’s try another example run across rows where we want to determine the position of the minimum value. Here, the margin will be 1. # Not using arguments names in this example. apply(CSVapply, 1, which.min) ## [1] 4 1 4 4 4 4 1 4 4 1 1 4 4 4 1 4 4 4 1 4 4 1 We can also insert a user defined function and use curly brackets to make greatly expand the coding. Let’s say for instance, we wanted to identify outliers at 1 standard deviations from the mean and then delete values that are past this threshold. Not saying this should be done in real life, but as an example here. # Not using arguments names in this example. apply(CSVapply, 2, FUN = function(x) { # calculate sd dataSD &lt;- sd(x) # calculate mean dataMEAN &lt;- mean(x) # calculate value of mean + sd meansd &lt;- dataMEAN + dataSD # identify absolute values less than mean+sd # and change them to NA x[abs(x) &gt; meansd] &lt;- NA return(x) }) ## Age Height Wt BMI Glucose ## [1,] 38 63.000 82.27 32.1 99 ## [2,] 37 62.000 94.20 37.9 108 ## [3,] NA 68.600 112.50 37.1 97 ## [4,] 49 64.250 107.70 NA 86 ## [5,] 46 63.800 80.91 29.9 102 ## [6,] 47 NA NA NA 104 ## [7,] 35 60.150 83.32 36.0 100 ## [8,] 49 64.500 92.09 34.6 NA ## [9,] 45 63.000 94.09 36.2 90 ## [10,] 25 68.000 94.68 31.5 87 ## [11,] 34 61.500 108.18 NA 88 ## [12,] 37 65.750 95.36 34.2 101 ## [13,] 49 66.000 109.70 39.1 NA ## [14,] 41 67.250 99.60 34.1 96 ## [15,] 18 66.625 105.70 36.8 99 ## [16,] NA NA 102.18 30.5 105 ## [17,] 37 66.000 99.80 35.4 97 ## [18,] 39 59.500 86.50 37.8 99 ## [19,] 29 NA NA NA 104 ## [20,] 44 64.750 92.80 34.7 94 ## [21,] 46 63.560 81.20 31.1 95 ## [22,] 31 66.500 109.82 38.4 99 Wow, that is fairly advanced set of R code! Remember, section 2.6.5.1 (Function: Name assignment) alluded to functions without name assignments in a function called sapply. We’re almost there, but the concept still works here with apply. Let’s discuss what happened here. apply passes each column, which is a vector, iteratively to the provided function. This was a user-defined function with a single argument, so each object ‘x’ in the body of our function was the iteratively passed data from columns in CSVapply. Each ‘x’ had its mean and standard deviation calculated, then the mean+sd was calculated. Next, we determined which absolute values were greater than mean+sd in ‘x’ and assigned those values as NA. Lastly, we returned x. 2.8.6.2 lapply lapply is similar to apply, where is runs a function iteratively, but it has 2 major differences: 1) it is not restricted to a 2- or 3-dimensional data structure. It can use a vector also. 2) it only returns a list, hence the ‘l’ in lapply. The list lapply produces will always have the same number of elements as its corresponding input. For example, a 10 element vector provide to lapply will produce a list with 10 elements. Be mindful when using a matrix, because, remember a matrix is technically a 2-dimensional vector. Therefore, lapply will produce a list for every value in a matrix. lapply does not require a margin and will never apply a function over rows. Why is that? 1) lapply treats matrices as a vector, and lapply treats data frames as a list. Since each column in a data frame is technically an element in a list, then lapply considers data frames as a list and will pass each element to a function. Let’s see an example of each. Using lapply with a vector. We will pass each numerical value to a function that calculates the tetration of a value (a value to the power of its own value). # Can use the same names for arguments across functions. No harm. lapply(1:5, function(x) x^x) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 27 ## ## [[4]] ## [1] 256 ## ## [[5]] ## [1] 3125 Using lapply with a matrix. We will make a small 2x2 matrix and then calculate the min value, with the expectation that we can run lapply across each column. We will see that this is impossible. # Create matrix, note that R will automatically compute the right number of columns if only nrow is inputed. Matrix_example3 &lt;- matrix(c(3,6,8,14), nrow=2) lapply(X = Matrix_example3, FUN = min) ## [[1]] ## [1] 3 ## ## [[2]] ## [1] 6 ## ## [[3]] ## [1] 8 ## ## [[4]] ## [1] 14 Using lapply with a data frame. We will calculate the number of unique values within each column (element within a list!). # If each of your outputs are equal in length, coerce output into a data frame with as.data.frame(). lapply(CSVimport, function(x) length(unique(x))) ## $Subject ## [1] 22 ## ## $Treatment ## [1] 2 ## ## $Gender ## [1] 2 ## ## $Age ## [1] 17 ## ## $Height ## [1] 20 ## ## $Wt ## [1] 22 ## ## $BMI ## [1] 22 ## ## $Glucose ## [1] 17 2.8.6.3 sapply sapply is exactly like lapply except it returns a vector. Since a matrix is also a vector, sapply will return a matrix if the single output is a vector of values. Let’s repeat the lapply tetration with sapply. # sapply is really extracting a single stat. sapply(1:5, function(x) x^x) ## [1] 1 4 27 256 3125 sapply returns a nice compact vector compared to the list output from lapply. Let’s look at an example where we use sapply to produce the range of numerical values.will provide a matrix output. # range() does not work on character or factors. We need to extract the numeric data. CSVapply &lt;- CSVimport[, colnames(CSVimport) %in% c(&quot;Age&quot;,&quot;Height&quot;,&quot;Wt&quot;,&quot;BMI&quot;,&quot;Glucose&quot;)] sapply(CSVapply, range) ## Age Height Wt BMI Glucose ## [1,] 18 59.50 80.91 29.9 86 ## [2,] 62 72.75 139.40 44.3 126 The apply functions can become extremely powerful when combined with user-defined functions. They have various outputs to fit various needs, flexible, and can almost replace loops. 2.8.7 Sweep R has a little secret behind the scenes when it deals with vectors of different lengths. It recycles the shorter vector to match the the larger one. Let’s look at a very simplistic example. # The arithmetic operators are actually functions! A &lt;- 1:10 A * 10 ## [1] 10 20 30 40 50 60 70 80 90 100 Behind the scenes, R recycles the 10 value, which is technically a vector with a single element, into a a ten element vector to match A. Let’s say we want to extract the even numbers out of A. We could use the repeat function, rep with TRUE and FALSE logical operators. Remember, use you can use TRUE in the element positions to capture those elements. So we’s want a TRUE in each even position. We could do the following code. # There are other repeat options in rep(). Check the help page! Logice_extract &lt;- rep(c(FALSE, TRUE), times=5) Logice_extract ## [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE A[Logice_extract] ## [1] 2 4 6 8 10 We see that rep built the vector equal to the length of A and all of the even numbers were extracted. However, there is a easier way to do this using recycling. # Logical operators are fundamental part of R A[c(FALSE, TRUE)] ## [1] 2 4 6 8 10 Here, the FALSE and TRUE combination is repeated automatically by R to match the length of A. Remember, R is very logical and will recycle vectors with odd numbers, resulting in mismatched outcomes if you’re not careful. Now let’s look at the sweep function. This is a nice function that will take a data frame and apply a specific operation across margins with an equal lengthed vector. Think of transforming data into proportions. Proportional transformation divides each value in a variable by the sum of all variables within a sample. Let’s see an example! # We need to extract the numeric data. CSVprop &lt;- CSVimport[, colnames(CSVimport) %in% c(&quot;Age&quot;,&quot;Height&quot;,&quot;Wt&quot;,&quot;BMI&quot;,&quot;Glucose&quot;)] # Calculate the sum of each sample with the rowSums() function. SampleSums &lt;- rowSums(CSVprop) # We want to divide, &quot;/&quot;, the CSVprop across each row CSVprop_sweep &lt;- sweep(CSVprop, 1, SampleSums, &quot;/&quot;) # Round to the nearest thousands round(CSVprop_sweep, 3) ## Age Height Wt BMI Glucose ## 1 0.121 0.200 0.262 0.102 0.315 ## 2 0.109 0.183 0.278 0.112 0.318 ## 3 0.164 0.182 0.298 0.098 0.257 ## 4 0.141 0.185 0.310 0.116 0.248 ## 5 0.143 0.198 0.251 0.093 0.316 ## 6 0.121 0.178 0.328 0.106 0.268 ## 7 0.111 0.191 0.265 0.114 0.318 ## 8 0.134 0.176 0.251 0.094 0.344 ## 9 0.137 0.192 0.287 0.110 0.274 ## 10 0.082 0.222 0.309 0.103 0.284 ## 11 0.101 0.183 0.322 0.132 0.262 ## 12 0.111 0.197 0.286 0.103 0.303 ## 13 0.131 0.177 0.293 0.105 0.294 ## 14 0.121 0.199 0.295 0.101 0.284 ## 15 0.055 0.204 0.324 0.113 0.304 ## 16 0.146 0.200 0.281 0.084 0.289 ## 17 0.110 0.197 0.298 0.106 0.289 ## 18 0.121 0.185 0.269 0.117 0.308 ## 19 0.075 0.183 0.361 0.112 0.269 ## 20 0.133 0.196 0.281 0.105 0.285 ## 21 0.145 0.201 0.256 0.098 0.300 ## 22 0.090 0.193 0.319 0.111 0.287 In this arbitrary example, the rows should sum to 1. Let’s go over what we did. The first argument in sweep contains the data frame or matrix. The ‘1’ specifies the margin, 1 for rows and 2 for columns, just like in apply. The third argument is the vector that will be applied along the rows in this case. The last argument specifices the function that will be applied. We can verify that all samples sum to 1 now. # Multiply CSVprop_sweep by 100 to get percentages. rowSums(CSVprop_sweep) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2.9 Stats 2.9.1 Basic summary based stats. # Mean mean(CSVimport$BMI) ## [1] 36.19091 # Median median(CSVimport$BMI) ## [1] 36.1 # Standard deviation sd(CSVimport$BMI) ## [1] 3.932515 # Range range(CSVimport$BMI) ## [1] 29.9 44.3 # Standard error # Actually, no SEM function in the core packages. Let&#39;s make one! SEM &lt;- function(x) sd(x)/sqrt(length(x)) # SEM function SEM(CSVimport$BMI) ## [1] 0.838415 2.9.2 t-test and Mann Whitney U test The functions are going to get a little more complicated with comparative tasks, like t.test and wilcox.test (functions for t-test and Mann Whitney U test, respectively). They need to be able to determine which data is being compared and what the comparison groups are. This also applies for ANOVA and regression based function as well. We will look at a couple ways to provide this information, but be aware that not all user defined functions follow these examples. It is entirely up to the author of the function to define their arguments. Both of these function need to be flexible to handle the various ways that a t-test and/or Mann Whitney U test can be deployed, e.g., one-sample or two-sample tests. Let’s look at the help page (Figure 2.5). You can access the help page as follows # Use the &#39;?&#39; followed by the function name to bring up the help page. # Googling the function name will normally work too. ?t.test Figure 2.5: t-test help page from the ‘stats’ package. The help page shows every arguments that can be supplied to the t.test under the Usage and Arguments headers. The Usage section shows 3 different default settings for the t.test function, while the Arguments section provides the details of each each argument. The first example of t.test (first example under Usage) shows us that t.test can be run with only a numeric vector. Let’s try it and see what happens. # This is a one-sample t-test t.test(CSVimport$BMI) ## ## One Sample t-test ## ## data: CSVimport$BMI ## t = 43.166, df = 21, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 34.44733 37.93449 ## sample estimates: ## mean of x ## 36.19091 This is a one-sample t-test. We need a two-sample t-test, and this can be done using the other two examples. Let’s try the “Default S3 method” first. And don’t worry about what “Default S3 method” means, just understand there are two syntaxes that will do the same job. Now, this syntax requires an ‘x’ and ‘y’ argument that will contain the respective data for each group. We need to subset these data before so we can input them into these arguments. # Little more coding required for this example. # Extract Control BMI data CtrlBMI &lt;- CSVimport[CSVimport$Treatment == &quot;Control&quot;, &quot;BMI&quot;] # Extract Experimental BMI data ExpBMI &lt;- CSVimport[CSVimport$Treatment == &quot;Experiment&quot;, &quot;BMI&quot;] t.test(x = CtrlBMI, y = ExpBMI) ## ## Welch Two Sample t-test ## ## data: CtrlBMI and ExpBMI ## t = 0.30759, df = 19.382, p-value = 0.7617 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.055784 4.110330 ## sample estimates: ## mean of x mean of y ## 36.45455 35.92727 # Don&#39;t have to set objects first, this is equivalent: # Minus the hashtags, as not to duplicate output in the chapter. # t.test( # x = CSVimport[CSVimport$Treatment == &quot;Control&quot;, &quot;BMI&quot;], # y = CSVimport[CSVimport$Treatment == &quot;Experiment&quot;, &quot;BMI&quot;] # ) The other way t.test can calculate these results is by using the formula notation. This syntax does not require you to subset your data and intuitively matches the mathematical formula. Example below: # Data must be in a matrix or data frame for the formula syntax to work t.test(formula = BMI ~ Treatment, data = CSVimport) ## ## Welch Two Sample t-test ## ## data: BMI by Treatment ## t = 0.30759, df = 19.382, p-value = 0.7617 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.055784 4.110330 ## sample estimates: ## mean in group Control mean in group Experiment ## 36.45455 35.92727 Exactly the same. Note that you must supply the object name of your matrix/data frame and use the ~ (tilde) to distinguish the numeric vector and a factor with 2 levels. I will always try to use the formula syntax if available because it uses less coding, its straight forward, and it allows a redundant workflow if you’re doing hundreds of tests (e.g., microbial sequencing data!). In addition, and probably more importantly, this is the preferred syntax for regression based functions (including ANOVA’s). So it is important to understand. The wilcox.test function for Mann Whitney U tests follows the same syntax as t.test. The help pages for both contain all of what we covered here, plus more. In addition the help pages have examples that you can use as models if your model is not working. 2.9.3 ANOVA There are several ways to conduct an ANOVA in R. ViThe most direct way is to use the aov function. Virtually all ANOVA functions from the base package require the ‘formula’ syntax to differentiate the Y variable from the X variables (i.e., the classification and covariate variables). Lets look at aov below: # Just like above, data is required to be in a data frame. See the help page, ?aov aov(formula = BMI ~ Treatment + Gender, data = CSVimport) ## Call: ## aov(formula = BMI ~ Treatment + Gender, data = CSVimport) ## ## Terms: ## Treatment Gender Residuals ## Sum of Squares 1.52909 14.70124 308.52785 ## Deg. of Freedom 1 1 19 ## ## Residual standard error: 4.029678 ## Estimated effects may be unbalanced Notice that we have a 2-way ANOVA, with main effects of Treatment and Gender regressed against BMI. You separate the exploratory variables with a + within the formula. Interactions are denoted with a :. An example of this is upcoming. Also note that the output is very minimal, only displaying the sum of squares and degrees of freedom. To get a regular ANOVA output, you need to wrap the output in the summary function. # You can also make the output of aov() into an object, # and then call the summary() function on the object summary(aov(formula = BMI ~ Treatment + Gender, data = CSVimport)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 1 1.53 1.529 0.094 0.762 ## Gender 1 14.70 14.701 0.905 0.353 ## Residuals 19 308.53 16.238 Now we see the main effects, degrees of freedom, sum of squares, mean squares, F-value and P-values for the ANOVA. Let’s see an interaction. tice that we have a 2-way ANOVA, with main effects of Treatment and Gender regressed against BMI. You separate the exploratory variables with a + within the formula. Interactions are denoted with a :. An example of this is upcoming. Also note that the output is very minimal, only displaying the sum of squares and degrees of freedom. To get a regular ANOVA output, you need to wrap the output in the summary function. # Add another : for a three way interaction, if model necessitates summary(aov(formula = BMI ~ Treatment + Gender + Treatment:Gender, data = CSVimport)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 1 1.53 1.529 0.090 0.767 ## Gender 1 14.70 14.701 0.868 0.364 ## Treatment:Gender 1 3.63 3.627 0.214 0.649 ## Residuals 18 304.90 16.939 You could see how this could get tedious if you have more than a couple main effects, plus several interactions. Fortunately, R has a shortcut with *. As this is in a formula, it does not result in a multiplication operation. # Do not have to identify the &#39;formula&#39; argument name to correctly run the function. summary(aov(BMI ~ Treatment*Gender, data = CSVimport)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 1 1.53 1.529 0.090 0.767 ## Gender 1 14.70 14.701 0.868 0.364 ## Treatment:Gender 1 3.63 3.627 0.214 0.649 ## Residuals 18 304.90 16.939 Let’s look at another way to run an ANOVA. You may recall from your stats class that an ANOVA is really an extension of a linear model. Therefore, you can first fit a linear model with the lm function, using the exact same formula syntax in aov. Then use the anova function. Why the extra step? Well, there may be other uses for the linear model that might not be related to the ANOVA; however, we will not cover that here. # Look at the lm help page for more info regarding linear models in R linearMOD &lt;- lm(BMI ~ Treatment*Gender, data = CSVimport) # Do not need to use summary() function with this workflow. anova(linearMOD) ## Analysis of Variance Table ## ## Response: BMI ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 1 1.529 1.5291 0.0903 0.7673 ## Gender 1 14.701 14.7012 0.8679 0.3639 ## Treatment:Gender 1 3.627 3.6273 0.2141 0.6491 ## Residuals 18 304.901 16.9389 If you Google ANOVA analysis in R, you may come across some articles describing discordant results between either SAS or SPSS and R. This is for 2 reasons, 1) Each software calculates their underlying contrast structure slightly different, and 2) R defaults to sum of squares type I, vs sum or squares type III in SAS and SPSS. It does NOT mean that any of the results provided by these tools are incorrect by any means. It is up to us as users of these tools to understand the underlying principles and use them accordingly. Unfortunately, the ANOVA functions in the core packages do not provide alternative ways to calculate sum of squares. However, a mechanism does exist using the car package. # Install the package if needed and load the library # install.packages(&quot;car&quot;) library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## some ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode # Need to update the contrasts for type III sum of squares options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) # Now we can use the lm model from above in Anova in the car package and specify the sum of squares type # Note that this ANOVA function has a capital &#39;A&#39;. Anova(linearMOD, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: BMI ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 11584.9 1 683.9240 8.999e-16 *** ## Treatment 0.1 1 0.0055 0.9415 ## Gender 16.5 1 0.9721 0.3372 ## Treatment:Gender 3.6 1 0.2141 0.6491 ## Residuals 304.9 18 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Set contrasts back to original setting. options(contrasts = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) This output should match the default SAS output. The debate regarding which sum of squares is beyond this workshop and I will refer you to this PDF from Nancy Reid. 2.9.4 Kruskal-Wallis Test The Kruskal-Wallis test is the non-parametric equivlant to a 1-way anova. Same syntax used for kruskal.test and aov. # Do not need summary() function for kruskal.test(). kruskal.test(BMI ~ Treatment, data = CSVimport) ## ## Kruskal-Wallis rank sum test ## ## data: BMI by Treatment ## Kruskal-Wallis chi-squared = 0.13043, df = 1, p-value = 0.718 2.9.5 Post-hoc Tests The TukeyHSD function is available without an add-on package and requires the output from the aov function. # We will fit the ANOVA first and set it into an object. TXTanova &lt;- aov(formula = BMI ~ Treatment + Gender, data = CSVimport) # Note the capital &#39;T&#39; in the TukeyHSD() function. TukeyHSD(TXTanova, &quot;Treatment&quot;) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = BMI ~ Treatment + Gender, data = CSVimport) ## ## $Treatment ## diff lwr upr p adj ## Experiment-Control -0.5272727 -4.123633 3.069088 0.7622845 "],
["microbial-sequencing-data-analysis.html", "Chapter 3 Microbial Sequencing Data Analysis", " Chapter 3 Microbial Sequencing Data Analysis UNDER CONSTRUCTION "],
["alpha-diversity.html", "Chapter 4 Alpha-Diversity", " Chapter 4 Alpha-Diversity UNDER CONSTRUCTION "],
["beta-diversity.html", "Chapter 5 Beta-Diversity", " Chapter 5 Beta-Diversity UNDER CONSTRUCTION "],
["differential-abundance.html", "Chapter 6 Differential Abundance", " Chapter 6 Differential Abundance UNDER CONSTRUCTION "]
]
