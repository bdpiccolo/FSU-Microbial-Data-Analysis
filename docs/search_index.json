[
["index.html", "Florida State University Microbiota Workshop 2019 Introduction to Data Analysis of Microbial Sequencing in R", " Florida State University Microbiota Workshop 2019 Brian Piccolo 2019-10-01 Introduction to Data Analysis of Microbial Sequencing in R "],
["intro.html", "Chapter 1 Introduction 1.1 Types of microbiota data 1.2 Microbial composition data from sequencing technologies 1.3 Statistical analysis of microbial composition data", " Chapter 1 Introduction 1.1 Types of microbiota data Next-generation sequencing has greatly increased our ability to survey the gut microbiota over the past 10-15 years. The most widely used technologies to assess microbial community data are 16S rRNA amplicon sequencing and shotgun metagenomics sequencing. Both have been globaly referred to as “metagenomics”, but we will make sure to differentiate the technologies. 1.1.1 16S rRNA amplicon sequencing Sequences a single gene, the 16S ribosomal RNA (rRNA) gene. The 16S rRNA gene is ubiquitous in the bacterial kingdom, but is highly variable among different types of bacteria. About 80% of bacterial RNA is made up of rRNA, so this technology can identify rare bacteria groups at a high sensitivity (Li, 2015). The advantage of 16S rRNA sequencing is that it is fairly inexpensive, routinely done, and has good sensitivey at the community level. However, there are 2 major limitations associated with this technology: It is not sensitive at species level, and It does not provide any information regarding genetic potential or bacterial functions. 16S rRNA sequencing sometimes gets a bad rap because of these limitations, but there good reasons to use this technology. Again, its much less expensive than shotgun metagenomics and is still very sensitive at what it can read. It is a good option if you have many samples, a limited budget, and if you just want to identify whether there is an alteration in the composition of the microbiota. 1.1.2 Shotgun metagenomic sequencing Sequences all microbial genomic DNA. Provides species level compositional data and bacterial genetic data. The compositional data is exactly similar to 16S rRNA data, but superior because it gives species level resolution. However, it is much more expensive on a per sample basis and requires significantly greater computational power to process. 1.2 Microbial composition data from sequencing technologies The overall structure of 16S rRNA and shotgun metagenomic sequencing data are similar. Lets quickly describe how the data is aquired to understand why this is. Note, this workshop is set up for statistical analysis, so we will keep this section at a very, very basic level. For both technologies, bacterial DNA must be isolated from the sample (e.g., feces, intestinal content, skin, soil, water, dog hair, etc.). Once the bacterial DNA is isolated, a subsample of the DNA is extracted for PCR, a subset of the resulting amplicon is pooled into a library, and a subset of the library is then sequenced (Morton et al., 2019). Main difference between the two technologies is the libraries. Again, 16S rRNA libraries are focused on the 16S rRNA gene, while shotgun metagenomics is focused on all bacterial genetic material. In both cases, you end up with raw sequences that have to be mapped to referenced phylogenetic tree. Generally, the sequences are aligned to reference gene until a set of sequences fill a percentage of the gene at a certain threshold. Once this threshold or probability is reached, then these clusters of sequences are assigned to the phylogeny. This is now considered a single “read” in your data. The accumulation of these read counts provide us the data that can then be statistically analyzed. There are some considerations you should consider when working with next-generation microbial sequencing data. It is essential to reach a minimum sampling depth (i.e., total number of sequence counts in a sample to ensure adequate coverage of all taxa). It is semi-quantitative data, i.e., counts of sequencing reads are not actually measuring a single bacteria. Data is commonly referred to as “relative abundance.” It does not take into consideration the microbial load of each sample. 1.3 Statistical analysis of microbial composition data Microbial composition data is multi-level and high-dimensional. What do I mean by these terms? It is multi-level because it can be analyzed at various taxonomy levels, e.g., species or phylum level. It is high-dimensional because you will likely have hundreds to tens of thousands of variables (i.e., bacterial taxa) to analyze. Spreadsheet or GUI (graphical user interface) based statistical software may be cumbersome or difficult to work with for these reasons. Therefore, it is highly recommended to use language based stastical software, like R, that can be tailored to meet the needs of these factors. R is a freely available statistical language and environment that is built around an extremely large and active user group. In fact, this web-book was created using R. While there are many, many benefits to using R, those without progamming experience may have a steep learning curve. The next chapter will be dedicated to detailing the basic knowledge required to analyze microbial sequencing data in R. "],
["rbasics.html", "Chapter 2 Basics of R 2.1 R 2.2 Downloading R 2.3 The R console 2.4 The R script 2.5 R packages 2.6 Basic operations, data types, missing values, and functions 2.7 Importing data 2.8 Working with your data 2.9 Stats", " Chapter 2 Basics of R 2.1 R R is an object-oriented programming language that was developed for statistical computing and visualization. It is freely available under the GNU General Public License and has a large and active user community. Figure 2.1 describes the most visited languages comparable to the R user base at Stack Overflow the largest question and answer programming site. Note the sustained linear growth since 2012. Figure 2.1: From stackoverflow blog: The Impressive Growth of R. By David Robinson, 10/10/2017. I bolded object-oriented because it highlights a major advantage of reproducibility in R and other programming-based langauge. Everything that is done in R uses named objects as either functions or data. Understanding how functions manipulate data types and data structures is the foundation of working with R. For example, if you want to obtain the mean over a set of numbers, the mean function is used to determine the mean of numbers within a vector. These bolded words and phrases may seem abstract at this point, but this chapter will expand these concepts. This chapter is dedicated to basic concepts needed to complete this course. Googling your question will more often than not provide an answer to your question, but there are several online tutorials for those interested in a deeper understanding of R. Several of these courses are listed below. DataCamp Coursera statistics.com codecademy Many of these excellent courses were not available when I learned R. I extensively used the Quick-R website by Robert Kabacoff as I learned how to use R. I will be providing links periodically to Quick-R for additional information not covered in this chapter. 2.2 Downloading R First step is to downloading R. R is available at the The Comprehensive R Archive Network (CRAN) website. Figure 2.2 shows the website and note the links needed for different systems are in a box at the top of the website. The default downloading prompts will result in aquiring 32 and 64 bit version for Windows users. You will likely be running a 64-bit Windows, but please refer here if you want to check your Windows version. Figure 2.2: CRAN website 2.3 The R console Figure 2.3 shows the R console after opening. Our color scheme differ because I customized my colors, but the information should be very similar. This is the R console and it is where you will type in code or pass code to run, causing R to work. Figure 2.3: R console Let’s try some basic operations. Type in 2 + 2, or copy the inline code (R code within the text) below, paste it into your R console, and press enter. Note, since this web-book was written in R, there will typically be an output box after provided code. # Operations after hashtag will not be processed by R. Good way to write notes! 2 + 2 ## [1] 4 You should see the output is ‘4’ like the output box above this text. Now let’s make a data object! Cut and paste the code below to make an object that contains the results of 2 + 2 # The &#39;&lt;-&#39; is referred to as an assignment operator. Use this when defining the name of an object. object1 &lt;- 2 + 2 object1 ## [1] 4 The output is 4, which is what the object contains. Try the next line of code to verify that ‘object1’ is the number 4. # Output should be 400. object1 * 100 ## [1] 400 The name of the object can be anything you want as long as it doesn’t start with a number or special characters. It can be as short as a single letter or as long as you want. Recommend making object names as short and succinct as possible. R is also case sensitive. Obj2 and obj2 are 2 different objects. Periods (.) and underscores are allowable, but special characters are not. Try to make use of periods or underscores for readability, e.g., Obj1_MEAN. You can add more numbers using the c function, for example. # the c() function means either combine or concatenate. obj2 &lt;- c(5, 6.3, 3, 0, -1, 13, 7) obj2 ## [1] 5.0 6.3 3.0 0.0 -1.0 13.0 7.0 We can now take the mean of obj2 using the mean function. Note, the mean function is a specialized object that performs and action on a data object. We will cover this in more detail. # In almost every case, an object will go inside the function&#39;s parantheses. mean(obj2) ## [1] 4.757143 2.4 The R script So far, we’ve been typing directly into the console. When you close R, you will normally lose everything you’ve done during the session. You can, however, write and save your code on a specialized text file (script). This will allow you to replicate your results every time you open a new R session. There is an intrinsic R script option in your R environment. Click File and then select ‘New Script’. You will see a blank window titled, ‘Untitled - R Editor.’ You can then type R commands in this window. Copy and paste the following code to the R Editor. To run this code, highlight all of the code with your mouse and then press Ctrl+R (hold down control and press the R key on your keyboard). You can also run the script by pressing Ctrl+R line by line. obj3 &lt;- c(5, -1, 3, -10, -2, -12, 15, 0) # The &lt;= means &#39;less than or equal to&#39; # &gt;= means &#39;greater than or equal to&#39; obj3 &lt;= 0 ## [1] FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE A very popular modular script editor is RStudio. RStudio is open-source and free! It nicely packages the R console, script(s), help pages, and graphical outputs in a convenient program (Figure 2.4). Many new users find RStudio extremely helpful. There are other text/script editors, but very few will pass your code directly to the R console like RStudio. 2.5 R packages You have been introduced to a couple functions that appear to be pre-loaded in R. Functions are what really drive R and makes it such a flexible tool. Anyone can write a function, and many, many people have. This is how people contribute to the growth of R, by releasing packages of R functions that perform specific tasks to their fields. In fact, the mean and c functions we used in 2.3 and 2.4 are contained in a preloaded ‘base’ package. Most of the preloaded packages contain core functions and widely used statistical and plotting functions. We will eventually be loading packages specific to ecology and microbial sequencing data, but let’s first learn how to identify and download packages. After you download R, the only packages you have are the core packages. You will have to identify packages you need and download them. The R community has 3 major repositories for R packages: CRAN Bioconductor Github You only have to install a package once; however, downloaded packages do not automatically load. You will have to load these add-on packages using the library function. Additional information about R packages, including installing packages using the R menu, can be viewed here 2.5.1 CRAN CRAN is the major repository of R packages. If you refer to their website (linked in 2.2) and select the ‘Package’ link under the Software header, you will see that there are almost 15,000 R packages available. You can view a table of all 15,000 packages, but this seems incredibly cumbersome way to identify a certain package. Fortunately, CRAN has a Task View option in the side-panel under CRAN. If you click the ‘Task View’ link you will see a list of topics that may be relevant to your field (Figure 2.6). Find ‘Phylogenetics’ and press that link. In this Task View, there are several options with a very brief description of each function’s functionality. We will actually be using functions from the ape and vegan package. Brief description of both can be found here. Most times, you will know exactly which package you need to install, so browsing random packages is not typically done. There are a couple ways to install packages and the easiest way is to use the install.packages function. However, before we actually install a function, let’s talk about Mirrors. Mirrors are host locations that house all of the available CRAN packages. A lot of redundancy, but choosing a Mirror site closest to your location will help with bandwith and latency and allows various locations to absorp the overall user traffic. You will be forced to choose a Mirror when you initially download a package. Will only have to do this once during a single session. Now, let’s download the vegan package. Choose the closest Mirror location when you’re prompted. # The quotations differentiate a &quot;character string&quot; from a data object. # Multiple packages can be downloaded at once, using the c() function. #install.packages(&quot;vegan&quot;) 2.5.2 Bioconductor Bioconductor provides R packages developed specifically for a wide variety of high-throughput data analysis (e.g., -omics data). Like CRAN, it is open source and free to use. It is fairly straight forward to download packages from the Bioconductor website. Each package on Bioconductor has a dedicated website with installation instructions. We will use the phyloseq package as an example (Figure 2.4). The phyloseq package was developed by Paul McMurdie and Susan Holmes and is a very important package for handling microbial sequencing data in R. We will be utilizing this package in subsequent chapters. Figure 2.4: phyloseq website at Bioconductor All you have to do is copy and paste the inline code under the Installation header into your R console and then press Enter. Simple as that. More detailed information can be found at the Bioconductor website. 2.5.3 Github Github is arguably the most popular hosting site for developing and disseminating coding. In regards to R packages, it serves 2 major functions: 1) Many CRAN and Bioconductor packages provide their underlying code at Github, and 2) packages in early stages of develoment are available at Github prior to release on CRAN or Bioconductor. We will not need to download any packages from Github for this course, but it is a valuable resource to consider if you decide to use R in the future. Github always tracks any changes made to files it hosts. So it a great way to track versions of your workflow and greatly helps with transperancy and reproducibility. 2.6 Basic operations, data types, missing values, and functions 2.6.1 Basic operations Although these operators may seem fairly low level, they are extremely important for coding. Here is a table containing basic arithmetic operators. Table 2.1: Arithmetic Operators Operator Use + addition - subtract * multiply / division ^ exponential Another table containing logical operators. Logical operators will return TRUE or FALSE. Table 2.2: Logical Operators Operator Use &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greather than or equal to == exactly equal to != not equal to R has some redundancy issues with special characters that can be quite confusing. The assignment operator, &lt;-, was shown in 2.3. Looks very close to the less than logical operators &lt; or &lt;=. You may also ask, why not just use = to designate equal to rather than ==. The single equal has multiple functions in R, 1) it is also an assignment operator like &lt;- and 2) it assigns arguments (parameters) in functions. Please see below # These will result in the same output. arrowAssign &lt;- 100/4 equalAssign = 100/4 ## We can check to see if they are equal using &#39;==&#39; arrowAssign == equalAssign ## [1] TRUE 2.6.2 Data types It is important to understand how R handles different data types. For example, how R handles categorical data that could be character strings (e.g., Obese or Lean), numerical data (e.g., 5, 8, 10567156.2, -400), etc. Most of this should be fairly straight forward, but there are some nuances to be aware of. numeric: Can get very technical, but generally any real number including decimals. integer: Will appear if all data are whole numbers. R will automatically determine this designation, primarily when importing data. Operations on numeric and integer data are similar. character: Generally, any non-numeric or non-logical values (programmers call these, “string values”). Default setting for data that R cannot determine as numbers/logic. Can almost force anything to be a character. Character values will always be surrounded by quotations. factor: This type of data looks like a character, but is technically a special type of integer. It has a special attribute that assigns integers to a corresponding set of characters. The underlying integer assignment allows R to order the characters in plots, in addition to some other special features. Widely used in R, but can be confusing and lead to some odd outputs if not aware of their presence. logic: Either TRUE or FALSE. Very, very important. Many of the most basic functionality of R is based on logical operators. 2.6.3 Data structure R has various ways that it structures similar and different data types. The most basic of which is called a vector. 2.6.3.1 Vector A vector is a single value or combination of values within a particular data type. You create vectors using the c function, introduced in 2.3. Here are examples of each data type. # Example of a vector with a single value singlevalue_Vector &lt;- 50 singlevalue_Vector ## [1] 50 # Example of a numeric vector with a combination of values numeric_Vector &lt;- c(1.5, 20, -5348964.165141, 6) numeric_Vector ## [1] 1.5 20.0 -5348964.2 6.0 # Example of a character vector with a combination of values character_Vector &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;character string&quot;, &quot;FSU workshop 10-9-2019&quot;, &quot;bdpiccolo@gmail.com&quot;) character_Vector ## [1] &quot;cat&quot; &quot;dog&quot; ## [3] &quot;character string&quot; &quot;FSU workshop 10-9-2019&quot; ## [5] &quot;bdpiccolo@gmail.com&quot; # Example of a logical vector with a combination of values logical_Vector &lt;- c(TRUE, TRUE, FALSE, FALSE, TRUE, FALSE) logical_Vector ## [1] TRUE TRUE FALSE FALSE TRUE FALSE # Example of a logical vector with a combination of values factor_Vector &lt;- as.factor(c(&quot;Control&quot;, &quot;HighFat&quot;, &quot;Control&quot;, &quot;HighFat&quot;, &quot;Control&quot;, &quot;HighFat&quot;)) factor_Vector ## [1] Control HighFat Control HighFat Control HighFat ## Levels: Control HighFat Note the differences between the character and factor outputs. The characters are surrounded by quotations, but the factors are not. In addition, the factor output has a level attribute that shows the unique characters within the vector. 2.6.3.2 Matrix A matrix is a 2-dimensional vector. Since vectors can only contain a single data type, a matrix can then only contain a single data type. Here is an example of a matrix. # Example of a matrix with 2 rows and 3 columns matrix_example1 &lt;- matrix(data = c(2,5,7,3,7,3), nrow = 2, ncol = 3) matrix_example1 ## [,1] [,2] [,3] ## [1,] 2 7 7 ## [2,] 5 3 3 2.6.3.3 List So far, we’ve only been introduced to data structures that can handle a single data type. Lists are a way that you can package different data types in a single object. We will combine each of the vectors we created in 2.6.3.1. # Example of a list with 5 elements list_example &lt;- list(singlevalue_Vector, numeric_Vector, character_Vector, logical_Vector, factor_Vector) list_example ## [[1]] ## [1] 50 ## ## [[2]] ## [1] 1.5 20.0 -5348964.2 6.0 ## ## [[3]] ## [1] &quot;cat&quot; &quot;dog&quot; ## [3] &quot;character string&quot; &quot;FSU workshop 10-9-2019&quot; ## [5] &quot;bdpiccolo@gmail.com&quot; ## ## [[4]] ## [1] TRUE TRUE FALSE FALSE TRUE FALSE ## ## [[5]] ## [1] Control HighFat Control HighFat Control HighFat ## Levels: Control HighFat Note the double brackets above each element within the list. This designates the position of each element within the list. Each element acts independently of the other elements, which allows a list to hold different data types within a list. There is no limit to the number of elements that you can define. 2.6.3.4 Data frame A data frame will look very much like a matrix in your console, but is technically a constrained list. Each element within a data frame must be a vector of equal length. This allows it to have a 2-dimensional shape with different data types. This is the most common data structure you will find when uploading a .csv file. Here is an example of a data frame. # Example of a data frame with 3 columns and 4 rows. # First create vectors num_vec &lt;- c(1, 2.234, 0, 34) num_vec_2 &lt;- c(1000, 100, 10, 1) char_vec &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;mouse&quot;, &quot;cow&quot;) # Create data frame dataframe_example &lt;- data.frame(Num1 = num_vec, Num2 = num_vec_2, Char1 = char_vec) dataframe_example ## Num1 Num2 Char1 ## 1 1.000 1000 cat ## 2 2.234 100 dog ## 3 0.000 10 mouse ## 4 34.000 1 cow Note that the column names are labeled in the data frame. It is actually required that column and row names are labeled. You can access these names by using the colnames and rownames function. You can also label the rows and columns of matrices, but it is not required. 2.6.4 Missing values R handles missing values by replacing the missing value with a NA, no matter the data type. R does not know what to do with missing values because it assumes it could be anything. # Example of vectors with NA num_vec_withNA &lt;- c(1, 2.234, 0, NA, 5) num_vec_withNA ## [1] 1.000 2.234 0.000 NA 5.000 char_vec_withNA &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;mouse&quot;, &quot;cow&quot;, NA) char_vec_withNA ## [1] &quot;cat&quot; &quot;dog&quot; &quot;mouse&quot; &quot;cow&quot; NA Sometimes, package authors have accounted for NAs in their coding, but generally, you need to account for NA values in core functions. # mean() function will not work with an NA mean(num_vec_withNA) ## [1] NA Note that the output is also NA! That is because R does not know what to do with the missing value, so the function is undefined. Can correct this by using the ‘na.rm’ argument in the mean function. # This is not a standard term, so each function will have its own terminology. mean(num_vec_withNA, na.rm=TRUE) ## [1] 2.0585 More information can be found here 2.6.5 Functions This can be considered a more intermediate to advanced topic. You will come across some user-defined functions in this workshop, so it is good to understand some of the concepts. Another reason for R’s flexibility is that the user can create functions that can help and/or streamline their analyses. We will need to define our own functions for microbial sequencing analysis, so it will be advantages to go over some key concepts regarding functions. There are 4 main components of a function as shown below. Rfunction &lt;- # Name assignment function(arg1, arg2) { # Defining arguments for function inputs sum_args &lt;- arg1 + arg2 # Body of function with coding statements using defined arguments. return(sum_args) # Return object } 2.6.5.1 Name assignment This is what you name the function. Make sure it is a unique name. R will mask functions from other packages with the same names. If we named this function ‘mean’ it would mask the mean function from the ‘basic’ package. You can force a function from a specific package by using the coding below. This can be used if a function is masked or if you do not want to load an entire package. # You will not be able to import your data unless you set your working directory! base::mean(num_vec) It is not always necessary to name a function. This would occur in very specific functions that have an argument to use a user-defined function. Think of a situation where you want to identify the median over every column in a data frame (e.g., every genera in microbial sequencing data). There is a function that specifically runs a defined function over columns within a data frame, called sapply. Now let’s say you want to define your own function across these columns, you could set your own defined function, minus the name, within the fucntion argument within sapply. An example will be provided when we discuss loops and loop related functions. 2.6.5.2 Arguments Think of arguments as objects that will be passed to the coding within the body of the function. They can be made to accept any input or force you to select a defined set of parameters. We will only be working with the former type of arguments. Look how we place an input in the function created above. Rfunction(arg1 = 10, arg2 = 50) You don’t have to assign the arguments directly, but R will automatically try to order non-defined arguments. Arguments do not need to be defined in order. # Here, R will assign 10 to the first non-defined argument, arg1 Rfunction(10, arg2 = 50) # Here, R will assign 10 to the first non-defined argument, arg1 Rfunction(arg2 = 500, 10) # Here, R will assign 500 to the first non-defined argument, arg1; # then, R will define 10 to the next non-defined argument, arg2. Rfunction(500, 10) 2.6.5.3 Body The body of the function is what give the functionality to a function. It takes the definition of the argument, passes it to the coding in the body, then computes the coding operation. In this case, it takes the arg1 and arg2, sums them, and assigns the summation to an object. An interesting aspect of a function is that the body of the function does not interact with the R session. Meaning, that objects that you define in a function will only be remembered within the function and not outside it, so they will not affect your analysis. Objects within your analysis environment will be recognized in the body of a function though! Note that the body is nested within curly brackets, { and }. These define the body of the function and allow functions to contain more than a single line of code. You do not need to use curly brackets if you have a single line of code. 2.6.5.4 Return This defines the function’s output. Can be a single output or multiple outputs. A list is typically used if there are multiple objects that are being returned. Let’s make another function based on the one above, but use a list to output multiple objects. In this case, we will see that the arguments contain the inputted numbers. Rfunction_list &lt;- # Name assignment function(arg1, arg2) { # Defining arguments for function inputs sum_args &lt;- arg1 + arg2 # Body of function with coding statements using defined arguments. return(list(sum_args, arg1, arg2)) # Return object } Now let’s see the output. Rfunction_list(500, 1) ## [[1]] ## [1] 501 ## ## [[2]] ## [1] 500 ## ## [[3]] ## [1] 1 Note that the first element in the list is the summation of 500 and 1, the second element is the input of 500, and the third element is the input of 1. W It is good habit to define the output using the return function; however, it is not necessary. If you do not use return, then the last line of code will be the output. A single line function will sometimes be very minimal, for example # Function to determine squared residuals # The first line of code in the body is the last line too! residualFUN &lt;- function(x) (x/mean(x))^2 # Create data using the rnorm function normalDATA &lt;- rnorm(20, mean=100, sd=10) # Output list(normalDATA, residualFUN(normalDATA)) ## [[1]] ## [1] 97.95646 99.95371 86.05863 97.69517 82.13705 102.89153 92.13194 ## [8] 108.92194 97.49098 82.52124 103.62964 101.11511 117.37319 111.94054 ## [15] 113.46530 82.76537 96.41698 96.53338 98.78454 97.14970 ## ## [[2]] ## [1] 0.9920813 1.0329492 0.7657199 0.9867958 0.6975244 1.0945620 0.8776100 ## [8] 1.2266252 0.9826752 0.7040647 1.1103223 1.0570930 1.4243572 1.2955552 ## [15] 1.3310896 0.7082368 0.9611434 0.9634654 1.0089255 0.9758072 The first element of the list is the output of rnorm and the second element in the list is the output of residualFUN function. Look how flexible a list is! It allowed a function to define its second element! 2.7 Importing data R can handle a wide range of file types. For example, we will import a biome file later in this workshop. However, spreadsheet structured data is most commonly imported as a text delimited or csv file. More recent developed packages have have made it easier to import directly from an Excel file. However, we need to understand what a working directory is before we can demostrate importing files. 2.7.1 Working directory Quite simply, you have to tell R where to locate the file you want to import. To do this, you have to specify the directory where your files are located at. You can do that by choosing ‘File - Change dir’ and then selecting the directory with your files. Or you can use the setwd function. For example, # You will not be able to import your data unless you set your working directory! setwd(&quot;C:\\\\users\\\\brian\\\\documents\\\\FSU Workshop\\\\&quot;) 2.7.2 Comma delimited text file (csv) For our purposes, the read.csv function will suffice. There are other functions that can # Data is in a subfolder, so can use &#39;.\\\\Dir\\\\&#39; to reach. # The &#39;.&#39; denotes the working directory CSVimport &lt;- read.csv(&quot;.\\\\Data\\\\CSVexample.csv&quot;) # The head() function will show the first 6 rows of a data frame. head(CSVimport) ## Subject Treatment Gender Age Height Wt BMI Glucose ## 1 579 Control F 38 63.00 82.27 32.1 99 ## 2 586 Control F 37 62.00 94.20 37.9 108 ## 3 502 Control M 62 68.60 112.50 37.1 97 ## 4 508 Control F 49 64.25 107.70 40.4 86 ## 5 569 Control F 46 63.80 80.91 29.9 102 ## 6 568 Control M 47 69.25 127.23 41.0 104 Use the read.table function to import a space delimited text file (.txt) 2.7.3 Excel file There are now several packages that provide functions to import Excel files. We will use the read_xlsx function from the readxl package. There are many arguments that can be inputted in this function, but we will focus on the first two. The ‘path’ argument defines the file and the ‘sheet’ argument defines the the sheet name to be read. If the ‘sheet’ argument is not defined, then the function will read the first sheet. # Need to load the package. library(readxl) # Import Excel file from subfolder XLSXimport &lt;- read_xlsx(&quot;.\\\\Data\\\\XLSXexample.xlsx&quot;) XLSXimport ## # A tibble: 22 x 8 ## Subject Treatment Gender Age Height Wt BMI Glucose ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 579 Control F 38 63 82.3 32.1 99 ## 2 586 Control F 37 62 94.2 37.9 108 ## 3 502 Control M 62 68.6 112. 37.1 97 ## 4 508 Control F 49 64.2 108. 40.4 86 ## 5 569 Control F 46 63.8 80.9 29.9 102 ## 6 568 Control M 47 69.2 127. 41 104 ## 7 556 Control F 35 60.2 83.3 36 100 ## 8 581 Control F 49 64.5 92.1 34.6 126 ## 9 577 Control F 45 63 94.1 36.2 90 ## 10 516 Control F 25 68 94.7 31.5 87 ## # ... with 12 more rows Notice that the difference between this output and the csv output. The Excel output says that it is a tibble. This is a newer data structure based on a family of packages globally referred to as the tidyverse. We will not have time to learn the tidyverse syntax, but it is a highly popular package and and area of active development. We can coerce the tibble into a regular data frame, similar to the csv import. # You can also coerce data types into different data types. XLSXimport_df &lt;- as.data.frame(XLSXimport) # View abbreviated data head(XLSXimport_df) ## Subject Treatment Gender Age Height Wt BMI Glucose ## 1 579 Control F 38 63.00 82.27 32.1 99 ## 2 586 Control F 37 62.00 94.20 37.9 108 ## 3 502 Control M 62 68.60 112.50 37.1 97 ## 4 508 Control F 49 64.25 107.70 40.4 86 ## 5 569 Control F 46 63.80 80.91 29.9 102 ## 6 568 Control M 47 69.25 127.23 41.0 104 2.8 Working with your data We will now go over some basic data wrangling concepts and perform some basic univariate statistics on our imported data. 2.8.1 Viewing your data Its always a good idea to see what your data looks like after you import it. We used head earlier to see the first 6 rows # You can use the &#39;n&#39; argument to set how many rows are returned. head(CSVimport) ## Subject Treatment Gender Age Height Wt BMI Glucose ## 1 579 Control F 38 63.00 82.27 32.1 99 ## 2 586 Control F 37 62.00 94.20 37.9 108 ## 3 502 Control M 62 68.60 112.50 37.1 97 ## 4 508 Control F 49 64.25 107.70 40.4 86 ## 5 569 Control F 46 63.80 80.91 29.9 102 ## 6 568 Control M 47 69.25 127.23 41.0 104 There are columns that have characters and others that have numbers. That would indicate that we have a data frame. We can look at the structure of the object using the str function. # You can use the &#39;n&#39; argument to set how many rows are returned. str(CSVimport) ## &#39;data.frame&#39;: 22 obs. of 8 variables: ## $ Subject : int 579 586 502 508 569 568 556 581 577 516 ... ## $ Treatment: Factor w/ 2 levels &quot;Control&quot;,&quot;Experiment&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Gender : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 2 1 1 2 1 1 1 1 ... ## $ Age : int 38 37 62 49 46 47 35 49 45 25 ... ## $ Height : num 63 62 68.6 64.2 63.8 ... ## $ Wt : num 82.3 94.2 112.5 107.7 80.9 ... ## $ BMI : num 32.1 37.9 37.1 40.4 29.9 41 36 34.6 36.2 31.5 ... ## $ Glucose : int 99 108 97 86 102 104 100 126 90 87 ... Note that this return tells us that the object is indeed a data frame. It also shows us that R assigned the columns with strings as factors and also defined the columns with numbers as either integer or numeric. 2.8.2 Extracting elements Square brackets [ and ] are the most basic way to extract elements from various data structures. The most basic way is to define the position within a data structure. Let’s look at the ‘num_vec’ object from 2.6.3.4. There are 4 elements within the vector, 1, 2.234, 0, and 34. Let’s say we want to extract the 0 from this vector. We can use square brackets to extract this position. # Here is the original vector num_vec ## [1] 1.000 2.234 0.000 34.000 # 0 is the third element. num_vec[3] ## [1] 0 Now let’s say we want to extract the 2nd through the 4th element. We can use : if numbers are sequential. # Will work as long as the numbers are sequential and whole numbers num_vec[2:4] ## [1] 2.234 0.000 34.000 What if we want to extract elements 1 and 3? We can use the c function and define positions out of order. # There are multiple ways to do this. num_vec[c(1,3)] ## [1] 1 0 # This would also work, why? positions &lt;- c(1,3) num_vec[positions] # or num_vec[c(3,1)] We can also use logical statements to extract elements. # Under the hood, R is keeping the positions that are TRUE. num_vec[num_vec &gt; 2] ## [1] 2.234 34.000 Let’s look at a list output to see what is going on here. # Under the hood, R is keeping the positions that are TRUE. list(num_vec, num_vec &gt; 2) ## [[1]] ## [1] 1.000 2.234 0.000 34.000 ## ## [[2]] ## [1] FALSE TRUE FALSE TRUE The TRUE statements are equivelant to telling R which position to return. You have to be very careful when using positional bracketing. Square brackets can also work with 2-dimensional data structures. With matrices and data frames, a comma is added to differentiate rows and columns. Let’s look at the ‘matrix_object’ from 2.6.3.2. # Here is the original matrix matrix_example1 ## [,1] [,2] [,3] ## [1,] 2 7 7 ## [2,] 5 3 3 # Statements to the right of the comma refer to column position matrix_example1[,2:3] ## [,1] [,2] ## [1,] 7 7 ## [2,] 3 3 # Statements to the left of the comma refer to column position matrix_example1[1,] ## [1] 2 7 7 We can designate positions for both columns and rows. Let’s use the data frame from 2.6.3.4 # Here is the original matrix dataframe_example ## Num1 Num2 Char1 ## 1 1.000 1000 cat ## 2 2.234 100 dog ## 3 0.000 10 mouse ## 4 34.000 1 cow # can embed functions as shown above. dataframe_example[1:3,c(3,1)] ## Char1 Num1 ## 1 cat 1.000 ## 2 dog 2.234 ## 3 mouse 0.000 Note, that you can re-order the columns and rows using positional bracketing. Double brackets [[ and ]] are used with lists and data frames and are typically used to extract a single element by position or name. A the dollar sign, $, has the same functionality. For example, # These are all equivalent dataframe_example[[2]] ## [1] 1000 100 10 1 dataframe_example[[&quot;Num2&quot;]] ## [1] 1000 100 10 1 dataframe_example$Num2 ## [1] 1000 100 10 1 The $ will generally only work if the label has no spaces or special characters. Otherwise, use the double brackets with a character string. 2.8.3 Subsetting We can see from the str output in 2.8.1 that the ‘Treatment’ column has two factor levels, “Treatment” and “Control”. Let’s say we want to extract the data related to the Control group and make a new object with it. We could determine what row positions within the Treatment column contain the string “Control.” Fortunately, this dataset has them nicely ordered, but what if they’re not nicely ordered. Or what if you have 20,000 rows. A better way to do this is to use logical operators. Let see what happens when we use == on the ‘Treatment’ column. # &#39;==&#39; will ask if the vector is exact to something. CSVimport$Treatment == &quot;Control&quot; ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE It returns a logical vector of equal length to CSVimport$Treatment where TRUE replaces “Control” and FALSE replaces “Treatment”. We can use this with positional bracketing to subset the data. # Data structure, followed by brackets. # For 2-dim, rows left of comma, columns right of comma. CSV_ctrl &lt;- CSVimport[CSVimport$Treatment == &quot;Control&quot;, ] CSV_ctrl ## Subject Treatment Gender Age Height Wt BMI Glucose ## 1 579 Control F 38 63.00 82.27 32.1 99 ## 2 586 Control F 37 62.00 94.20 37.9 108 ## 3 502 Control M 62 68.60 112.50 37.1 97 ## 4 508 Control F 49 64.25 107.70 40.4 86 ## 5 569 Control F 46 63.80 80.91 29.9 102 ## 6 568 Control M 47 69.25 127.23 41.0 104 ## 7 556 Control F 35 60.15 83.32 36.0 100 ## 8 581 Control F 49 64.50 92.09 34.6 126 ## 9 577 Control F 45 63.00 94.09 36.2 90 ## 10 516 Control F 25 68.00 94.68 31.5 87 ## 11 565 Control F 34 61.50 108.18 44.3 88 Now, what if we have a vector containing Subject numbers that we want to filter on. Let’s see what happens when we use ==. # We can make an object containing the subject IDs first. ID_filter &lt;- c(&quot;579&quot;,&quot;586&quot;,&quot;502&quot;,&quot;508&quot;,&quot;569&quot;,&quot;568&quot;,&quot;556&quot;) CSVimport$Subject == ID_filter ## Warning in CSVimport$Subject == ID_filter: longer object length is not a ## multiple of shorter object length A weird error returns. I will not get into the reason for the error, but there is a more optimal way to do this using the match operator, %in%. The %in% operator will return TRUE if anything on the left matches on the right. Let’s see it in action. # Use &#39;==&#39; if you&#39;re matching on a single element # use &#39;%in% if you&#39;re filtering on more than 1 element. CSVimport$Subject %in% ID_filter ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE ## [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # Now subset the data CSV_IDfilter &lt;- CSVimport[CSVimport$Subject %in% ID_filter, ] CSV_IDfilter ## Subject Treatment Gender Age Height Wt BMI Glucose ## 1 579 Control F 38 63.00 82.27 32.1 99 ## 2 586 Control F 37 62.00 94.20 37.9 108 ## 3 502 Control M 62 68.60 112.50 37.1 97 ## 4 508 Control F 49 64.25 107.70 40.4 86 ## 5 569 Control F 46 63.80 80.91 29.9 102 ## 6 568 Control M 47 69.25 127.23 41.0 104 ## 7 556 Control F 35 60.15 83.32 36.0 100 Finally, let’s filter ‘Experiment’ subjects that are less than 40 years old. We also only want the ‘Age’, ‘Wt’, ‘BMI’, and ‘Glucose’ columns. # Use &#39;&amp;&#39; to evaluate logical combiners. There are other specialized logical combiners. CSV_exp40 &lt;- CSVimport[CSVimport$Treatment == &quot;Experiment&quot; &amp; CSVimport$Age &lt; 40, ] # Create vector containing the column names that we want to filter. # Remember, R is case sensitive. column_filter &lt;- c(&quot;Age&quot;, &quot;Wt&quot;, &quot;BMI&quot;, &quot;Glucose&quot;) CSV_exp40_filter &lt;- CSV_exp40[ ,colnames(CSV_exp40) %in% column_filter] CSV_exp40_filter ## Age Wt BMI Glucose ## 12 37 95.36 34.2 101 ## 15 18 105.70 36.8 99 ## 17 37 99.80 35.4 97 ## 18 39 86.50 37.8 99 ## 19 29 139.40 43.1 104 ## 22 31 109.82 38.4 99 There are many other ways to filter and subset your data. There is actually a subset function that does the same things we covered here, but within a function. tidyverse has a more linear approach to filtering, which has become extremely popular. Some workflows are better if you have millions of data points, but this way minimizes the need for add on functions. 2.8.4 Reshaping data This can be considered a more intermediate to advanced topic. Well, I told myself that I was not going to introduce functions from the tidyverse, but the reshaping functions are SO VERY convenient for high-throughput data analysis. Before we get into these functions, let me introduce you to the t (transpose) function first. # Will make a matrix first using the rnorm() function. randomDATA &lt;- rnorm(n = 18, mean = 10, sd = 2) Matrix_example2 &lt;- matrix(randomDATA, ncol=6, nrow=3) Matrix_example2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 8.942020 9.887677 11.44960 12.070519 9.944745 8.147628 ## [2,] 11.967007 11.958866 12.03037 8.524405 10.160582 10.422952 ## [3,] 8.293849 7.835140 13.85544 10.710589 10.431478 9.933755 # Transpose t(Matrix_example2) ## [,1] [,2] [,3] ## [1,] 8.942020 11.967007 8.293849 ## [2,] 9.887677 11.958866 7.835140 ## [3,] 11.449603 12.030368 13.855445 ## [4,] 12.070519 8.524405 10.710589 ## [5,] 9.944745 10.160582 10.431478 ## [6,] 8.147628 10.422952 9.933755 Now, a couple things to know about the tidyverse. It is a cluster of packages built primarily by Hadley Wickham, who is arguably the closest you can get to rock star status in the R programming world. The primary reason for tidyverse was to make the coding grammar uniform and linear in structure, as opposed to the nesting structure that R was originally developed in. One way to do this was to use an operator, %&gt;% (referred to as a pipe), that forwards a value into the next function or expression. Let me provide an example first: Let’s say we want to import our data, subset subjects &lt; 50, and then calculate mean and standard deviation across Control and Experimental groups for all continuous variables. Let’s see how this would like # Remember to load package library(tidyverse) ## Warning: package &#39;ggplot2&#39; was built under R version 3.6.1 ## Warning: package &#39;readr&#39; was built under R version 3.6.1 # Note that this syntax no longer requires quotation marks for characters. CSVfilter &lt;- filter(CSVimport, Age &lt; 50) CSVfilter_byTxt &lt;- group_by(CSVfilter, Treatment) summarise(CSVfilter_byTxt, Avg_Age = mean(Age), sd_Age = sd(Age)) ## # A tibble: 2 x 3 ## Treatment Avg_Age sd_Age ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 40.5 7.95 ## 2 Experiment 37.1 9.16 summarise(CSVfilter_byTxt, Avg_Height = mean(Height), sd_Height = sd(Height)) ## # A tibble: 2 x 3 ## Treatment Avg_Height sd_Height ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 63.9 2.81 ## 2 Experiment 65.7 2.86 summarise(CSVfilter_byTxt, Avg_Wt = mean(Wt), sd_Wt = sd(Wt)) ## # A tibble: 2 x 3 ## Treatment Avg_Wt sd_Wt ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 96.5 14.4 ## 2 Experiment 102. 16.1 summarise(CSVfilter_byTxt, Avg_BMI = mean(BMI), sd_BMI = sd(BMI)) ## # A tibble: 2 x 3 ## Treatment Avg_BMI sd_BMI ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 36.4 4.59 ## 2 Experiment 36.5 3.34 summarise(CSVfilter_byTxt, Avg_Glucose = mean(Glucose), sd_Glucose = sd(Glucose)) ## # A tibble: 2 x 3 ## Treatment Avg_Glucose sd_Glucose ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 99 12.3 ## 2 Experiment 99.4 4.74 We did this line by line and take notice that we had to make a couple objects, CSVfilter and CSVfilter_byTxt. Then we had to copy and paste the summarise function for each continuous variable. Lots of potential to make some typing erros here. Let’s try it again, but we’ll use the %&gt;% operator to avoid making several objects and we’ll use the gather function to reshape the data into a tall form. This will allow us to use the continuous variable as a grouping variable and make a single value column. Let’s look at an example and piece it together. # Do not have to load the library unless not previously loaded. # library(tidyverse) # We initially set the object and the pipes will transfer each function to each other. CSVimport %&gt;% filter(Age &lt; 50) %&gt;% gather(key = Variables, value = Measurement, -(Subject:Gender)) %&gt;% group_by(Variables, Treatment) %&gt;% summarise(mean = mean(Measurement), sd = sd(Measurement)) ## # A tibble: 10 x 4 ## # Groups: Variables [5] ## Variables Treatment mean sd ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Age Control 40.5 7.95 ## 2 Age Experiment 37.1 9.16 ## 3 BMI Control 36.4 4.59 ## 4 BMI Experiment 36.5 3.34 ## 5 Glucose Control 99 12.3 ## 6 Glucose Experiment 99.4 4.74 ## 7 Height Control 63.9 2.81 ## 8 Height Experiment 65.7 2.86 ## 9 Wt Control 96.5 14.4 ## 10 Wt Experiment 102. 16.1 Okay, so a lot to take in here. Let’s start at the beginning. Remember, the pipe operator allows data to be transfered to the next function. The pipe operator actually assigns it to the first argument in the function, thus, your object to the left of the pipe should be what is required in the first argument of the function. Let’s look at a very simple use of a pipe. # Let&#39;s make the same matrix that we did at the beginning of this section (2.9.3) randomDATA %&gt;% matrix(ncol=6, nrow=3) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 8.942020 9.887677 11.44960 12.070519 9.944745 8.147628 ## [2,] 11.967007 11.958866 12.03037 8.524405 10.160582 10.422952 ## [3,] 8.293849 7.835140 13.85544 10.710589 10.431478 9.933755 See how the pipe operator can be used in other ways. Here the pipe deposited the numeric vector into the first argument and we supplied the rest of the arguments to match the prior output. Now let’s look talk about gather. gather reshapes your column names and data points into a two new columns. One that we defined as ‘Variables’ and the other as ‘Measurement’. Then, it takes the rest of the data and repeats it evenly along each column name. Now can use the ‘Measurement’ column as a grouping variable. Let’s see the output. # Do not have to load the library unless not previously loaded. # library(tidyverse) # We initially set the object and the pipes will transfer each function to each other. CSVimport %&gt;% filter(Age &lt; 50) %&gt;% gather(key = Variables, value = Measurement, -(Subject:Gender)) %&gt;% head() ## Subject Treatment Gender Variables Measurement ## 1 579 Control F Age 38 ## 2 586 Control F Age 37 ## 3 508 Control F Age 49 ## 4 569 Control F Age 46 ## 5 568 Control M Age 47 ## 6 556 Control F Age 35 Next, we group by ‘Variables’ and ‘Treatment’ in the group_by function. Then we define which outputs we want to summarise. You may ask, if this is so straight-forward and linear, why do we have to learn all the other more complicated syntax. Well, pipes are good when you have an established workflow and are working with single output; however, it gets just as complicated when you require multiple inputs (e.g., t-test, ANOVA, regression, etc.). A new version of a major tidyverse package was released 9/14/2019 which released new functions to replace gather and spread. Still trying to determine if this new version will affect this workflow. This may affect those who have installed tidyverse or tidyr packages after 9/14/2019. 2.8.5 Merging data Just like with most tasks in R, there are several ways to do things. Let’s look at a couple functions to merge data by rows or columns. We will first split the data by Gender and then re-combine each by rows using the rbind function. # We can use brackets to subset data CSVmale &lt;- CSVimport[CSVimport$Gender %in% &quot;M&quot;,] CSVfemale &lt;- CSVimport[CSVimport$Gender %in% &quot;F&quot;,] # column labels must match for rbind() to work rbind(CSVmale, CSVfemale) ## Subject Treatment Gender Age Height Wt BMI Glucose ## 3 502 Control M 62 68.600 112.50 37.1 97 ## 6 568 Control M 47 69.250 127.23 41.0 104 ## 16 576 Experiment M 53 72.750 102.18 30.5 105 ## 19 548 Experiment M 29 70.750 139.40 43.1 104 ## 1 579 Control F 38 63.000 82.27 32.1 99 ## 2 586 Control F 37 62.000 94.20 37.9 108 ## 4 508 Control F 49 64.250 107.70 40.4 86 ## 5 569 Control F 46 63.800 80.91 29.9 102 ## 7 556 Control F 35 60.150 83.32 36.0 100 ## 8 581 Control F 49 64.500 92.09 34.6 126 ## 9 577 Control F 45 63.000 94.09 36.2 90 ## 10 516 Control F 25 68.000 94.68 31.5 87 ## 11 565 Control F 34 61.500 108.18 44.3 88 ## 12 598 Experiment F 37 65.750 95.36 34.2 101 ## 13 515 Experiment F 49 66.000 109.70 39.1 110 ## 14 535 Experiment F 41 67.250 99.60 34.1 96 ## 15 510 Experiment F 18 66.625 105.70 36.8 99 ## 17 550 Experiment F 37 66.000 99.80 35.4 97 ## 18 507 Experiment F 39 59.500 86.50 37.8 99 ## 20 517 Experiment F 44 64.750 92.80 34.7 94 ## 21 506 Experiment F 46 63.560 81.20 31.1 95 ## 22 530 Experiment F 31 66.500 109.82 38.4 99 There will be cases when we use one set of codes to determine P- and FDR-values from comparison tests for individual bacterial taxa, and then separately assess medians and IQRs for the same taxa. For convenience and table output, we will join those outputs. The cbind function combines data by columns. This will make data wider. We will add CSVimport to itself as an arbitrary example. # Looks odd, but data in CSVimport object is stored in memory, # so it doesn&#39;t matter how many times you use the object. head(cbind(CSVimport, CSVimport)) ## Subject Treatment Gender Age Height Wt BMI Glucose Subject ## 1 579 Control F 38 63.00 82.27 32.1 99 579 ## 2 586 Control F 37 62.00 94.20 37.9 108 586 ## 3 502 Control M 62 68.60 112.50 37.1 97 502 ## 4 508 Control F 49 64.25 107.70 40.4 86 508 ## 5 569 Control F 46 63.80 80.91 29.9 102 569 ## 6 568 Control M 47 69.25 127.23 41.0 104 568 ## Treatment Gender Age Height Wt BMI Glucose ## 1 Control F 38 63.00 82.27 32.1 99 ## 2 Control F 37 62.00 94.20 37.9 108 ## 3 Control M 62 68.60 112.50 37.1 97 ## 4 Control F 49 64.25 107.70 40.4 86 ## 5 Control F 46 63.80 80.91 29.9 102 ## 6 Control M 47 69.25 127.23 41.0 104 These low level functions are very dangerous, especially cbind if not used correctly. An alternative to cbind is the _join family of functions in the ‘tidyverse’. We will consider a couple of these, inner_join and full_join. The major difference between the two is that inner_join will filter rows that do not match between both data frames and full_join will add NA in rows that do not match. How do these functions know which rows match? Both data frames must have a matching column name that is specified in the function. Let’s see an example. # Can access these functions from tidyverse package, but they&#39;re technically in tidyr package # library (tidyverse) # Make 2 random data frames. DF1 &lt;- data.frame(ID = c(&quot;S1&quot;, &quot;S2&quot;, &quot;S3&quot;, &quot;S4&quot;), BMI = c(34.2, 36.1, 29.6, 33.4), Wt = c(88.5, 94.4, 96.6, 100.0)) DF1 ## ID BMI Wt ## 1 S1 34.2 88.5 ## 2 S2 36.1 94.4 ## 3 S3 29.6 96.6 ## 4 S4 33.4 100.0 # DF1 and DF2 rows differ. DF1 includes ID - S4, and DF2 includes ID - S5. DF2 &lt;- data.frame(ID = c(&quot;S1&quot;, &quot;S2&quot;, &quot;S3&quot;, &quot;S5&quot;), VitD = c(56.4, 80.3, 49.6, 46.1), SunHabit = c(&quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;)) DF2 ## ID VitD SunHabit ## 1 S1 56.4 Yes ## 2 S2 80.3 Yes ## 3 S3 49.6 No ## 4 S5 46.1 No # inner_join() - should only merge matching rows # the &#39;by&#39; argument tells inner_join() what to match on. It will only read a character input. inner_join(DF1, DF2, by=&quot;ID&quot;) ## ID BMI Wt VitD SunHabit ## 1 S1 34.2 88.5 56.4 Yes ## 2 S2 36.1 94.4 80.3 Yes ## 3 S3 29.6 96.6 49.6 No # full_join() - will merge all rows with both ID columns full_join(DF1, DF2, by=&quot;ID&quot;) ## ID BMI Wt VitD SunHabit ## 1 S1 34.2 88.5 56.4 Yes ## 2 S2 36.1 94.4 80.3 Yes ## 3 S3 29.6 96.6 49.6 No ## 4 S4 33.4 100.0 NA &lt;NA&gt; ## 5 S5 NA NA 46.1 No Note the NA values within the rows for “S4” and “S5”. 2.8.6 apply family of functions This is definitely an intermediate to advanced topic. But it is unavoidable for high-throughput data analysis. These functions are also what really sets R apart from more traditional statistical software. Similar to other programming languages, R utilizes loops to repeatedly performs a specific task over a set of defined conditions. Fortunately for us, we won’t have to use the loop system in R! However, R has a another system that can iteratively run a task over vectors and matrices, which is generally executed through the apply family of functions. What really sets the apply functions apart from a loop, is that they were built to have outputs for different purposes. We will only use apply, lapply, and sapply. Let’s start with the core apply function, apply. 2.8.6.1 apply If you look at the help page for apply, the description states: “Returns a vector or array or list of values obtained by applying a function to margins of an array or matrix.” Couple new concepts here. An array is another data structure that stores a single data type in greater than 2 dimensions. We won’t use array’s in this workshop so disregard this concept. A margin refers to the dimension in your data structure where 1 indicates rows and 2 indicates columns. Now the description states that apply applies a function to margins of a matrix, thus apply runs a function iteratively across either rows or columns in a matrix and returns the value. Let’s see it in action. # apply() is under the hood of all other apply related functions # Extract the continuous data. CSVapply &lt;- CSVimport[, colnames(CSVimport) %in% c(&quot;Age&quot;,&quot;Height&quot;,&quot;Wt&quot;,&quot;BMI&quot;,&quot;Glucose&quot;)] # Determine median for each column apply(X = CSVapply, MARGIN = 2, FUN = median) ## Age Height Wt BMI Glucose ## 40.00 65.25 97.48 36.10 99.00 At minimum, we need 3 arguments: the matrix, margin, and function. Let’s try another example run across rows where we want to determine the position of the minimum value. Here, the margin will be 1. # Not using arguments names in this example. apply(CSVapply, 1, which.min) ## [1] 4 1 4 4 4 4 1 4 4 1 1 4 4 4 1 4 4 4 1 4 4 1 We can also insert a user defined function and use curly brackets to make greatly expand the coding. Let’s say for instance, we wanted to identify outliers at 1 standard deviations from the mean and then delete values that are past this threshold. Not saying this should be done in real life, but as an example here. # Not using arguments names in this example. apply(CSVapply, 2, FUN = function(x) { # calculate sd dataSD &lt;- sd(x) # calculate mean dataMEAN &lt;- mean(x) # calculate value of mean + sd meansd &lt;- dataMEAN + dataSD # identify absolute values less than mean+sd # and change them to NA x[abs(x) &gt; meansd] &lt;- NA return(x) }) ## Age Height Wt BMI Glucose ## [1,] 38 63.000 82.27 32.1 99 ## [2,] 37 62.000 94.20 37.9 108 ## [3,] NA 68.600 112.50 37.1 97 ## [4,] 49 64.250 107.70 NA 86 ## [5,] 46 63.800 80.91 29.9 102 ## [6,] 47 NA NA NA 104 ## [7,] 35 60.150 83.32 36.0 100 ## [8,] 49 64.500 92.09 34.6 NA ## [9,] 45 63.000 94.09 36.2 90 ## [10,] 25 68.000 94.68 31.5 87 ## [11,] 34 61.500 108.18 NA 88 ## [12,] 37 65.750 95.36 34.2 101 ## [13,] 49 66.000 109.70 39.1 NA ## [14,] 41 67.250 99.60 34.1 96 ## [15,] 18 66.625 105.70 36.8 99 ## [16,] NA NA 102.18 30.5 105 ## [17,] 37 66.000 99.80 35.4 97 ## [18,] 39 59.500 86.50 37.8 99 ## [19,] 29 NA NA NA 104 ## [20,] 44 64.750 92.80 34.7 94 ## [21,] 46 63.560 81.20 31.1 95 ## [22,] 31 66.500 109.82 38.4 99 Wow, that is fairly advanced set of R code! Remember, section 2.6.5.1 (Function: Name assignment) alluded to functions without name assignments in a function called sapply. We’re almost there, but the concept still works here with apply. Let’s discuss what happened here. apply passes each column, which is a vector, iteratively to the provided function. This was a user-defined function with a single argument, so each object ‘x’ in the body of our function was the iteratively passed data from columns in CSVapply. Each ‘x’ had its mean and standard deviation calculated, then the mean+sd was calculated. Next, we determined which absolute values were greater than mean+sd in ‘x’ and assigned those values as NA. Lastly, we returned x. 2.8.6.2 lapply lapply is similar to apply, where is runs a function iteratively, but it has 2 major differences: 1) it is not restricted to a 2- or 3-dimensional data structure. It can use a vector also. 2) it only returns a list, hence the ‘l’ in lapply. The list lapply produces will always have the same number of elements as its corresponding input. For example, a 10 element vector provide to lapply will produce a list with 10 elements. Be mindful when using a matrix, because, remember a matrix is technically a 2-dimensional vector. Therefore, lapply will produce a list for every value in a matrix. lapply does not require a margin and will never apply a function over rows. Why is that? 1) lapply treats matrices as a vector, and lapply treats data frames as a list. Since each column in a data frame is technically an element in a list, then lapply considers data frames as a list and will pass each element to a function. Let’s see an example of each. Using lapply with a vector. We will pass each numerical value to a function that calculates the tetration of a value (a value to the power of its own value). # Can use the same names for arguments across functions. No harm. lapply(1:5, function(x) x^x) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 27 ## ## [[4]] ## [1] 256 ## ## [[5]] ## [1] 3125 Using lapply with a matrix. We will make a small 2x2 matrix and then calculate the min value, with the expectation that we can run lapply across each column. We will see that this is impossible. # Create matrix, note that R will automatically compute the right number of columns if only nrow is inputed. Matrix_example3 &lt;- matrix(c(3,6,8,14), nrow=2) lapply(X = Matrix_example3, FUN = min) ## [[1]] ## [1] 3 ## ## [[2]] ## [1] 6 ## ## [[3]] ## [1] 8 ## ## [[4]] ## [1] 14 Using lapply with a data frame. We will calculate the number of unique values within each column (element within a list!). # If each of your outputs are equal in length, coerce output into a data frame with as.data.frame(). lapply(CSVimport, function(x) length(unique(x))) ## $Subject ## [1] 22 ## ## $Treatment ## [1] 2 ## ## $Gender ## [1] 2 ## ## $Age ## [1] 17 ## ## $Height ## [1] 20 ## ## $Wt ## [1] 22 ## ## $BMI ## [1] 22 ## ## $Glucose ## [1] 17 2.8.6.3 sapply sapply is exactly like lapply except it returns a vector. Since a matrix is also a vector, sapply will return a matrix if the single output is a vector of values. Let’s repeat the lapply tetration with sapply. # sapply is really extracting a single stat. sapply(1:5, function(x) x^x) ## [1] 1 4 27 256 3125 sapply returns a nice compact vector compared to the list output from lapply. Let’s look at an example where we use sapply to produce the range of numerical values.will provide a matrix output. # range() does not work on character or factors. We need to extract the numeric data. CSVapply &lt;- CSVimport[, colnames(CSVimport) %in% c(&quot;Age&quot;,&quot;Height&quot;,&quot;Wt&quot;,&quot;BMI&quot;,&quot;Glucose&quot;)] sapply(CSVapply, range) ## Age Height Wt BMI Glucose ## [1,] 18 59.50 80.91 29.9 86 ## [2,] 62 72.75 139.40 44.3 126 The apply functions can become extremely powerful when combined with user-defined functions. They have various outputs to fit various needs, flexible, and can almost replace loops. 2.8.7 Sweep R has a little secret behind the scenes when it deals with vectors of different lengths. It recycles the shorter vector to match the the larger one. Let’s look at a very simplistic example. # The arithmetic operators are actually functions! A &lt;- 1:10 A * 10 ## [1] 10 20 30 40 50 60 70 80 90 100 Behind the scenes, R recycles the 10 value, which is technically a vector with a single element, into a a ten element vector to match A. Let’s say we want to extract the even numbers out of A. We could use the repeat function, rep with TRUE and FALSE logical operators. Remember, use you can use TRUE in the element positions to capture those elements. So we’s want a TRUE in each even position. We could do the following code. # There are other repeat options in rep(). Check the help page! Logice_extract &lt;- rep(c(FALSE, TRUE), times=5) Logice_extract ## [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE A[Logice_extract] ## [1] 2 4 6 8 10 We see that rep built the vector equal to the length of A and all of the even numbers were extracted. However, there is a easier way to do this using recycling. # Logical operators are fundamental part of R A[c(FALSE, TRUE)] ## [1] 2 4 6 8 10 Here, the FALSE and TRUE combination is repeated automatically by R to match the length of A. Remember, R is very logical and will recycle vectors with odd numbers, resulting in mismatched outcomes if you’re not careful. Now let’s look at the sweep function. This is a nice function that will take a data frame and apply a specific operation across margins with an equal lengthed vector. Think of transforming data into proportions. Proportional transformation divides each value in a variable by the sum of all variables within a sample. Let’s see an example! # We need to extract the numeric data. CSVprop &lt;- CSVimport[, colnames(CSVimport) %in% c(&quot;Age&quot;,&quot;Height&quot;,&quot;Wt&quot;,&quot;BMI&quot;,&quot;Glucose&quot;)] # Calculate the sum of each sample with the rowSums() function. SampleSums &lt;- rowSums(CSVprop) # We want to divide, &quot;/&quot;, the CSVprop across each row CSVprop_sweep &lt;- sweep(CSVprop, 1, SampleSums, &quot;/&quot;) # Round to the nearest thousands round(CSVprop_sweep, 3) ## Age Height Wt BMI Glucose ## 1 0.121 0.200 0.262 0.102 0.315 ## 2 0.109 0.183 0.278 0.112 0.318 ## 3 0.164 0.182 0.298 0.098 0.257 ## 4 0.141 0.185 0.310 0.116 0.248 ## 5 0.143 0.198 0.251 0.093 0.316 ## 6 0.121 0.178 0.328 0.106 0.268 ## 7 0.111 0.191 0.265 0.114 0.318 ## 8 0.134 0.176 0.251 0.094 0.344 ## 9 0.137 0.192 0.287 0.110 0.274 ## 10 0.082 0.222 0.309 0.103 0.284 ## 11 0.101 0.183 0.322 0.132 0.262 ## 12 0.111 0.197 0.286 0.103 0.303 ## 13 0.131 0.177 0.293 0.105 0.294 ## 14 0.121 0.199 0.295 0.101 0.284 ## 15 0.055 0.204 0.324 0.113 0.304 ## 16 0.146 0.200 0.281 0.084 0.289 ## 17 0.110 0.197 0.298 0.106 0.289 ## 18 0.121 0.185 0.269 0.117 0.308 ## 19 0.075 0.183 0.361 0.112 0.269 ## 20 0.133 0.196 0.281 0.105 0.285 ## 21 0.145 0.201 0.256 0.098 0.300 ## 22 0.090 0.193 0.319 0.111 0.287 In this arbitrary example, the rows should sum to 1. Let’s go over what we did. The first argument in sweep contains the data frame or matrix. The ‘1’ specifies the margin, 1 for rows and 2 for columns, just like in apply. The third argument is the vector that will be applied along the rows in this case. The last argument specifices the function that will be applied. We can verify that all samples sum to 1 now. # Multiply CSVprop_sweep by 100 to get percentages. rowSums(CSVprop_sweep) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2.9 Stats 2.9.1 Basic summary based stats. # Mean mean(CSVimport$BMI) ## [1] 36.19091 # Median median(CSVimport$BMI) ## [1] 36.1 # Standard deviation sd(CSVimport$BMI) ## [1] 3.932515 # Range range(CSVimport$BMI) ## [1] 29.9 44.3 # Standard error # Actually, no SEM function in the core packages. Let&#39;s make one! SEM &lt;- function(x) sd(x)/sqrt(length(x)) # SEM function SEM(CSVimport$BMI) ## [1] 0.838415 2.9.2 t-test and Mann Whitney U test The functions are going to get a little more complicated with comparative tasks, like t.test and wilcox.test (functions for t-test and Mann Whitney U test, respectively). They need to be able to determine which data is being compared and what the comparison groups are. This also applies for ANOVA and regression based function as well. We will look at a couple ways to provide this information, but be aware that not all user defined functions follow these examples. It is entirely up to the author of the function to define their arguments. Both of these function need to be flexible to handle the various ways that a t-test and/or Mann Whitney U test can be deployed, e.g., one-sample or two-sample tests. Let’s look at the help page (Figure 2.5). You can access the help page as follows # Use the &#39;?&#39; followed by the function name to bring up the help page. # Googling the function name will normally work too. ?t.test Figure 2.5: t-test help page from the ‘stats’ package. The help page shows every arguments that can be supplied to the t.test under the Usage and Arguments headers. The Usage section shows 3 different default settings for the t.test function, while the Arguments section provides the details of each each argument. The first example of t.test (first example under Usage) shows us that t.test can be run with only a numeric vector. Let’s try it and see what happens. # This is a one-sample t-test t.test(CSVimport$BMI) ## ## One Sample t-test ## ## data: CSVimport$BMI ## t = 43.166, df = 21, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 34.44733 37.93449 ## sample estimates: ## mean of x ## 36.19091 This is a one-sample t-test. We need a two-sample t-test, and this can be done using the other two examples. Let’s try the “Default S3 method” first. And don’t worry about what “Default S3 method” means, just understand there are two syntaxes that will do the same job. Now, this syntax requires an ‘x’ and ‘y’ argument that will contain the respective data for each group. We need to subset these data before so we can input them into these arguments. # Little more coding required for this example. # Extract Control BMI data CtrlBMI &lt;- CSVimport[CSVimport$Treatment == &quot;Control&quot;, &quot;BMI&quot;] # Extract Experimental BMI data ExpBMI &lt;- CSVimport[CSVimport$Treatment == &quot;Experiment&quot;, &quot;BMI&quot;] t.test(x = CtrlBMI, y = ExpBMI) ## ## Welch Two Sample t-test ## ## data: CtrlBMI and ExpBMI ## t = 0.30759, df = 19.382, p-value = 0.7617 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.055784 4.110330 ## sample estimates: ## mean of x mean of y ## 36.45455 35.92727 # Don&#39;t have to set objects first, this is equivalent: # Minus the hashtags, as not to duplicate output in the chapter. # t.test( # x = CSVimport[CSVimport$Treatment == &quot;Control&quot;, &quot;BMI&quot;], # y = CSVimport[CSVimport$Treatment == &quot;Experiment&quot;, &quot;BMI&quot;] # ) The other way t.test can calculate these results is by using the formula notation. This syntax does not require you to subset your data and intuitively matches the mathematical formula. Example below: # Data must be in a matrix or data frame for the formula syntax to work t.test(formula = BMI ~ Treatment, data = CSVimport) ## ## Welch Two Sample t-test ## ## data: BMI by Treatment ## t = 0.30759, df = 19.382, p-value = 0.7617 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.055784 4.110330 ## sample estimates: ## mean in group Control mean in group Experiment ## 36.45455 35.92727 Exactly the same. Note that you must supply the object name of your matrix/data frame and use the ~ (tilde) to distinguish the numeric vector and a factor with 2 levels. I will always try to use the formula syntax if available because it uses less coding, its straight forward, and it allows a redundant workflow if you’re doing hundreds of tests (e.g., microbial sequencing data!). In addition, and probably more importantly, this is the preferred syntax for regression based functions (including ANOVA’s). So it is important to understand. The wilcox.test function for Mann Whitney U tests follows the same syntax as t.test. The help pages for both contain all of what we covered here, plus more. In addition the help pages have examples that you can use as models if your model is not working. 2.9.3 ANOVA There are several ways to conduct an ANOVA in R. ViThe most direct way is to use the aov function. Virtually all ANOVA functions from the base package require the ‘formula’ syntax to differentiate the Y variable from the X variables (i.e., the classification and covariate variables). Lets look at aov below: # Just like above, data is required to be in a data frame. See the help page, ?aov aov(formula = BMI ~ Treatment + Gender, data = CSVimport) ## Call: ## aov(formula = BMI ~ Treatment + Gender, data = CSVimport) ## ## Terms: ## Treatment Gender Residuals ## Sum of Squares 1.52909 14.70124 308.52785 ## Deg. of Freedom 1 1 19 ## ## Residual standard error: 4.029678 ## Estimated effects may be unbalanced Notice that we have a 2-way ANOVA, with main effects of Treatment and Gender regressed against BMI. You separate the exploratory variables with a + within the formula. Interactions are denoted with a :. An example of this is upcoming. Also note that the output is very minimal, only displaying the sum of squares and degrees of freedom. To get a regular ANOVA output, you need to wrap the output in the summary function. # You can also make the output of aov() into an object, # and then call the summary() function on the object summary(aov(formula = BMI ~ Treatment + Gender, data = CSVimport)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 1 1.53 1.529 0.094 0.762 ## Gender 1 14.70 14.701 0.905 0.353 ## Residuals 19 308.53 16.238 Now we see the main effects, degrees of freedom, sum of squares, mean squares, F-value and P-values for the ANOVA. Let’s see an interaction. tice that we have a 2-way ANOVA, with main effects of Treatment and Gender regressed against BMI. You separate the exploratory variables with a + within the formula. Interactions are denoted with a :. An example of this is upcoming. Also note that the output is very minimal, only displaying the sum of squares and degrees of freedom. To get a regular ANOVA output, you need to wrap the output in the summary function. # Add another : for a three way interaction, if model necessitates summary(aov(formula = BMI ~ Treatment + Gender + Treatment:Gender, data = CSVimport)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 1 1.53 1.529 0.090 0.767 ## Gender 1 14.70 14.701 0.868 0.364 ## Treatment:Gender 1 3.63 3.627 0.214 0.649 ## Residuals 18 304.90 16.939 You could see how this could get tedious if you have more than a couple main effects, plus several interactions. Fortunately, R has a shortcut with *. As this is in a formula, it does not result in a multiplication operation. # Do not have to identify the &#39;formula&#39; argument name to correctly run the function. summary(aov(BMI ~ Treatment*Gender, data = CSVimport)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 1 1.53 1.529 0.090 0.767 ## Gender 1 14.70 14.701 0.868 0.364 ## Treatment:Gender 1 3.63 3.627 0.214 0.649 ## Residuals 18 304.90 16.939 Let’s look at another way to run an ANOVA. You may recall from your stats class that an ANOVA is really an extension of a linear model. Therefore, you can first fit a linear model with the lm function, using the exact same formula syntax in aov. Then use the anova function. Why the extra step? Well, there may be other uses for the linear model that might not be related to the ANOVA; however, we will not cover that here. # Look at the lm help page for more info regarding linear models in R linearMOD &lt;- lm(BMI ~ Treatment*Gender, data = CSVimport) # Do not need to use summary() function with this workflow. anova(linearMOD) ## Analysis of Variance Table ## ## Response: BMI ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 1 1.529 1.5291 0.0903 0.7673 ## Gender 1 14.701 14.7012 0.8679 0.3639 ## Treatment:Gender 1 3.627 3.6273 0.2141 0.6491 ## Residuals 18 304.901 16.9389 If you Google ANOVA analysis in R, you may come across some articles describing discordant results between either SAS or SPSS and R. This is for 2 reasons, 1) Each software calculates their underlying contrast structure slightly different, and 2) R defaults to sum of squares type I, vs sum or squares type III in SAS and SPSS. It does NOT mean that any of the results provided by these tools are incorrect by any means. It is up to us as users of these tools to understand the underlying principles and use them accordingly. Unfortunately, the ANOVA functions in the core packages do not provide alternative ways to calculate sum of squares. However, a mechanism does exist using the car package. # Install the package if needed and load the library # install.packages(&quot;car&quot;) library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## some ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode # Need to update the contrasts for type III sum of squares options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) # Now we can use the lm model from above in Anova in the car package and specify the sum of squares type # Note that this ANOVA function has a capital &#39;A&#39;. Anova(linearMOD, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: BMI ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 11584.9 1 683.9240 8.999e-16 *** ## Treatment 0.1 1 0.0055 0.9415 ## Gender 16.5 1 0.9721 0.3372 ## Treatment:Gender 3.6 1 0.2141 0.6491 ## Residuals 304.9 18 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Set contrasts back to original setting. options(contrasts = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) This output should match the default SAS output. The debate regarding which sum of squares is beyond this workshop and I will refer you to this PDF from Nancy Reid. 2.9.4 Kruskal-Wallis Test The Kruskal-Wallis test is the non-parametric equivlant to a 1-way anova. Same syntax used for kruskal.test and aov. # Do not need summary() function for kruskal.test(). kruskal.test(BMI ~ Treatment, data = CSVimport) ## ## Kruskal-Wallis rank sum test ## ## data: BMI by Treatment ## Kruskal-Wallis chi-squared = 0.13043, df = 1, p-value = 0.718 2.9.5 Post-hoc tests The TukeyHSD function is available without an add-on package and requires the output from the aov function. # We will fit the ANOVA first and set it into an object. TXTanova &lt;- aov(formula = BMI ~ Treatment + Gender, data = CSVimport) # Note the capital &#39;T&#39; in the TukeyHSD() function. TukeyHSD(TXTanova, &quot;Treatment&quot;) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = BMI ~ Treatment + Gender, data = CSVimport) ## ## $Treatment ## diff lwr upr p adj ## Experiment-Control -0.5272727 -4.123633 3.069088 0.7622845 "],
["chapt3intro.html", "Chapter 3 Microbial Sequencing Data 3.1 Structure 3.2 Files 3.3 Diversity 3.4 Challenges 3.5 Normalization 3.6 R packages 3.7 Additional resources 3.8 Final thoughts", " Chapter 3 Microbial Sequencing Data We were introduced to basic concepts of microbial sequencing data in Chapter 1, but let’s take a deeper dive into what kind of data we’re dealing with. Not going to lie, microbial sequencing data has several characteristics that are problematic for many statistical tools. Let’s use this chapter to discuss the characteristics of microbial sequencing data and the basic measurements that we can derive from it. We will be data from a rat model of diabetes that was recently published at the American Journal of Physiology (Piccolo et al., 2018, Am J Physiol Endocrinol Metab). This was a study investigating whether diabetes progression would alter the cecal content microbiota in UC-Davis Type 2 Diabetes Mellitus Rats, which spontaneously develop diabetes and maintain fully functional leptin signaling. Cecal content microbes from age-matched rats who either had not developed diabetes (PD), recently developed diabetes (RD), 3-months post onset of diabetes, and 6-months post onset of diabetes were assessed using 16S rRNA and shotgun metagenomics sequencing. Additionally, cecal contents from lean Sprague Dawley (LSD) rats were also analyzed. Rats were studied in either 2014 or 2016. For this chapter, Excel spreadsheets will be displayed to assist visualizations and we will use this data in future chapters. 3.1 Structure We discussed in Chapter 1.2 that microbial sequencing data from 16S rRNA amplicon and shotgun metagenomic sequencing are counts of sequencing reads. These sequencing reads could assigned to a unique identifier (e.g., 16S rRNA OTU number) or a bacterial species itself (shotgun metagenomics). Microbial sequencing data could be provided in a spreadsheet formula, where the rows are taxa and columns are samples. Typically, the first several columns will contain the taxonomy levels of each taxa (Figure 3.1). It is possible that you may receive the data as proportions (i.e., relative abundance) rather than raw counts. Although proportions are commonly used for data analysis, it means that the data has already been processed and limits several downstream analyses. Let’s cover what the minimum types of data you should receive when obtaining microbial sequencing data. Figure 3.1: Example of rRNA amplicon sequencing data. 3.1.1 Count data As described above, this is an example of your actual counts of sequencing reads. The sequence counts begin on column ‘I’ in Figure 3.1. Notice that all the counts are equal or above 0 and are whole numbers. The raw data should be provided at the lowest taxonomic level that the technology allows. For 16S rRNA sequencing data, this could be either Operational Taxonomic Units (OTU) or Sequence Variants. Note in Figure 3.1, that there is a column for Species and OTU. This is because 16S rRNA cannot provide species resolution. Shotgun metagenomics analysis should provide species level data. 3.1.2 Taxonomy data I like to think of microbial sequencing data as multi-leveled, because it can be assessed at various taxonomy levels (Figure 3.2). Generally, the most granular level is going to give you the best biological representation of the bacterial community structure, but higher taxonomy levels have been known to differ in certain conditions. For example, it has been well established that the phylum level Firmicutes are enriched at the expense of Bacteroidetes in obese rodents compared to their lean counterparts. Sometimes, the higher level analyses can provide different insights into your microbial data that may not be apparent in granular level assessments. Therefore, sequence counts in a particular taxonomy level can be summed up to higher levels. I also refer to any taxon unit within a particular phylogenetic level globally as a “taxa”, but these “taxa” will have a name within (i.e., circle or rectangle on Figure 3.2) as “taxa”. Figure 3.2: Example of Phylogenetic Tree from Phylum through Species. Plurals of each levels are in paranthesis. 3.1.3 Sample Metadata This is your experimental data. Must include sample IDs that match the sequencing count data labels. Figure 3.3: Example of rRNA amplicon sequencing data. 3.1.4 Phylogenetic tree Phylogenic data is not absolutely required, but encouraged. There are a couple well-used beta-diversity estimates (Unifrac) that require a phylogenic tree to calculate. We will not cover phylogenetic methods in this course, but additional resources are available. 3.2 Files 3.2.1 Spreadsheets It is actually more practical to import three separate spreadsheet-based files for Count, Taxonomy, and Sample Meta data into R. After these files are imported in R, they will be packaged together as an individual object; thus, keeping them as a separate files is helpful, but not necessary. Still, it is advised not to input a single Excel file formatted to containing all of these pieces in a single spreadsheet. As covered in Chapter 2.6.2, R will automatically import a vector as a character if it contains any non-numerical values. Best way would be to create 3 separate files, with a single header row. 3.2.2 Biome A Biome file is a Biological Observation Matrix (BIOM) file that contains both Count and Taxonomy data. It is a popular output file for QIIME1, and also can be produced by QIIME2. I highly recommend using this format because it is generated directly from the sequencing informatics pipeline and has little-to-no opportunity for random errors associated with spreadsheet handling and formatting. The biomformat R package was specifically designed to import biom files into R, so it is rather easy to import this file type. 3.3 Diversity You should always remember that microbial sequencing data is an estimate of a microbiota. Broadly defined, a microbiota is an ecological community of bacteria, archaea, viruses, fungi, and other micro-organisms. This microbial biodiveristy is dynamic and community members are working with or competing against other members for the limited available resources. Perturbations or alterations in the environment, e.g., changes in host health or alterations in energy substrates, can lead to major shifts in community biodiversity. Thus, it is important to understand diversity measurements, as these ecological measures provide estimates of biodiversity and how it varies between environments or as a response to pertubations. 3.3.1 Alpha-diversity Alpha-diversity is the biodiversity within a specified site (Figure 3.4). In terms of microbial sequencing data, this would be the microbial diversity within a sample. So, how many microbes were identified in a single fecal sample derived from a participant in your study, and what is the distribution of all microbes within this sample. Are all sequencing count totals similar between all microbes within a sample, or are there only a few microbes with much greater counts relative to the rest? These are some of the questions we can ask by estimating alpha-diversity. Generally, the estimates provide a measure of richness and evenness within a sample. There are MANY estimates of alpha-diversity, but we will go over a few of the most commonly used estimates in microbial studies. Figure 3.4: Example of ecological diversity; each letter represents an indivual site or sample. (A) rich and even, (B) rich, but not even, (c) not rich, but even, and (D) not rich and not even. 3.3.1.1 Total amount This is self explanatory. It is the number of observed taxa within a particular sample. Estimate of richness can be biased in microbial studies due to the high prevalence of low abundant taxa. 3.3.1.2 Chao1 Estimates total richness, but makes a corrective factor based on the presence of singletons and doubletons. In this context, singletons and doubletons refer to a species that was detected once or twice, respectively, within a sample. Thought to underestimate total richness if sample size is low. 3.3.1.3 Abundance-Based Coverage (ACE) Similar to Chao1, but correction factor is based on taxa with less than 10 counts. Like, Chao1, will underestimate richness with small sample sizes. 3.3.1.4 Shannon’s Diversity Index The Shannon index incorporates both species richness and evenness. It will increase as both richness and evenness increases. 3.3.1.5 Simpson’s Diversity Index Similar to the Shannon index, but incorporates species richness as a proporation and dominance. If there is a dominant taxa, e.g., very high reads relative to others, then evenness is decreased. If dominance is present, then the Simpson Index will be lower. 3.3.2 Beta-diversity Beta-diversity refers to the variation between sites. For example, how does the community of microbes in a fecal sample in one person differ from communites within all other fecal samples collected from other people. How does the microbial diversity in a sample collected from your armpit differ then the biodiveristy in sample collected from between your toes? Thus, beta-diversity measures the between sample variation in your samples. Estimates of bio-diversity consist of distance-, dissimilarity-, or phylogentic-based calculations. These calculations are based on the comparison of 2 samples, then calculated across all pairs of samples. This results in a symmetrical matrix with a diagonal containing zeros or ones (Figure 3.5). Like alpha-diversity, there are many, many ways to estimate beta-diversity. We will focus on a few here. Figure 3.5: Example of Bray-Curtis Dissimilarity matrix output. 3.3.2.1 Bray-Curtis Dissimilarity One of the most popular diversity indices. Calculated by summing the absolute differences between counts and then dividing by the abundances between the two samples. Sometimes incorrectly referred to as a distance, but it violates a certain property of distances. Thus, it should always be referenced as a “dissimilarity” measure. 3.3.2.2 Jaccard Index Measures diversity based on presence/absence. More specifically, the Jaccard index is based on the co-presence of a taxa relative to the number of taxa present in at least one of the two comparative samples. Think of a Venn Diagram. The Jaccard index is essentially measuring the amount within the overlapping circles relative to the amount outside of the overlap. 3.3.2.3 Ordinations This is not a measure of beta-diversity, but are multivariate statistical procedures that summarise a high-dimensional dataset into a new set of variables that can be visualized in a 2- or 3-dimensional plot. Principal components analysis (PCA) is an example of an ordination. PCA uses a covariance- or correlation-matrix, but a Principle Co-ordinante Analysis (PCoA) is a PCA that uses a distance or dissimilarity matrix instead. Another type of ordination is multi-dimensional scaling (MDS): MDS tries to orient samples within a specified lower dimensional space that bests replicates the observed distances between samples. There is a best-fit algorithm that measures how congruent (or not) the distance fit is and will stop at a specified criteria. There are others, but we will only cover PCA, PCoA and non-multi dimensional scaling (NMDS) in this workshop. 3.4 Challenges I alluded to some difficult properties associated with microbial sequencing data earlier in this chapter (3). Let’s take some time to understand these properties. 3.4.1 Compositional data It is becoming more appreciated that microbial sequencing data is compositional data, which invalidates covariance- or correlation-based statistical techniques. Compositional data refers to data which the total amount is arbitrary (meaningless) or each individual component sums to an arbitrary or equal amount. How does microbial sequencing data fall into this space? Let’s consider what the data actually are: They are counts of sequencing reads. Thus, it does not exactly fit to what we are trying to measure (a microbial organism). Additionally, the counts are finite based on the flow cell. Thus, counts will sum to whatever the limit of the flow cell was to produce the data. Furthermore, since there is a finite amount of reads, removing a taxa from a sample would mean that some other taxa would have to have a higher count. The good thing about sequencing data is that if you increase a certain bacteria, you are also going to increase the genetic material. So sequencing does track changes in abundance; however, it does not provide an accurate measurement of the total number of taxa. And that is the inherent limitations of the technology, that it will always be proportional based on the amount of reads available for sequencing. A recent paper by Morton et al (Nature Communications, 2019) provided an excellent example of the drawback to abundance data (Figure 3.6). Briefly, Morton et al describes the fact that proportional differences may show the same differences even in cases where there are severe differences in the total microbial load between experimental groups. It has also been recognized for quite sometime that microbial sequencing data does not contain any information regarding the actual microbial load, thus, it is common to transform microbial sequencing data into a proportions where data is fixed between 0 and 1 within a sample. Multiplying by 100 will give relative percentages. We are now clearly in a compositional space, where all samples have fixed and arbitrary sum. Luckily, there are several transformations, e.g., the centered log-ratio and additive log ratio transformations, that can convert compositional data back to an euclidian space that makes the data more amendable for traditional statistical approaches. However, there is still one more challenge to overcome that must be dealt with before you can transform your data with these procedures… 3.4.2 Zero inflation If you refer back to Figure 3.1, you’ll notice there is one number that is more frequent than the others. Zeros! Microbial abundance is full of low abundant taxa and zeros are extremely prevalent. It is a huge statistical hurdle, and unfortunately, I do not have a good answer for this limitation. Generally, we will use a filtering criteria to remove low abundant taxa. This normally takes some form of either removing taxa that have a certain percentage of counts at a defined level, say 10 counts in less than 50% of all samples will be removed. Or, determine the proportional abundance and then remove taxa that have an average abundance &lt; 0.1%. By no means is there a consistent or agreed upon criteria to do this. You may have to consider you group as well. What if you’re assessing the gut microbiota in infants fed breastmilk vs 2 different formulas with different sources. You may have several microbes that are found in one diet but not in the others. So, would you say you needs reads to appear in 33% of your samples because you have 3 groups? But not all infants may seed a particular bacteria, so then do you make your cutoff at 50% of your lowest sample size? Now you’re adding taxa that may only be identified in 16-17% of your total sample size. What if you have 5 diets? As you can see, there is no good answer and it may depend on your study. How do we deal with zeros? A common technique is to add a small small constant to all samples to ensure there are no zeros, and then apply the transformations. It also has been suggested to impute zeros using a Bayesian-multiplicative replacement method. We will cover these transformations in the next chapter. 3.5 Normalization Variation due technical aspects of the analysis (e.g., sample collection, differences in bacterial load, and uneven sampling depth) is common in microbial sequencing data. We have already discussed one normalization technique to combat these effects (proportional transformation), but there are several others that are used. We will focus on some of the more straight foward normalization techniques. Please refer to Weiss et al Microbiome, 2017 for a more in depth description of these and others. 3.5.1 Rarefying This is the process of randomly removing counts from samples until all sample depths are even. Remember, sampling depth refers to the total number of counts across all taxa within a particular sample. So, rarefying equalizes the sample depth among all samples, but the drawback is you lose quite a bit of sensitivity due to the significant amount of data. It also does not combat the compositional structure of the data and it may contibute to zero inflation in some cases. Rarefy has been suggested for comparisons among samples with known and large differences between samples, e.g., colon vs duodenumn samples. Some have strongly advised against rarefying, but others suggest it is suitable for beta-analysis and does not increase false discoveries associated with differential analyses. 3.5.2 Log upper quartile Similar to a proportional abundance, but each sample is divide by its 75th percentile of its count distribution rather than the sample sum. The scaled data is then log transformed. Like proportional scaling, ignores differences in sampling depth. Log transformation means that special handling of zeros needs to be considered. 3.6 R packages We will mainly use ecology/microbiota based R packages for this workshop, vegan, phyloseq, ape, and microbiome. 3.6.1 vegan The vegan package contains is the major community ecology package in R and is maintained by several ecologists. While there are others ecology R packags that may have more specialized functions, vegan has the widest array of ecology based functions. In fact, some of the other packages we’ll use heavily rely on functions from this packages. 3.6.2 phyloseq Developed in Susan Holmes’ lab at Stanford by Paul McMurdie. phyloseq is the most well-developed R package for complete handling of microbial sequencing data. This group also maintains a very nice website for phyloseq and host several additional tutorials that contain topics not covered in this workshop. 3.6.3 microbiome Written by Leo Lahti and Sudarshan Shetty. Contains functions that extends phyloseq’s functionalities. 3.6.4 ape The ape package is major package for phylogenetic based analyses. It is maintained by a large number of authors and has a wide range of functions for data analysis in a phylogentic framework. We will only be using this package for Principle Co-ordinate Analysis (PCoA). 3.7 Additional resources There are several online resources that provide data visualization and analyses for microbial sequencing data. I am more familiar with some, but not all. The goal here is to provide alternative workflows for your convenience, especially for those that are more comfortable with graphical user interfaces (i.e., not coding based interfaces like R). While many of these resources are very good at what they do, most will not integrate non-microbial data in your analysis and most do not produce scripts to track your analysis steps. QIIME2 has now integrated analysis specific plug-ins to their informatics environment to become a 1-stop tool for all 16S rRNA informatics needs. If you’re already using QIIME2, then most of the topics we cover here will be available in QIIME2. It is built in the Linux environment, so Windows users may not be able to access without a virtual box. Bottom line, you need to be familiar with the Linux environment. I have authored a free Shiny app called DAME that allows you to upload a Biom file and dynamically assess alpha- and beta-diversity measurements, and differential abundance between individual taxa. It is only compatible with Biom files derived from QIIME1. The authors of phyloseq have also developed a Shiny app that is freely available, called Shiny-phyloseq. Calypso is another web-based software that is freely available. Files derived from QIIME and MOTHUR pipelines can be uploaded into Calypso for visualization and data analyses. MicrobiomeAnalyst is another freely available web-based resources, developed by the same authors of the popular metabolomics web resource, MetaboAnalyst. 3.8 Final thoughts We will cover the minimum analyses needed to publish microbial taxonomy data in this workshop. Chapter 4 will detail importing microbial sequencing data into R and preparing the data for statistical analysis. Chapters 5, 6, and 7 will cover alpha-diversity, beta-diversity, and differential analysis, respectively. By no means does this cover all of the potential analyses available for microbial sequencing data, but will cover the minimum analyses needed to publish data derived from comparitive studies. I will provide examples in the subsequent chapters that I think you will encounter if using R. It may seem too detailed for a relatively short workshop, but I’d like to make sure that you have sufficient information for some troubleshooting. "],
["importing-and-pre-processing.html", "Chapter 4 Importing and Pre-Processing 4.1 Importing Data into R 4.2 phyloseq object 4.3 Pre-processing 4.4 Conclusion", " Chapter 4 Importing and Pre-Processing This chapter will focus on the importing microbial sequencing data in R, setting up a phyloseq object, and preparing the data for statistical analysis. 4.1 Importing Data into R As detailed in Chapter 3.2, you may receive your sequencing data as a biom file or in a text-delimited/spreadsheet format. Let’s detail how each can be imported into R 4.1.1 Biom file If you recall from Chapter 3.2.2, this file format will contain the count data and taxonomy data in a single file. We will use the read_biom function from the biomformat package. The biomformat package is located at Bioconductor and can be installed using the following code # Remove hashtags prior to running in your console. # if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) # install.packages(&quot;BiocManager&quot;) # BiocManager::install(&quot;biomformat&quot;) We will use the ‘UCDT2DM16S.biom’ file for this example. Make sure to set your working directory to the directory containing your biom file. Let’s load the package and use the read_biom function to import into Remove # You will not be able to import your biom file unless you tell R where to look # Remove hashtag from this code and add your directory # setwd(&quot;C:\\\\Users\\\\name\\\\documents\\\\FSUworkshop\\\\&quot;) # Place all of your loaded libraries at the top of your script library(biomformat) Biom &lt;- read_biom(&quot;.\\\\Data\\\\UCDT2DM16S.biom&quot;) Biom ## biom object. ## type: OTU table ## matrix_type: dense ## 50539 rows and 55 columns The output gives us a little bit of information; the most important of which is the number of rows and columns. This is the dimension of the count table. Rows are taxa and columns are samples. Thus, we have &gt;55,000 taxa and 56 samples. We need to extract the count and taxonomy tables and place them in their own objects. Both the count and taxonomy tables are in a format we cannot use. We will use the biom_data and observation_metadata function to extract the count and taxonomy data, respectively. Then, we will need to convert it to a matrix using the as.matrix function. We will use the %&gt;% pipe operator to link functions when available. # Load the tidyverse package if not already loaded # library(tidyverse) ## Extract OTU table and convert to a matrix CountMatrix &lt;- biom_data(Biom) %&gt;% as.matrix() ## Extract taxonomy data and convert to a matrix TaxaMatrix &lt;- observation_metadata(Biom) %&gt;% as.matrix() Let’s use the head function to see the first 6 rows of the taxonomy data # The tails() function does the opposite of the head() function head(TaxaMatrix) ## taxonomy1 taxonomy2 taxonomy3 ## 366623 &quot;k__Bacteria&quot; &quot;p__Firmicutes&quot; &quot;c__Clostridia&quot; ## 182771 &quot;k__Bacteria&quot; &quot;p__Verrucomicrobia&quot; &quot;c__Verrucomicrobiae&quot; ## 1800048 &quot;k__Bacteria&quot; &quot;p__Bacteroidetes&quot; &quot;c__Bacteroidia&quot; ## 276629 &quot;k__Bacteria&quot; &quot;p__Bacteroidetes&quot; &quot;c__Bacteroidia&quot; ## 206494 &quot;k__Bacteria&quot; &quot;p__Firmicutes&quot; &quot;c__Clostridia&quot; ## 22466 &quot;k__Bacteria&quot; &quot;p__Bacteroidetes&quot; &quot;c__Bacteroidia&quot; ## taxonomy4 taxonomy5 ## 366623 &quot;o__Clostridiales&quot; &quot;f__Lachnospiraceae&quot; ## 182771 &quot;o__Verrucomicrobiales&quot; &quot;f__Verrucomicrobiaceae&quot; ## 1800048 &quot;o__Bacteroidales&quot; &quot;f__Porphyromonadaceae&quot; ## 276629 &quot;o__Bacteroidales&quot; &quot;f__S24-7&quot; ## 206494 &quot;o__Clostridiales&quot; &quot;f__Lachnospiraceae&quot; ## 22466 &quot;o__Bacteroidales&quot; &quot;f__Prevotellaceae&quot; ## taxonomy6 taxonomy7 ## 366623 &quot;g__Coprococcus&quot; &quot;s__&quot; ## 182771 &quot;g__Akkermansia&quot; &quot;s__muciniphila&quot; ## 1800048 &quot;g__Parabacteroides&quot; &quot;s__&quot; ## 276629 &quot;g__&quot; &quot;s__&quot; ## 206494 &quot;g__&quot; &quot;s__&quot; ## 22466 &quot;g__Prevotella&quot; &quot;s__&quot; In this case, the column names (“taxonomy1”, “taxanomy2”, etc.) could be updated to reflect the taxonomy level. Note the prefixes of each character within the columns. In these data, this references the taxonomy level. These prefixes may not always appear and they can be removed if absolutely necessary. Let’s just update the column names so we know which taxonomy level belongs to which column. # We will make an object with the taxonomy levels incase we need to use it later TaxaLevels &lt;- c(&quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) # Overwrite column names colnames(TaxaMatrix) &lt;- TaxaLevels # Look at data again head(TaxaMatrix) ## Kingdom Phylum Class ## 366623 &quot;k__Bacteria&quot; &quot;p__Firmicutes&quot; &quot;c__Clostridia&quot; ## 182771 &quot;k__Bacteria&quot; &quot;p__Verrucomicrobia&quot; &quot;c__Verrucomicrobiae&quot; ## 1800048 &quot;k__Bacteria&quot; &quot;p__Bacteroidetes&quot; &quot;c__Bacteroidia&quot; ## 276629 &quot;k__Bacteria&quot; &quot;p__Bacteroidetes&quot; &quot;c__Bacteroidia&quot; ## 206494 &quot;k__Bacteria&quot; &quot;p__Firmicutes&quot; &quot;c__Clostridia&quot; ## 22466 &quot;k__Bacteria&quot; &quot;p__Bacteroidetes&quot; &quot;c__Bacteroidia&quot; ## Order Family ## 366623 &quot;o__Clostridiales&quot; &quot;f__Lachnospiraceae&quot; ## 182771 &quot;o__Verrucomicrobiales&quot; &quot;f__Verrucomicrobiaceae&quot; ## 1800048 &quot;o__Bacteroidales&quot; &quot;f__Porphyromonadaceae&quot; ## 276629 &quot;o__Bacteroidales&quot; &quot;f__S24-7&quot; ## 206494 &quot;o__Clostridiales&quot; &quot;f__Lachnospiraceae&quot; ## 22466 &quot;o__Bacteroidales&quot; &quot;f__Prevotellaceae&quot; ## Genus Species ## 366623 &quot;g__Coprococcus&quot; &quot;s__&quot; ## 182771 &quot;g__Akkermansia&quot; &quot;s__muciniphila&quot; ## 1800048 &quot;g__Parabacteroides&quot; &quot;s__&quot; ## 276629 &quot;g__&quot; &quot;s__&quot; ## 206494 &quot;g__&quot; &quot;s__&quot; ## 22466 &quot;g__Prevotella&quot; &quot;s__&quot; 4.1.2 Excel A little more straight-foward then Biom files, but can be variable depending on how the data is presented in the spreadsheet. It is important to only have the first row in the spreadsheet as column headers with no empty cells as shown in Figure 3.1 (Chapter 3.1). We will use the example where the taxonomy data is combined with the count data. The same data is found in the biom and Excel files, so we will overwrite the biom objects. # Load the readxl package if necessary. # library(readxl) # Need to specify data type if numbers and characters are mixed. See help page for read_excel(). Excel &lt;- read_xlsx(&quot;.\\\\Data\\\\UCDT2DM16SExcel.xlsx&quot;, col_types=c(rep(&quot;text&quot;, 8), rep(&quot;numeric&quot;, 55))) %&gt;% as.data.frame() # The OTU column are the row names in the biom file. Will set to be similar with other workflow rownames(Excel) &lt;- Excel$OTU # Remove OTU column Excel$OTU &lt;- NULL # Make new object with taxonomy column labels if necessary. # We will use the Taxonomy object we made earlier to subset the taxonomy data TaxaData &lt;- Excel[, colnames(Excel) %in% TaxaLevels] # Convert to a matrix TaxaMatrix &lt;- TaxaData %&gt;% as.matrix() # Use &#39;!()&#39; to reverse logical statements CountData &lt;- Excel[, !(colnames(Excel) %in% TaxaLevels)] CountMatrix &lt;- CountData %&gt;% as.matrix() # Let&#39;s look at new TaxaMatrix object head(TaxaMatrix) ## Kingdom Phylum Class ## 366623 &quot;k__Bacteria&quot; &quot;p__Firmicutes&quot; &quot;c__Clostridia&quot; ## 182771 &quot;k__Bacteria&quot; &quot;p__Verrucomicrobia&quot; &quot;c__Verrucomicrobiae&quot; ## 1800048 &quot;k__Bacteria&quot; &quot;p__Bacteroidetes&quot; &quot;c__Bacteroidia&quot; ## 276629 &quot;k__Bacteria&quot; &quot;p__Bacteroidetes&quot; &quot;c__Bacteroidia&quot; ## 206494 &quot;k__Bacteria&quot; &quot;p__Firmicutes&quot; &quot;c__Clostridia&quot; ## 22466 &quot;k__Bacteria&quot; &quot;p__Bacteroidetes&quot; &quot;c__Bacteroidia&quot; ## Order Family ## 366623 &quot;o__Clostridiales&quot; &quot;f__Lachnospiraceae&quot; ## 182771 &quot;o__Verrucomicrobiales&quot; &quot;f__Verrucomicrobiaceae&quot; ## 1800048 &quot;o__Bacteroidales&quot; &quot;f__Porphyromonadaceae&quot; ## 276629 &quot;o__Bacteroidales&quot; &quot;f__S24-7&quot; ## 206494 &quot;o__Clostridiales&quot; &quot;f__Lachnospiraceae&quot; ## 22466 &quot;o__Bacteroidales&quot; &quot;f__Prevotellaceae&quot; ## Genus Species ## 366623 &quot;g__Coprococcus&quot; &quot;s__&quot; ## 182771 &quot;g__Akkermansia&quot; &quot;s__muciniphila&quot; ## 1800048 &quot;g__Parabacteroides&quot; &quot;s__&quot; ## 276629 &quot;g__&quot; &quot;s__&quot; ## 206494 &quot;g__&quot; &quot;s__&quot; ## 22466 &quot;g__Prevotella&quot; &quot;s__&quot; 4.1.3 Metadata We will assume the metadata will be in Excel format. You will need to ensure that your sample IDs exactly match the IDs on your count file. For those using a Biome file, you would have supplied the sample labels within your informatics pipeline (e.g., QIIME) to generate these labels. It is advised to use this file to ensure matching labels. # Load the readxl package if necessary. # library(readxl) # Import Excel file and coerce to data frame Metadata &lt;- read_xlsx(&quot;.\\\\Data\\\\UCDT2DMmetadata.xlsx&quot;) %&gt;% as.data.frame() # Set row names with column that matches the labels in your count data # In this case, it is the column labeled &quot;SampleID&quot;. # This is required to create a phyloseq object. rownames(Metadata) &lt;- Metadata$SampleID # Look at data head(Metadata) ## SampleID Rat GroupAbbrev Collection ## BP.H1138 BP.H1138 H1138 LSD Y16 ## BP.H1145 BP.H1145 H1145 LSD Y16 ## BP.H1147 BP.H1147 H1147 LSD Y16 ## BP.H1156 BP.H1156 H1156 LSD Y16 ## BP.H1157 BP.H1157 H1157 LSD Y16 ## BP.H1166 BP.H1166 H1166 LSD Y16 How can we ensure the names match between the count and metadata file? We can extract and compare the column names from the CountMatrix object and the vector named “SampleID” from the Metadata object. The == logical operator will match in order, so we need to ensure that both are in order. # Extract column names CountNames &lt;- colnames(CountData) # Extract &#39;SampleID&#39; file. The label, &quot;SampleID&quot;, was the name I provided, but yours may be different. MetaIDs &lt;- Metadata$SampleID # Sort both CountNameSort &lt;- sort(CountNames) MetaIDsSort &lt;- sort(MetaIDs) CountNameSort == MetaIDsSort ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [15] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [29] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [43] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE Note that the output using == is a long vector of logical responses. You could imagine having a long vector of many TRUE and not knowing if a FALSE is nested somewhere in between. There is a handy function that I use to determine if all elements within a vector are the same. It is the setequal function. setequal require 2 arguments, the first is the vector, and the second is the comparison vector. If the 2 vectors are exactly the same, then it will return a TRUE. If there is any difference, it will return a FALSE. You may refer to chapter 1.2 to refresh yourself with the concept of recycling if this function does not seem intuitive. # Make object of logical output NameCompLogical &lt;- CountNameSort == MetaIDsSort # Use TRUE as comparison vector (TRUE is recycled!) in the setequal() function. setequal(NameCompLogical, TRUE) ## [1] TRUE We get a TRUE returned, so our data is now reconciled. The following section describes the steps needed to identify samples that are missing in your metadata file. This is not necessary to finish the workflow and can be skipped. What would you do if they do not match? We will look at an example where your metadata is missing samples relative to your count data. This is more likely to occur if you are using a Biom file and not using the metadata file used in your informatics pipeline. Thus, an example will be provided that demonstrates steps to identify the missing samples. # Make arbitrary sample ID vector with missing data. The first 5 elements will be removed. MetaIDsSort_missing &lt;- MetaIDsSort[-(1:5)] # The %in% operator will match any two vectors. Combine with !() to identify elements that mismatch. # We want to know which names from the count data do not have a match in the SampleID column CountNameSort[!(CountNameSort %in% MetaIDsSort_missing)] ## [1] &quot;BP.H1138&quot; &quot;BP.H1145&quot; &quot;BP.H1147&quot; &quot;BP.H1156&quot; &quot;BP.H1157&quot; This provides an output of the labels from the count data that are not found within the sample metadata. From here, you will need to investigate your metadata file to determine where the discrepancy is. 4.2 phyloseq object A phyloseq object is a convenient way to package all of the data into a single object that facilites common tasks required for microbial community data. The benefit of this is you can apply a single function to the phyloseq object, which will alter all files within it simultaneously. Let’s say, for example, you want to analyze your data at the phylum level. The phyloseq package has a single function that will sum all of your counts to the phylum taxonomy and adjust the taxonomy file accordingly. Now imagine having to do this without all of the files packaged together. 4.2.1 Setting up a phyloseq object I realize it seems a little burdensome to split all of your data out and then package them back together in a new format. However, once you get it packaged into a single object, then things become much easier. The 3 objects we’ve created must match or the phyloseq function will reject them. The phyloseq function uses row and column names to match each data object. They do not have to be in order, but each data set must have an exact pair in the comparative object. The following objects should match each other: Count data row names -&gt; Taxa data row names. Count data column names -&gt; Sample metadata row names We are using the Excel import, so the files should match since the we set the row names of the Excel object and then split the taxonomic and count data into their own objects. The second match was done above in 4.1.3. Still, the first match will be shown for the sake of completeness. # Extract row names from count data Count_taxauniqueID &lt;- rownames(CountData) # Extract row names from taxa data Taxa_taxauniqueID &lt;- rownames(CountData) # Sort both Count_taxauniqueIDSort &lt;- sort(Count_taxauniqueID) Taxa_taxauniqueIDSort &lt;- sort(Taxa_taxauniqueID) # Make object of logical output using == CountTaxaCompLogical &lt;- Count_taxauniqueIDSort == Taxa_taxauniqueIDSort # Use TRUE as comparison vector (TRUE is recycled!) in the setequal() function. setequal(CountTaxaCompLogical, TRUE) ## [1] TRUE We can now create the phyloseq object now that row and column names match. # phyloseq requires the data be in a specific class before creating &#39;phyloseq&#39; object. library(phyloseq) # Convert OTU data into phyloseq otu_table class otuTABLE &lt;- otu_table(CountMatrix, taxa_are_rows = TRUE) # Convert taxa data into phyloseq taxonomyTable class taxTABLE &lt;- tax_table(TaxaMatrix) # Convert Sample Metadata data frame into phyloseq sample_data class sampleDATA &lt;- sample_data(Metadata) # Create phyloseq object phylo_obj &lt;- phyloseq(otuTABLE, taxTABLE, sampleDATA) phylo_obj ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 50539 taxa and 55 samples ] ## sample_data() Sample Data: [ 55 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 50539 taxa by 7 taxonomic ranks ] Lets discuss the phylo_obj output. Very compact with 3 rows and 3 columns of information. The rows provide information relating to the OTU data (what we’ve been referring to as “count” data), Sample Data (our experimental metadata), and the Taxonomy data. The first column of the output describes the helper functions to extract the data, the second column is the row descriptor, and the last column provides the stats for each row. Note that we will now use the term, “OTU table”, to refer to the sequencing data because the count data will be modified with transformed/normalized etc. 4.2.2 Extracting data phyloseq objects are built in a special coding system called S4. It is slightly different then standard R, but I have found it cumbersome to extract data from these objects. So we will try to avoid the extractor functions provided in the phyloseq output when necessary. Fortunately, the ‘microbiome’ package has functions that make this extraction very easy. # load library library(microbiome) ## ## microbiome R package (microbiome.github.com) ## ## ## ## Copyright (C) 2011-2019 Leo Lahti, ## Sudarshan Shetty et al. &lt;microbiome.github.io&gt; ## ## Attaching package: &#39;microbiome&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha ## The following object is masked from &#39;package:base&#39;: ## ## transform # Extract OTU table from OTUdata &lt;- abundances(phylo_obj) # Will show an abbreviated output head(OTUdata[,1:15]) ## BP.K2491 BP.H931 BP.H933 BP.H1147 BP.H1157 BP.K2503 BP.P8163 ## 366623 1 1 1 0 0 0 0 ## 182771 0 0 0 1 1 3 0 ## 1800048 0 0 0 0 0 0 2 ## 276629 0 0 0 5 0 0 0 ## 206494 0 0 0 0 0 0 0 ## 22466 1 0 4 0 0 0 0 ## BP.H1166 BP.P7611 BP.P7609 BP.H937 BP.P7589 BP.P9227 BP.P9257 ## 366623 0 0 0 0 0 0 0 ## 182771 0 0 0 0 0 0 0 ## 1800048 2 3 2 1 5 2 1 ## 276629 0 0 0 0 2 1 0 ## 206494 0 0 0 0 0 0 0 ## 22466 3 0 0 1 1 0 0 ## BP.P9259 ## 366623 0 ## 182771 0 ## 1800048 2 ## 276629 1 ## 206494 0 ## 22466 0 # Extract Sample Metadata SampleData &lt;- meta(phylo_obj) head(SampleData) ## SampleID Rat GroupAbbrev Collection ## BP.K2491 BP.K2491 K2491 D6M Y14 ## BP.H931 BP.H931 H931 LSD Y14 ## BP.H933 BP.H933 H933 LSD Y14 ## BP.H1147 BP.H1147 H1147 LSD Y16 ## BP.H1157 BP.H1157 H1157 LSD Y16 ## BP.K2503 BP.K2503 K2503 D6M Y14 # microbiome package does not have an extractor function for taxonomy data # Will provide coding to extract here TAXAData &lt;- as.data.frame(tax_table(phylo_obj)@.Data) head(TAXAData) ## Kingdom Phylum Class ## 366623 k__Bacteria p__Firmicutes c__Clostridia ## 182771 k__Bacteria p__Verrucomicrobia c__Verrucomicrobiae ## 1800048 k__Bacteria p__Bacteroidetes c__Bacteroidia ## 276629 k__Bacteria p__Bacteroidetes c__Bacteroidia ## 206494 k__Bacteria p__Firmicutes c__Clostridia ## 22466 k__Bacteria p__Bacteroidetes c__Bacteroidia ## Order Family Genus ## 366623 o__Clostridiales f__Lachnospiraceae g__Coprococcus ## 182771 o__Verrucomicrobiales f__Verrucomicrobiaceae g__Akkermansia ## 1800048 o__Bacteroidales f__Porphyromonadaceae g__Parabacteroides ## 276629 o__Bacteroidales f__S24-7 g__ ## 206494 o__Clostridiales f__Lachnospiraceae g__ ## 22466 o__Bacteroidales f__Prevotellaceae g__Prevotella ## Species ## 366623 s__ ## 182771 s__muciniphila ## 1800048 s__ ## 276629 s__ ## 206494 s__ ## 22466 s__ Why do we have so many objects with the same information? So far, it has been redundant, but the value of knowing the extractor functions will become clear in the following sections. In addition, several differential analyses will not work with a phyloseq object and requires the data to be extracted. 4.2.3 Subsetting The prune_samples function will subset samples based on a logical expression that matches the number of samples within the phyloseq object. The function will automatically determine if your expression fits within the Sample Metadata or OTU Table. Let’s go through a few examples. Pay attention to the sample number in the phyloseq output. 4.2.3.1 Metadata Rats in the UCD-T2DM study were collected in 2014 and in 2016. We’re worried about a potential batch effect. Thus, we wish to remove 2014 samples and only examine 2016 samples. We can extract the metadata and define a logical statement to keep the rats sampled in 2016. # We extracted the metadata in the above chapter. CollectionYear &lt;- SampleData$Collection # Make the logical statement using the %in% (match) operator CollectionYearLogic &lt;- CollectionYear %in% &quot;Y16&quot; CollectionYearLogic ## [1] FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## [12] FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE FALSE ## [23] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE FALSE TRUE ## [34] FALSE TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE TRUE ## [45] FALSE FALSE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE # Use the prune_samples() function # First argument is the logical statement, the second is the phyloseq object phylo_obj2016 &lt;- prune_samples(CollectionYearLogic, phylo_obj) # This coding 2 lines below works as well, minus the hashtag. Why? # The curly brackets help contain the statement. # prune_samples({meta(phylo_obj)$Collection %in% &quot;Y16&quot;}, phylo_obj) # The &#39;pruned&#39; phyloseq object phylo_obj2016 ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 50539 taxa and 33 samples ] ## sample_data() Sample Data: [ 33 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 50539 taxa by 7 taxonomic ranks ] While the lean Sprague Dawley rats were used to compare metabolic differences between the diabetic animals, they may not be appropriate control animal for the microbiota study due to genetic influence on the composition of the gut microbiota. We need to remove this group from our 2016 object. We have a problem though. We cannot use the SampleData object derived from our initial phyloseq object, phylo_obj. phylo_obj has more samples then phylo_obj2016. We need to make sure to extract the Sample Metadata from the appropriate phyloseq object. # Extract the metadata from phylo_obj2016 Group2016 &lt;- meta(phylo_obj2016)$GroupAbbrev # Make the logical statement using the != (does not equal) operator Group2016Logic &lt;- Group2016 != &quot;LSD&quot; # Use the prune_samples() function # First argument is the logical statement, the second is the phyloseq object phylo_obj2016_noLSD &lt;- prune_samples(Group2016Logic, phylo_obj2016) # The &#39;pruned&#39; phyloseq object phylo_obj2016_noLSD ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 50539 taxa and 26 samples ] ## sample_data() Sample Data: [ 26 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 50539 taxa by 7 taxonomic ranks ] prune_sample will also match the row names in the Sample Metadata. We can use the sample_names function to select a batch of samples and filter on these selections. # Extract sample names from the phyloseq object phyloSampNames &lt;- sample_names(phylo_obj2016) # Use the first 20 samples phyloSampNames20 &lt;- phyloSampNames[1:20] # First argument is the names vector, the second is the phyloseq object phylo_obj20 &lt;- prune_samples(phyloSampNames20, phylo_obj2016) # The &#39;pruned&#39; phyloseq object phylo_obj20 ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 50539 taxa and 20 samples ] ## sample_data() Sample Data: [ 20 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 50539 taxa by 7 taxonomic ranks ] Beware, once you subset a phyloseq object, there is no going back. Thus, it is important to make a new object. 4.2.3.2 Sample depth We can also subset samples based on the characteristics of the OTU table. An important example of this is when we check sample depths. As mentioned in Chapter 1.2, it is important to have enough sequencing depth to ensure adequate coverage of the microbial community. Some suggest 1000 reads are adequate to see differences in large populations. Some have suggested 5000-10,000 reads are more than enough for 16S. I typically look at the depths and see if any are well of the distribution of other samples and only remove samples if way off. Fortunately, the sample set we’re using does not have any samples that were inadequately sequenced, but let’s look at how we would can identify sample depths per sample, followed by a removal. The prune_samples function will also work in this case. Why? It is because we’re determining the sample depths for each sample. Thus, any logical vector that is the length of the number of samples will cause prune_samples to filter the phyloseq object. Doesn’t matter if you derive it from the Sample Metadata or OTU table. # The sample_sums() function will calculate sample depths from your phyloseq object phyloSampDepth &lt;- sample_sums(phylo_obj) phyloSampDepth ## BP.K2491 BP.H931 BP.H933 BP.H1147 BP.H1157 BP.K2503 BP.P8163 BP.H1166 ## 57866 38803 49944 45749 48164 57676 33797 56259 ## BP.P7611 BP.P7609 BP.H937 BP.P7589 BP.P9227 BP.P9257 BP.P9259 BP.P9255 ## 49353 43076 34969 68237 44791 41221 47160 55334 ## BP.P9395 BP.P9303 BP.P8113 BP.P9295 BP.P9335 BP.P8287 BP.P9393 BP.P9223 ## 43180 51844 46863 49847 46007 45662 36846 46585 ## BP.P9263 BP.P9291 BP.H1156 BP.P9375 BP.K4153 BP.H935 BP.P9285 BP.P7603 ## 39094 24610 54457 39072 51542 33684 55471 42933 ## BP.P9387 BP.P7563 BP.H1167 BP.H939 BP.P7573 BP.P9261 BP.P9327 BP.K2513 ## 40971 39258 51734 42184 43707 49496 34724 45519 ## BP.P8153 BP.P8117 BP.P9251 BP.P9329 BP.P7561 BP.P7949 BP.P9225 BP.P9323 ## 40491 53924 58912 41702 40078 46910 46410 49909 ## BP.P8259 BP.K4206 BP.H1145 BP.H1138 BP.P9243 BP.K4164 BP.P9253 ## 45304 54215 43794 39588 39867 48852 38109 # What is our total sample depth? sum(phyloSampDepth) ## [1] 2515754 # Let&#39;s filter depths &lt; 40,000. ***DO NOT USE THIS AS A CUTOFF GUIDE - FOR PURPOSE OF EXAMPLE ONLY*** # We want to keep samples that have sequence depths greater than 30000 Depth40K &lt;- phyloSampDepth &gt; 40000 # Use the prune_samples() function # First argument is the logical statement, the second is the phyloseq object phylo40K &lt;- prune_samples(Depth40K, phylo_obj) # The &#39;pruned&#39; phyloseq object phylo40K ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 50539 taxa and 42 samples ] ## sample_data() Sample Data: [ 42 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 50539 taxa by 7 taxonomic ranks ] We are not covering rarefaction in this workshop, but it is a good idea to know about it. It addresses whether you have more taxa in a sample due to your sampling depth. You may come across a rarefraction curve when dealing with microbial sequencing data because it helps determine whether a sample is sequenced deep enough. It is computationally expensive because you have to rarify your samples to various depth and measure how many taxa are remaining. So the curves generally have a steep rise at low depths and plateau at higher depths. If you have a sample with a low depth, but it would be okay if its curve fits in the plateau regions of samples with larger sample depths. If it doesn’t fit, then the sample may not be sequenced deep enough. 4.2.3.3 Taxonomy There are several ways to subset your data based on the taxonomy table using the prune_taxa function. Most common are by name or by the counts assigned to each taxa. prune_taxa works similar to prune_samples in that it will take a logical statement or a vector of names. The length of the logical vector must be equal to the number of taxa in the phyloseq object, and the vector of names should match with the row names of the taxonomy table. Let’s try some examples of prune_taxa. We have &gt;50,000 taxa in the UCD-T2DM and we’re worried that we sequenced too deeply. Many of these taxa could be spurious, especially if there are only a few reads across all samples. Thus, we want to filter any taxa that doesn’t sum to 50 reads. # The taxa_sums() function will calculate sample depths across taxa phyloTaxaDepth &lt;- taxa_sums(phylo_obj) # We want to keep samples that have sequence depths greater than 50 TaxaDepth50 &lt;- phyloTaxaDepth &gt; 50 # Use the prune_samples() function # First argument is the logical statement, the second is the phyloseq object phyloTaxa50 &lt;- prune_taxa(TaxaDepth50, phylo_obj) # The &#39;pruned&#39; phyloseq object phyloTaxa50 ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 2333 taxa and 55 samples ] ## sample_data() Sample Data: [ 55 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 2333 taxa by 7 taxonomic ranks ] Woah!! We lost 99.95% of our OTUs! This actually may be appropriate. As you see, there will be other ways to filter low abundant taxa. Let’s look at an example of how we can use prune_taxa with a vector of names. We will use an example were extract the row names from the taxonomy table and use those names to filter the phyloseq object. # Extract row names from the taxonomy table phyloTaxaNames &lt;- taxa_names(phylo_obj2016) # Use the first 20 samples phyloTaxaNames20 &lt;- phyloTaxaNames[1:20] # View names, note that they are a unique identifier and not bacterial names phyloTaxaNames20 ## [1] &quot;366623&quot; &quot;182771&quot; &quot;1800048&quot; &quot;276629&quot; &quot;206494&quot; &quot;22466&quot; &quot;276195&quot; ## [8] &quot;276622&quot; &quot;130468&quot; &quot;296165&quot; &quot;189110&quot; &quot;193591&quot; &quot;782984&quot; &quot;262104&quot; ## [15] &quot;522433&quot; &quot;3943182&quot; &quot;949863&quot; &quot;306528&quot; &quot;48899&quot; &quot;4474844&quot; # First argument is the names vector, the second is the phyloseq object phyloTaxa20 &lt;- prune_taxa(phyloTaxaNames20, phylo_obj2016) # The &#39;pruned&#39; phyloseq object phyloTaxa20 ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 20 taxa and 33 samples ] ## sample_data() Sample Data: [ 33 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 20 taxa by 7 taxonomic ranks ] 4.2.4 Aggregating I have referred to microbial sequencing data as multi-level data due to the fact that you can analyze data at different phylogenetic levels. We are not working with a phylogenetic tree in this workshop, but we have all of the taxonomic information to aggregate lower level taxa into their parent taxa level. This is done using the tax_glom function on a phyloseq object. It works by providing the column name in the taxonomic table that you’d like to aggregate to. This is why we updated these labels to reflect the taxonomy levels in section 4.1.1, to make them more consistent with aggregation scheme. You can access the column names in the taxonomy table using the rank_names function. Many early investigations relating the microbiota to obesity identified an increased ratio of Firmicutes to Bacteroidetes in obese mice relative to their lean counterparts. This is at the phylum level, so the most broad phylogenetic level of bacteria. Let’s aggregate the phyloseq object to the phylum level. # Aggregate the phyloseq object to Phylum level. # First argument is the phyloseq object and second argument is the level to aggregate to # Will take 5-10 seconds to process because of the large amount of taxa phylo_Phylum &lt;- tax_glom(phylo_obj, &quot;Phylum&quot;) # The &#39;aggregated&#39; phyloseq object phylo_Phylum ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 13 taxa and 55 samples ] ## sample_data() Sample Data: [ 55 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 13 taxa by 7 taxonomic ranks ] The output doesn’t reflect which taxonomic level we’ve aggregated to, but it does show that there are now only 13 taxa. Let’s first look at the taxonomy table. # Using the tax_table() function from the phyloseq package. tax_table(phylo_Phylum) ## Taxonomy Table: [13 taxa by 7 taxonomic ranks]: ## Kingdom Phylum Class Order Family Genus ## 1108599 &quot;k__Bacteria&quot; &quot;p__Proteobacteria&quot; NA NA NA NA ## 1107027 &quot;k__Bacteria&quot; &quot;p__Firmicutes&quot; NA NA NA NA ## 277626 &quot;k__Bacteria&quot; &quot;p__Bacteroidetes&quot; NA NA NA NA ## 581048 &quot;k__Bacteria&quot; &quot;p__Elusimicrobia&quot; NA NA NA NA ## 828676 &quot;k__Bacteria&quot; &quot;p__Fusobacteria&quot; NA NA NA NA ## 2136916 &quot;k__Bacteria&quot; &quot;p__Lentisphaerae&quot; NA NA NA NA ## 4432841 &quot;k__Bacteria&quot; &quot;p__Tenericutes&quot; NA NA NA NA ## 997439 &quot;k__Bacteria&quot; &quot;p__Actinobacteria&quot; NA NA NA NA ## 381666 &quot;k__Bacteria&quot; &quot;p__Cyanobacteria&quot; NA NA NA NA ## 363731 &quot;k__Bacteria&quot; &quot;p__Verrucomicrobia&quot; NA NA NA NA ## 3528445 &quot;k__Archaea&quot; &quot;p__Euryarchaeota&quot; NA NA NA NA ## 25453 &quot;k__Bacteria&quot; &quot;p__Deferribacteres&quot; NA NA NA NA ## 401717 &quot;k__Bacteria&quot; &quot;p__TM7&quot; NA NA NA NA ## Species ## 1108599 NA ## 1107027 NA ## 277626 NA ## 581048 NA ## 828676 NA ## 2136916 NA ## 4432841 NA ## 997439 NA ## 381666 NA ## 363731 NA ## 3528445 NA ## 25453 NA ## 401717 NA Note that Class through Species are NA values because we’ve aggregated to the Phylum level. Similar to the ‘prune_’ functions, there is no going back after you use tax_glom. Make sure you create a new phyloseq object. Now lets say we want to extract Firmicutes and Bacteroidetes data so we can eventually compute a ratio. Let’s try it! # Manually extract taxonomy table and coerce it into a data frame PhylumTaxaTable &lt;- tax_table(phylo_Phylum)@.Data %&gt;% as.data.frame() # Extract the OTU table using the abundances() function and coerce into data frame # Need to be data frame to add unique identifier column for joining PhylumOTUTable &lt;- abundances(phylo_Phylum) %&gt;% data.frame # Extract the sample metadata using the meta() function PhylumSampleTable &lt;- meta(phylo_Phylum) # the row names are the unique OTU identifier # Make it into a column so we can use to join both data frames PhylumTaxaTable$OTU &lt;- rownames(PhylumTaxaTable) PhylumOTUTable$OTU &lt;- rownames(PhylumOTUTable) # Join data frames Phylum_TaxaOTU &lt;- full_join(PhylumTaxaTable, PhylumOTUTable, by=&quot;OTU&quot;) # Make Phylum column as rownames rownames(Phylum_TaxaOTU) &lt;- Phylum_TaxaOTU$Phylum # Remove all taxonomy columns using !() Phylum_OTUdf &lt;- Phylum_TaxaOTU[, !(colnames(Phylum_TaxaOTU) %in% colnames(PhylumTaxaTable))] # Transpose data and add sample data # the t() function coerces a data frame to a matrix, so you have to convert back. tPhylum_OTUdf &lt;- t(Phylum_OTUdf) %&gt;% as.data.frame # Remember, the count data column names had to match the sample data row names. # Make a new column with a column name that mirrors the one in the metadata data frame # You will need to make a similar column in the metadata if not done already. tPhylum_OTUdf$SampleID &lt;- rownames(tPhylum_OTUdf) # Join data frames Phylum_DF &lt;- full_join(PhylumSampleTable, tPhylum_OTUdf, by=&quot;SampleID&quot;) # View final object head(Phylum_DF) ## SampleID Rat GroupAbbrev Collection p__Proteobacteria p__Firmicutes ## 1 BP.K2491 K2491 D6M Y14 4805 27860 ## 2 BP.H931 H931 LSD Y14 795 15358 ## 3 BP.H933 H933 LSD Y14 1077 17874 ## 4 BP.H1147 H1147 LSD Y16 4147 17268 ## 5 BP.H1157 H1157 LSD Y16 4662 15752 ## 6 BP.K2503 K2503 D6M Y14 2256 24037 ## p__Bacteroidetes p__Elusimicrobia p__Fusobacteria p__Lentisphaerae ## 1 24402 58 0 1 ## 2 21377 90 0 0 ## 3 27066 44 0 2 ## 4 20690 13 0 4 ## 5 14143 76 0 4 ## 6 24489 11 0 3 ## p__Tenericutes p__Actinobacteria p__Cyanobacteria p__Verrucomicrobia ## 1 227 23 57 0 ## 2 60 37 475 71 ## 3 30 42 445 2937 ## 4 147 42 683 2380 ## 5 358 84 991 11735 ## 6 171 1139 230 4970 ## p__Euryarchaeota p__Deferribacteres p__TM7 ## 1 0 26 0 ## 2 0 290 5 ## 3 0 87 0 ## 4 0 52 0 ## 5 0 56 4 ## 6 0 0 0 We now have the data formatted in a way were we have all the necessary information to complete our analysis in the regular R environment (i.e., not phyloseq environment). 4.3 Pre-processing Pre-processing is arguably the most important step in your data analysis pipeline. We have already been exposed to some pre-processing examples in the last section - filtering low abundant reads and identifying samples with low read counts. These concepts will be detailed with more precision in this chapter, and we will cover how to perform a few normalizations procedures. One of which is commonly used to remove low abundant taxa. So it may feel we are jumping back and forth between sections, but the order is important. Some normalizations (e.g., proportions) are sensitive to differences in taxa, so we want to make that we’ve removed non-informative and potentially spurious taxa prior to normalization. 4.3.1 Low abundance taxa In section 4.2.3.3 we filtered low abundant taxa based on whether the sum of a taxa across all samples met a total count threshold. That is actually a very crude way to eliminate low abundant taxa. What if you had a taxa that had 5-10 counts consistently in 1 group but not in the other groups. You may lose this information with that filtering method. Another approach is to identify a minimum detection limit within a sample and then identify a whether a certain percentage of your samples meet that criteria. The phyloseq package has a function that does this, filter_taxa, but honestly, the core function from the microbiome package is much more straight-foward. We will examine the latter. The core function requires 3 arguments. The first is the phyloseq object, the second defines the ‘detection’ limit, and the third defines the ‘prevalence’ (i.e., proportion) that meets the defined limit. Let’s say we want to remove the LSD rat group and assess the microbiota on only UCD-T2DM rats. We now have 4 groups and want to ensure we can detect a taxa that is only abundant in a single group. That would be 25% of the samples, if the samples were evenly balanced. As there is still inter-individual variability between rats, we decide to settle on a certain taxa having counts in at least 20% of the total sample. Now, what should we define as the count limit? There is evidence that large effect sizes can be detected at sequences as low as 10 reads. Thus, we will set our mimimum limit at 10 reads. # The function will fail if your criteria is too strong. phyobj_d10p25 &lt;- core(phylo_obj, detection = 10, prevalence = 0.25) # View object phyobj_d10p25 ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 400 taxa and 55 samples ] ## sample_data() Sample Data: [ 55 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 400 taxa by 7 taxonomic ranks ] If you recall, the earlier filter resulted in 2333 taxa, but this filtering procedure resulted in 400 taxa. Both are a considerable drop from the original total of over 55,000. How much data did we actually lose? We can calculate it using the sample_sums function! # Sum the sample sums will give us the sum of all reads Original_DepthSize &lt;- sum(sample_sums(phylo_obj)) # Filtered summed sample depths d10p25_DepthSize &lt;- sum(sample_sums(phyobj_d10p25)) 100 * (d10p25_DepthSize/Original_DepthSize) ## [1] 75.81838 Reducing the taxa total by &gt; 99% only resulted in 25% data loss. This is a pretty glaring example of over-sequencing, but in general, microbial sequencing data tends to be skewed to low abundant taxa. 4.3.2 Proportional Abundance Proportional abundance, also sometimes referred to relative abundance, can applied to a phyloseq object. Both phyloseq and microbiome packages contain functions that can transform the OTU table. The transform_sample_counts function from phyloseq provides a ‘function’ argument where you can provide a single-argument function or create your own function to pass to the phyloseq object. So it is very flexible. The microbiome::transform function has several transformations coded into it, including the transformation to proportional abundances. Please note the ‘microbiome::’ prefix to the transform function. Transform is a common used function name, so the ’microbiome::&quot; prefix denotes that this transform is specifically from the microbiome package. I advise using it in every use of transform. Now that we have filtered our data, let’s calculate the proportional abundances of our taxa. # Two arguments required, the phyloseq object and the transformation # Please see the help page for the full list of transformations: ?microbiome::transform phyobj_d10p25_prop &lt;- microbiome::transform(phyobj_d10p25, &quot;compositional&quot;) # View a subset of the data head(abundances(phyobj_d10p25_prop)[,1:10]) ## BP.K2491 BP.H931 BP.H933 BP.H1147 BP.H1157 ## 4474844 7.787353e-05 0.000000e+00 0.0000000000 0.0078841512 9.672165e-04 ## 1106614 3.242135e-02 7.760190e-03 0.0060768994 0.0001340842 0.000000e+00 ## 354987 1.168103e-03 0.000000e+00 0.0000000000 0.0013944757 2.545306e-05 ## 266445 2.595784e-05 6.867424e-05 0.0000000000 0.0003754358 1.018123e-04 ## 97151 1.505555e-03 4.051780e-03 0.0061329077 0.0008581389 5.599674e-04 ## 443853 2.076628e-04 3.433712e-05 0.0001400207 0.0000000000 7.126858e-04 ## BP.K2503 BP.P8163 BP.H1166 BP.P7611 BP.P7609 ## 4474844 0.0000000000 0.0000000000 0.0111216387 0.0000000000 0.000000e+00 ## 1106614 0.0051581018 0.0001416983 0.0006767731 0.0000000000 2.157364e-04 ## 354987 0.0000000000 0.0007084913 0.0032710702 0.0000000000 0.000000e+00 ## 266445 0.0009596468 0.0010273123 0.0002481502 0.0001290989 2.527198e-03 ## 97151 0.0008396910 0.0018775019 0.0006542140 0.0007487736 3.698339e-04 ## 443853 0.0000000000 0.0000000000 0.0000000000 0.0000000000 3.081949e-05 # Do the taxa sum to 1? sample_sums(phyobj_d10p25_prop) ## BP.K2491 BP.H931 BP.H933 BP.H1147 BP.H1157 BP.K2503 BP.P8163 BP.H1166 ## 1 1 1 1 1 1 1 1 ## BP.P7611 BP.P7609 BP.H937 BP.P7589 BP.P9227 BP.P9257 BP.P9259 BP.P9255 ## 1 1 1 1 1 1 1 1 ## BP.P9395 BP.P9303 BP.P8113 BP.P9295 BP.P9335 BP.P8287 BP.P9393 BP.P9223 ## 1 1 1 1 1 1 1 1 ## BP.P9263 BP.P9291 BP.H1156 BP.P9375 BP.K4153 BP.H935 BP.P9285 BP.P7603 ## 1 1 1 1 1 1 1 1 ## BP.P9387 BP.P7563 BP.H1167 BP.H939 BP.P7573 BP.P9261 BP.P9327 BP.K2513 ## 1 1 1 1 1 1 1 1 ## BP.P8153 BP.P8117 BP.P9251 BP.P9329 BP.P7561 BP.P7949 BP.P9225 BP.P9323 ## 1 1 1 1 1 1 1 1 ## BP.P8259 BP.K4206 BP.H1145 BP.H1138 BP.P9243 BP.K4164 BP.P9253 ## 1 1 1 1 1 1 1 Perhaps we want to determine the median class proportional abundance? Let’s do it! # Use tax_glom() function phyobj_PA_Class &lt;- tax_glom(phyobj_d10p25_prop, &quot;Class&quot;) # Extract data phyobj_PA_Class_OTU &lt;- abundances(phyobj_PA_Class) # Use apply to determine median for each row # Multiply by 100 to get percentage and round for readability. phyobj_PA_Class_OTUmedian &lt;- round(apply(phyobj_PA_Class_OTU, 1, median) * 100, 2) # Extract taxonomy table phyobj_PA_ClassTaxa &lt;- tax_table(phyobj_PA_Class)@.Data %&gt;% as.data.frame() # Check to make sure row names of taxa table matches phyobj_PA_Class_OTUmedian names setequal(names(phyobj_PA_Class_OTUmedian) == rownames(phyobj_PA_ClassTaxa), TRUE) ## [1] TRUE # If TRUE, add phyobj_PA_Class_OTUmedian to taxa table phyobj_PA_ClassTaxa$Median &lt;- phyobj_PA_Class_OTUmedian # Keep necessary colums phyobj_PA_ClassTaxa_DF &lt;- phyobj_PA_ClassTaxa[,colnames(phyobj_PA_ClassTaxa) %in% c(&quot;Phylum&quot;, &quot;Class&quot;, &quot;Median&quot;)] # View based on phylum order phyobj_PA_ClassTaxa_DF[order(phyobj_PA_ClassTaxa_DF$Phylum),] ## Phylum Class Median ## 997439 p__Actinobacteria c__Actinobacteria 0.06 ## 277626 p__Bacteroidetes c__Bacteroidia 49.27 ## 381666 p__Cyanobacteria c__4C0d-2 0.16 ## 25453 p__Deferribacteres c__Deferribacteres 0.06 ## 581048 p__Elusimicrobia c__Elusimicrobia 0.03 ## 3528445 p__Euryarchaeota c__Methanobacteria 0.00 ## 555945 p__Firmicutes c__Clostridia 28.20 ## 1107027 p__Firmicutes c__Bacilli 6.75 ## 386788 p__Firmicutes c__Erysipelotrichi 0.02 ## 2564048 p__Proteobacteria c__Epsilonproteobacteria 0.41 ## 359809 p__Proteobacteria c__Betaproteobacteria 0.06 ## 1108599 p__Proteobacteria c__Deltaproteobacteria 8.68 ## 1111294 p__Proteobacteria c__Gammaproteobacteria 0.12 ## 1113282 p__Tenericutes c__Mollicutes 0.08 ## 363731 p__Verrucomicrobia c__Verrucomicrobiae 0.01 Only 4 classes have a median proportional abundance greater than 1%. This is fairly common in rodent and human studies. At least 80-90% of taxa are within the Firmicutes and Bacteroidetes phyla. It is also common to filter taxa based on proportional abundances. Many times, a minimum average or median proportion must be met. Say, 1% on a scale of 0-100% (equivalent to 0.01 on a 0 to 1 scale). We will use the prune_taxa function to do this # Transform counts to proportional abundances. phylo_obj_PA &lt;- microbiome::transform(phylo_obj, &quot;compositional&quot;) # Determine mean proportional abundances by taxa. phylo_obj_PA_taxamean &lt;- rowMeans(abundances(phylo_obj_PA)) # Use logical statement in prune_taxa()Using prune_taxa() function. phyobj_PA_p01 &lt;- prune_taxa(phylo_obj_PA_taxamean &gt; 0.01, phylo_obj_PA) # View phyloseq object phyobj_PA_p01 ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 14 taxa and 55 samples ] ## sample_data() Sample Data: [ 55 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 14 taxa by 7 taxonomic ranks ] 14 taxa! I guess we only want to see the most abundant taxa. How much data was lost? # Filtered summed sample depths PA_p01_DepthSize &lt;- sum(sample_sums(phyobj_PA_p01)) 100 * (PA_p01_DepthSize/Original_DepthSize) ## [1] 0.000812965 In this case, we lost the majority of our data. Likely due to the fact that the proportional abundances are spread across 55,000 taxa. Still, it shows how a single filtering criteria may not be advantageous. 4.3.3 Zeros This is arguably the most difficult aspect of microbial sequence analysis. The zeros in the sequencing data do not necessarily mean that the bacteria is not present, it could also mean the sampling depth was not adequate. Thus, treatment of zeros can sometimes depend on how the zero is viewed. If there are all zeros for a particular taxa in 1 diet group and non-zero counts in another, then it may be good evidence that that particular taxa is not in the former diet group. However, a few zeros in a taxa with random low counts may be a result of low sampling depth. As there are so many zeros and many normalization methods require imputations to work, all zeros are generally considered equally. Many normalization methods apply logarithmic treatments, which makes it necessary to add a non-negative value to zeros. The easiest way is to shift the entire count table by a defined positive number. Typically this is with the number 1. The microbiome::tranform function can apply this to your OTU table # The function will fail if your criteria is too strong. phyobj_d10p25_shift &lt;- microbiome::transform(phyobj_d10p25, transform=&quot;shift&quot;, shift=1) # Compare abbreviated taxa data.frame(Original=abundances(phyobj_d10p25)[1,1:15], Shifted=abundances(phyobj_d10p25_shift)[1,1:15]) ## Original Shifted ## BP.K2491 3 4 ## BP.H931 0 1 ## BP.H933 0 1 ## BP.H1147 294 295 ## BP.H1157 38 39 ## BP.K2503 0 1 ## BP.P8163 0 1 ## BP.H1166 493 494 ## BP.P7611 0 1 ## BP.P7609 0 1 ## BP.H937 0 1 ## BP.P7589 0 1 ## BP.P9227 0 1 ## BP.P9257 0 1 ## BP.P9259 0 1 Note that all counts increased by 1. This actually has a profound effect on the data. The low abundant counts are affected much greater than the higher counts, and our data is skewed to low abundant counts! Another proposed approach, within a compositional framework, is a Bayesian-multiplicative treatment. In depth details of the method can be found here, but it essentially replaces the zero with an estimate of the likelhood of the 0 being an artifact relative to the remainder of the data. The estimate is not a count, so this procedure is not useful for statistical tools that only handle counts. In order to perform this zero-imputation method, we will need to use the cmultRepl function from the zCompositions package. The first argument of cmultRepl takes your data, then we will input “CZM” (count zero multiplicative) in the ‘method’ argument, and “p-counts” (pseudo-counts) in the ‘output’ argument. We have to do a bit of data wrangling to get the treated data back into the phyloseq object. # Load library library(zCompositions) # Extract OTU table d10p25_OTU &lt;- abundances(phyobj_d10p25) # Taxa needs be rows and samples need to be columns d10p25_OTU_BMz &lt;- cmultRepl(d10p25_OTU, method=&quot;CZM&quot;, output=&quot;p-counts&quot;) ## No. corrected values: 3564 # Make duplicate phyloseq object with new name phyobj_d10p25_BMz &lt;- phyobj_d10p25 # Replace OTU table data with zero imputed data otu_table(phyobj_d10p25_BMz) &lt;- otu_table(d10p25_OTU_BMz, taxa_are_rows = TRUE) # View abbreviated data head(abundances(phyobj_d10p25_BMz)[,1:15]) ## BP.K2491 BP.H931 BP.H933 BP.H1147 BP.H1157 ## 4474844 3 0.3253469 3.533513e-02 294.0000000 38.000000 ## 1106614 1249 226.0000000 2.170000e+02 5.0000000 0.325119 ## 354987 45 0.3254957 7.971551e-03 52.0000000 1.000000 ## 266445 1 2.0000000 9.620607e-03 14.0000000 4.000000 ## 97151 58 118.0000000 2.190000e+02 32.0000000 22.000000 ## 443853 8 1.0000000 5.000000e+00 0.1139959 28.000000 ## BP.K2503 BP.P8163 BP.H1166 BP.P7611 BP.P7609 ## 4474844 0.3253469 0.3253469 493.0000000 0.3253469 0.32534691 ## 1106614 215.0000000 4.0000000 30.0000000 0.3251190 7.00000000 ## 354987 0.3254957 20.0000000 145.0000000 0.1113692 0.07958477 ## 266445 40.0000000 29.0000000 11.0000000 5.0000000 82.00000000 ## 97151 35.0000000 53.0000000 29.0000000 29.0000000 12.00000000 ## 443853 0.3253579 0.1226230 0.1139959 0.1139959 1.00000000 ## BP.H937 BP.P7589 BP.P9227 BP.P9257 BP.P9259 ## 4474844 0.32534691 0.3253469 0.3253469 0.3253469 0.07060998 ## 1106614 6.00000000 2.0000000 17.0000000 544.0000000 8.00000000 ## 354987 0.06364503 0.3254957 3.0000000 42.0000000 149.00000000 ## 266445 6.00000000 163.0000000 112.0000000 6.0000000 103.00000000 ## 97151 132.00000000 88.0000000 44.0000000 67.0000000 36.00000000 ## 443853 1.00000000 8.0000000 183.0000000 7.0000000 174.00000000 Be careful with the phyloseq objects that have had zero treatments and/or transformations. Some of the functionalities may give incorrect or incosistent results, e.g., tax_glom. It is best to subset or aggregate prior to these treatments. If these treatments are required to make a subset/aggregation decision, then I advise making 2 separate phyloseq objects as we did in the above example. 4.3.4 Centered log ratios A centered-log ratio (CLR) transformation has been suggested to render compositional data compatible with standard multivariate techniques. However, the individual counts become ratios between all parts of the sample, so the interpretion is now altered. We will use the clr function from the rgr package to carry out this function. We will use the same coding workflow as in Section 4.3.3. The CLR transformation requires a counts or proportions values &gt; 0. Therefore, we will use the phyobj_d10p25_BMz object in the analysis. # Load library library(rgr) # Extract OTU table from the zero imputed phyloseq object. BMz_OTU &lt;- abundances(phyobj_d10p25_BMz) # Taxa needs be rows and samples need to be columns BMz_clr_OTU &lt;- clr(BMz_OTU, ifwarn=FALSE) # Make duplicate phyloseq object with new name phyobj_BMz_clr &lt;- phyobj_d10p25_BMz # Replace OTU table data with zero imputed data otu_table(phyobj_BMz_clr) &lt;- otu_table(BMz_clr_OTU, taxa_are_rows = TRUE) # View abbreviated data head(abundances(phyobj_BMz_clr)[,1:15]) ## BP.K2491 BP.H931 BP.H933 BP.H1147 BP.H1157 BP.K2503 ## 4474844 0.3119017 -1.909574 -4.1295883 4.89686919 2.85087558 -1.9095738 ## 1106614 5.0102160 3.300652 3.2600149 -0.51044459 -3.24344662 3.2507555 ## 354987 2.7686400 -2.160429 -5.8698986 2.91322126 -1.03802246 -2.1604286 ## 266445 -2.6026610 -1.909514 -7.2465089 0.03639635 -1.21636662 1.0862185 ## 97151 1.0035820 1.713824 2.3322107 0.40887492 0.03418147 0.4984871 ## 443853 1.0720100 -1.007432 0.6020064 -3.17902407 2.32477300 -2.1302609 ## BP.P8163 BP.H1166 BP.P7611 BP.P7609 BP.H937 BP.P7589 ## 4474844 -1.9095738 5.4137986 -1.9095738 -1.9095738 -1.9095738 -1.909574 ## 1106614 -0.7335881 1.2813149 -3.2434466 -0.1739724 -0.3281230 -1.426735 ## 354987 1.9577098 3.9387113 -3.2329266 -3.5689549 -3.7924565 -2.160429 ## 266445 0.7646348 -0.2047657 -0.9932231 1.8040583 -0.8109015 2.491089 ## 97151 0.9134309 0.3104348 0.3104348 -0.5719543 1.8259409 1.420476 ## 443853 -3.1060718 -3.1790241 -3.1790241 -1.0074315 -1.0074315 1.072010 ## BP.P9227 BP.P9257 BP.P9259 ## 4474844 -1.90957383 -1.9095738 -3.43729436 ## 1106614 0.71333084 4.1790667 -0.04044096 ## 354987 0.06058983 2.6996472 3.96592385 ## 266445 2.11583789 -0.8109015 2.03206800 ## 97151 0.72732865 1.1478316 0.52665795 ## 443853 4.20205464 0.9384786 4.15162379 We now have positive and negative values due to the centering of the data. We will use this transformation in Chapter 6 when looking at beta-diversity. 4.4 Conclusion We have been using the phyloseq object as a way to package all of the data associated with microbial sequencing in a convenient wrapper. It allows for a single object to be manipulated when filtering and aggregating, but you’ll realize in subsequent chapters that it can be claustrophic in certain data pipelines. For example, you may not want to package your data into a phyloseq object if you’re going to use the compositional approach to visualizing and analyzing (e.g., zero replacement and CLR transformation). However, maybe you want to run your analysis at Family level before the compositional approach. Packaging into a phyloseq package and pre-processing your data prior to the compositional approach would then be helpful. "],
["alpha-diversity.html", "Chapter 5 Alpha-Diversity 5.1 phyloseq package 5.2 microbiome package 5.3 Data analysis", " Chapter 5 Alpha-Diversity We went over the concept of alpha-diversity in Chapter 3.3.1 and now we get to actually estimate these indices in R. Fortunately, producing these indices is straight forward using the phyloseq environment. We will explore the functions that calculate various alpha-diversity indices and how to analyze them by groups. If you recall, some of the richness measurements adjust for the number of taxa found as singletons and doubletons. Therefore, some would argue that using filtered data is not appropriate, but I tend to believe it is a balance between removing spurious taxa in 16S data while maintaining true low abundant taxa. Your filtering criteria should not be so strong to remove ALL low abundant taxa. That would defeat the purpose of maximizing your sampling depth to discover true low abundant taxa. Some of these functions will give a warning if they sense strong filters. 5.1 phyloseq package Let’s use the estimate_richness function to calculate alpha-diversity indices in the UCD-T2DM Rat data. We will use the phyloseq object that we pre-processed in Chapter 4.3.1. 5.1.1 estimate_richness() # You can specify specific indices to calculate by using the &#39;measures&#39; argument, see help page, ?estimate_richness UCDrats_Adiv &lt;- estimate_richness(phyobj_d10p25) # View object head(UCDrats_Adiv) ## Observed Chao1 se.chao1 ACE se.ACE Shannon Simpson ## BP.K2491 322 332.9286 6.853401 329.8674 8.288581 4.164139 0.9649630 ## BP.H931 264 267.4737 2.910417 268.9841 7.687761 3.674326 0.9164394 ## BP.H933 279 282.9286 3.337384 283.7086 7.368780 3.743879 0.9155674 ## BP.H1147 345 357.3636 7.952144 350.6429 8.879509 4.004695 0.9548076 ## BP.H1157 336 356.3125 10.512706 348.4945 8.949091 3.542787 0.8986565 ## BP.K2503 327 342.4000 8.680170 336.2866 8.703965 3.702618 0.9287288 ## InvSimpson Fisher ## BP.K2491 28.541288 48.16245 ## BP.H931 11.967365 40.05883 ## BP.H933 11.843766 41.24267 ## BP.H1147 22.127608 52.54106 ## BP.H1157 9.867431 50.45967 ## BP.K2503 14.030907 48.37238 The output provides all indices that we covered, “Observed”, “Chao1”, “ACE”, and “Shannon”, plus a few extras. It is not attached to the phyloseq object, so we will need to extract the Sample Metadata and attach it to these data. This will be demonstrated soon. 5.2 microbiome package There are several functions in the microbiome that measure alpha-diversity. They are named based on what is measured, e.g., dominance, richness, rarity, etc. 5.2.1 richness() Returns ‘observed’ and ‘chao1’ estimates. The chao1 estimate varies slightly from the estimate_richness version. The former is calculated based on Chao (Scand J Stat, 1984), while the latter uses the estimateR function from the vegan package. Details can be found at the help page, ?vegan::estimateR. # You can specify specific index/indices to calculate by using the &#39;index&#39; argument UCDrats_Adiv_rich &lt;- richness(phyobj_d10p25) # View abbreviated object head(UCDrats_Adiv_rich) ## observed chao1 ## BP.K2491 322 334.4615 ## BP.H931 264 268.0000 ## BP.H933 279 283.6538 ## BP.H1147 345 359.4500 ## BP.H1157 336 358.5333 ## BP.K2503 327 344.2857 5.2.2 dominance() Returns 7 estimates of dominance. Note, that the Simpson’s index from dominance is in a different unit then the Simpson’s index from estimate_richness, but they are the same. We will demonstrate this by plotting both. # You can specify specific index/indices to calculate by using the &#39;index&#39; argument UCDrats_Adiv_dom &lt;- dominance(phyobj_d10p25) # View abbreviated object head(UCDrats_Adiv_dom) ## dbp dmn absolute relative simpson core_abundance ## BP.K2491 0.1089710 0.1721524 4198 0.1089710 0.03503696 0.9879296 ## BP.H931 0.2545411 0.3236617 7413 0.2545411 0.08356058 0.9845826 ## BP.H933 0.2595144 0.3357977 9267 0.2595144 0.08443260 0.9764485 ## BP.H1147 0.1345937 0.2221775 5019 0.1345937 0.04519241 0.9657549 ## BP.H1157 0.2855834 0.3723020 11220 0.2855834 0.10134351 0.9618713 ## BP.K2503 0.2095389 0.3214817 8734 0.2095389 0.07127123 0.8773811 ## gini ## BP.K2491 0.8493294 ## BP.H931 0.8888539 ## BP.H933 0.8759507 ## BP.H1147 0.8586950 ## BP.H1157 0.8892545 ## BP.K2503 0.8867204 # Comparing Simpson&#39;s index from estimate_richness() output plot(UCDrats_Adiv$Simpson, UCDrats_Adiv_dom$simpson, xlab=&quot;estimate_richness()&quot;, ylab=&quot;dominance()&quot;) It is a perfect linear relationship. 5.2.3 evenness() Returns 5 estimates of evenness. Note, there are several variants to the Simpson’s index. evenness returns Simpson’s evenness, which is different than Simpson’s dominance from estimate_richness and dominance. # You can specify specific index/indices to calculate by using the &#39;index&#39; argument UCDrats_Adiv_even &lt;- evenness(phyobj_d10p25) # View abbreviated object head(UCDrats_Adiv_even) ## camargo pielou simpson evar bulla ## BP.K2491 0.1862695 0.7211190 0.08863754 0.2147279 0.3376624 ## BP.H931 0.1896463 0.6589597 0.04533093 0.2227112 0.3188006 ## BP.H933 0.2077114 0.6648444 0.04245078 0.2310484 0.3286754 ## BP.H1147 0.1625825 0.6853195 0.06413799 0.2325972 0.3106326 ## BP.H1157 0.1303117 0.6090286 0.02936735 0.2133367 0.2670447 ## BP.K2503 0.1385092 0.6394894 0.04290797 0.2091379 0.2774727 5.2.4 rarity() Returns 4 estimates of rarity. # You can specify specific index/indices to calculate by using the &#39;index&#39; argument UCDrats_Adiv_rar &lt;- rarity(phyobj_d10p25) # View abbreviated object head(UCDrats_Adiv_rar) ## log_modulo_skewness low_abundance noncore_abundance ## BP.K2491 2.059103 0.1453380 0 ## BP.H931 2.058384 0.1106342 0 ## BP.H933 2.056569 0.1204178 0 ## BP.H1147 2.059638 0.1330383 0 ## BP.H1157 2.059311 0.1345194 0 ## BP.K2503 2.059966 0.1159013 0 ## rare_abundance ## BP.K2491 0 ## BP.H931 0 ## BP.H933 0 ## BP.H1147 0 ## BP.H1157 0 ## BP.K2503 0 5.3 Data analysis Now that we know how to estimate a wide range of alpha-diversity measurements, we now want to know whether the indices are altered by our experimental treatments. We will continue to work with the UCD-T2DM rat dataset. Remember, there are 5 groups, but also 2 collection periods. We will use a 1-way ANOVA for the rat groups and block by collection year. We will cover how to measure a single measurement first, and then go through the workflow of assessing all indices at the same time. Let’s see if there is a difference in Chao1. First, we need to combine the diversity measurements with our Sample Metadata. # Extract Sample Metadata UCDrats_MetaData &lt;- meta(phyobj_d10p25) # Make matching columns for both Alpha-diversity and metadata data frames using the row names # Sample Metadata already has matching column, but will need to create if not in data frame. UCDrats_Adiv$SampleID &lt;- rownames(UCDrats_Adiv) # Join alpha-diversity and metadata UCDrats_Adiv_DF &lt;- full_join(UCDrats_MetaData, UCDrats_Adiv) ## Joining, by = &quot;SampleID&quot; Now, let’s plot Chao1 by group. We can quickly do this with the boxplot() function using the formula notation described in Chapter 2.9.2. It is a base plotting function, so probably not publication quality. We just want to visually see if there are difference in the distribution of samples by group. # Boxplot by Groups UCDrats_MetaData &lt;- boxplot(Chao1 ~ GroupAbbrev, data=UCDrats_Adiv_DF) # Boxplot by collection year UCDrats_MetaData &lt;- boxplot(Chao1 ~ Collection, data=UCDrats_Adiv_DF) Looks like the pre-diabetics (PD) and recent-diabetics (RD) rats may have higher diversity compared to the rats with more advanced diabetes (D3M and D6M). Also appears that rats collected in 2016 may have higher diversity compared to those collected in 2014. Let’s see if these differences are statistically significant. # Assess ANOVA on Chao1 Chao1ANOVA &lt;- aov(Chao1 ~ GroupAbbrev + Collection, data=UCDrats_Adiv_DF) # View ANOVA table summary(Chao1ANOVA) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GroupAbbrev 4 40082 10021 6.205 0.000403 *** ## Collection 1 7480 7480 4.631 0.036347 * ## Residuals 49 79137 1615 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both ‘GroupAbbev’ and ‘Collection’ effects are significant. Now, which groups differ from each other. # Using the tukey HSD test. TukeyHSD(Chao1ANOVA, &quot;GroupAbbrev&quot;) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Chao1 ~ GroupAbbrev + Collection, data = UCDrats_Adiv_DF) ## ## $GroupAbbrev ## diff lwr upr p adj ## D6M-D3M 8.563722 -46.462146 63.58959 0.9919255 ## LSD-D3M 15.997278 -31.509215 63.50377 0.8742124 ## PD-D3M 58.227646 13.050300 103.40499 0.0054965 ## RD-D3M 66.050235 16.323572 115.77690 0.0039501 ## LSD-D6M 7.433555 -46.693346 61.56046 0.9949982 ## PD-D6M 49.663924 -2.430676 101.75852 0.0684687 ## RD-D6M 57.486513 1.400902 113.57212 0.0420272 ## PD-LSD 42.230368 -1.847607 86.30834 0.0663569 ## RD-LSD 50.052957 1.322922 98.78299 0.0413906 ## RD-PD 7.822589 -38.639677 54.28485 0.9891199 Looks like the Chao1 index in RD rats is greater than in both the D3M and D6M rats, while the Chao1 index in PD is higher than D3M and approaches significance relative to D6M. This is not a stats course, so we cannot go over every statistical approach that R offers. However, these indices can be treated like any other data. For example you could correlate it to other continuous data, e.g., blood glucose, dietary intake data, etc. Now, let’s say we wanted to analyze the rest. We could copy and paste this workflow, and then change Chao1 to Shannon in every object. But, that may lead to errors that we may not notice. For example, you may think you have an output for the Shannon index, but oops, you still have Chao1 in your formula. It would be much better if R did it all of the heavy work for us. Let’s see how this can be done. This next example includes immediate and advance coding. Examples of this workflow were shown in Chapters 2.8.4 and 2.8.6. # Start with the combined data frame, UCDrats_Adiv_DF, and rearrange it into a tall format. UCDrats_Adiv_DF_Tall &lt;- UCDrats_Adiv_DF %&gt;% gather(AlphaDiv, value, -(SampleID:Collection)) # Calculate means and sd AdivMeans &lt;- UCDrats_Adiv_DF_Tall %&gt;% group_by(AlphaDiv, GroupAbbrev, Collection) %&gt;% summarise(MEAN = mean(value)) # Make new column combining effect columns, and then arrange to wide format AdivMeans &lt;- AdivMeans %&gt;% as.data.frame() %&gt;% mutate(GroupYear = paste(GroupAbbrev, Collection, sep=&quot;_&quot;)) %&gt;% dplyr::select(-GroupAbbrev, -Collection) %&gt;% spread(GroupYear, MEAN) # Extract the string labels of all indices AdivIndices &lt;- unique(UCDrats_Adiv_DF_Tall$AlphaDiv) # Use sapply() to get p-values from the ANOVAs AdivANOVA_P &lt;- sapply(AdivIndices, function(x) { # Subset by Alpha diversity index DAT &lt;- UCDrats_Adiv_DF_Tall[UCDrats_Adiv_DF_Tall$AlphaDiv %in% x,] # Use subsetted data in ANOVA AOV &lt;- aov(value ~ GroupAbbrev + Collection, data=DAT) # Extract p-value AOV_P &lt;- round(summary(AOV)[[1]][1:2,&quot;Pr(&gt;F)&quot;], 5) # Name vector elements if &gt; 2 effects. # The order should follow the order in which they were coded into the ANOVA. names(AOV_P) &lt;- c(&quot;GroupAbbrev&quot;, &quot;Collection&quot;) # return final object AOV_P }) # Transpose AdivANOVA_P and coerce to data frame tAdivANOVA_P &lt;- t(AdivANOVA_P) %&gt;% as.data.frame() # Calculate FDR (really should apply across all tests, not by effect...) tAdivANOVA_P$GroupAbbrev_FDR &lt;- p.adjust(tAdivANOVA_P$GroupAbbrev, method=&quot;fdr&quot;) tAdivANOVA_P$Collection_FDR &lt;- p.adjust(tAdivANOVA_P$Collection, method=&quot;fdr&quot;) # Add AlphaDiv column with row names to match AdivMeans object tAdivANOVA_P$AlphaDiv &lt;- rownames(tAdivANOVA_P) # Join tAdivANOVA_P and AdivMeans AdivMeansAOVresults &lt;- full_join(AdivMeans, tAdivANOVA_P, by=&quot;AlphaDiv&quot;) # View results AdivMeansAOVresults ## AlphaDiv D3M_Y14 D3M_Y16 D6M_Y14 D6M_Y16 LSD_Y14 ## 1 ACE 281.8517222 312.3514805 324.5529581 254.2882149 261.800819 ## 2 Chao1 280.3871580 313.8759079 326.1230037 249.2981283 261.193784 ## 3 Fisher 41.9759331 45.1747667 48.0912433 37.1303328 38.644968 ## 4 InvSimpson 20.3555362 19.2820119 18.7224463 13.5182113 18.852212 ## 5 Observed 273.6666667 306.0000000 317.4000000 242.5000000 256.200000 ## 6 se.ACE 8.0136821 8.1249692 8.6005251 7.7482355 7.327363 ## 7 se.chao1 4.2440218 4.7121530 5.5009447 4.2026271 3.908604 ## 8 Shannon 3.7481400 3.8612767 3.8611519 3.5647088 3.858031 ## 9 Simpson 0.9430829 0.9409606 0.9429804 0.9259768 0.939014 ## LSD_Y16 PD_Y14 PD_Y16 RD_Y16 GroupAbbrev Collection ## 1 344.9607688 344.4031812 357.7353626 361.3101107 0.00030 0.03666 ## 2 347.6157449 345.6883266 359.2693875 361.6595519 0.00040 0.03635 ## 3 51.0866685 51.9029751 55.6493218 55.2750479 0.00003 0.04327 ## 4 17.5565617 18.3841252 26.9384674 22.3568960 0.30941 0.52910 ## 5 338.4285714 340.5000000 353.1111111 357.7000000 0.00010 0.03960 ## 6 8.8176946 8.4503873 8.6271711 8.6868210 0.14873 0.05744 ## 7 5.6763162 3.9463138 4.3373940 3.0864254 0.38306 0.45193 ## 8 3.8543452 4.0414242 4.2396395 4.0990683 0.00125 0.49366 ## 9 0.9392268 0.9417145 0.9564142 0.9490509 0.47243 0.72189 ## GroupAbbrev_FDR Collection_FDR ## 1 0.0009000 0.0973575 ## 2 0.0009000 0.0973575 ## 3 0.0002700 0.0973575 ## 4 0.3978129 0.5952375 ## 5 0.0004500 0.0973575 ## 6 0.2230950 0.1033920 ## 7 0.4309425 0.5952375 ## 8 0.0022500 0.5952375 ## 9 0.4724300 0.7218900 Now we have means, P-values, and FDR for all indices. Note, the “se.ACE” and “se.Chao1” can be removed as they are standard error calculations of ACE and Chao1 indices. "],
["beta-diversity.html", "Chapter 6 Beta-Diversity 6.1 Calculating beta-diversity in R 6.2 Ordinations 6.3 Visualizing ordinations 6.4 PERMANOVA 6.5 Conclusion", " Chapter 6 Beta-Diversity There have only been a few plotting examples so far in this workbook. This chapter will change this. The best way to understand the concept of beta-diversity is to actually see the differences between samples. In this chapter will learn how to calculate beta-diversity, ordinate the results, visualize beta diversity, and determine if there are statistical differences in beta diversity. 6.1 Calculating beta-diversity in R We discussed the various ways to calculate beta-diversity in Chapter 3.3.2. There are distance-, dissimilarity-, and covariance-, and phylogenetic-based estimations of beta-diversity, all of which may uncover different aspects of your data. On top of that, normalization procedures will also influence the results. So, the first step is to calculate a matrix of your beta-diversity index. We will first cover where to find available indices, followed by the the functions that will calculate your chosen beta-diversity index. 6.1.1 Identifying your beta diversity indices The most comphrehensive resources is the distanceMethodList function from the phyloseq package. distanceMethodList ## $UniFrac ## [1] &quot;unifrac&quot; &quot;wunifrac&quot; ## ## $DPCoA ## [1] &quot;dpcoa&quot; ## ## $JSD ## [1] &quot;jsd&quot; ## ## $vegdist ## [1] &quot;manhattan&quot; &quot;euclidean&quot; &quot;canberra&quot; &quot;bray&quot; &quot;kulczynski&quot; ## [6] &quot;jaccard&quot; &quot;gower&quot; &quot;altGower&quot; &quot;morisita&quot; &quot;horn&quot; ## [11] &quot;mountford&quot; &quot;raup&quot; &quot;binomial&quot; &quot;chao&quot; &quot;cao&quot; ## ## $betadiver ## [1] &quot;w&quot; &quot;-1&quot; &quot;c&quot; &quot;wb&quot; &quot;r&quot; &quot;I&quot; &quot;e&quot; &quot;t&quot; &quot;me&quot; &quot;j&quot; &quot;sor&quot; ## [12] &quot;m&quot; &quot;-2&quot; &quot;co&quot; &quot;cc&quot; &quot;g&quot; &quot;-3&quot; &quot;l&quot; &quot;19&quot; &quot;hk&quot; &quot;rlb&quot; &quot;sim&quot; ## [23] &quot;gl&quot; &quot;z&quot; ## ## $dist ## [1] &quot;maximum&quot; &quot;binary&quot; &quot;minkowski&quot; ## ## $designdist ## [1] &quot;ANY&quot; Many options here. The ‘Unifrac’, ‘dpcoa’, and ‘JSD’ require a phylogenetic tree and are computationally more expensive than the others. The options under ‘vegdist’ and ‘betadiver’ are derived from ecology sciences and implemented in the vegdist function from the vegan package. The most common diversity indices in microbial based studies tend to be the Unifrac distances, Bray-Curtis Dissimilarities, and the Jaccard index. We will focus on Bray-Curtis and Jaccard for example purposes. 6.1.2 phyloseq All of beta-diversity options listed above can be calculated by the phyloseq::distance function from the phyloseq package. Requires 2 arguments, the phyloseq object and the name of the index. The index name has to exactly match one of those listed above. Let’s try it. # Returns a matrix, which won&#39;t be shown for the sake of brevity. phylo_BrayDis &lt;- phyloseq::distance(phyobj_d10p25, &quot;bray&quot;) # Jaccard index. phylo_Jaccard &lt;- phyloseq::distance(phyobj_d10p25, &quot;jaccard&quot;) 6.1.3 vegan Calculating beta-diversity indices in vegan is also straight forward. The vegdist function is similar to the phyloseq::distance function in that it requires a matrix in the first argument (not a phyloseq object), followed by the name of the index. Thus, we need to extract the OTU table from the phyloseq object. Although there is an extra step here, there could be instances where you are not using phyloseq. # Load package library(vegan) # Extract OTU table. UCDcounts &lt;- abundances(phyobj_d10p25) # Samples need to be rows for vegdist() to calculate between sample distances/dissimilarities. tUCDcounts &lt;- t(UCDcounts) # Bray index. vegan_BrayDis &lt;- vegdist(tUCDcounts, &quot;bray&quot;) Are the two distance matrices equal? They should be, the phyloseq::distance uses vegdist underneath it’s hood. # Use setequal() to test whether all elements are TRUE setequal(phylo_BrayDis == vegan_BrayDis, TRUE) ## [1] TRUE 6.2 Ordinations Ordinations are used to reduce the dimension of a high-dimensional dataset into a new set of components. Hopefully, just a few of these new components can summarise the variance in the data and then project it onto a 2- or 3-dimensional plot. Those of you who are familiar with Principal Component Analysis should have no problem understanding these methods. We actually will be using PCA later in this chapter! In this section, we will learn how to take our dissimilarity/distance matrix and apply it to Principal Co-ordinate Analysis (PCoA) and non-multi dimensional scaling (NMDS). We will do this in outside of the phyloseq environment for the sake of completeness. 6.2.1 phyloseq The phyloseq package uses the ordinate function to calculate ordinations. The ordinate function is a wrapper for ordinations in other packages, which means it does not contain the coding itself to perform the ordination. It has to call functions from other packages to conduct the ordination. However, it is very convenient because the ordination function preps the data based on the required syntax of the called ordination function. Thus, there is no need for additional coding. Let’s take a look at an example. The ordinate runs Detrended Correspondence Analysis as the default ordination using Bray-Curtis dissilarities. We will update this to ensure we run PCoA. # ordinate() calls the pcoa() function from the ape package. UCD_bray_PCoA &lt;- ordinate(phyobj_d10p25, method=&quot;PCoA&quot;, distance=&quot;bray&quot;) Since ordinate is a wrapper for the ape::pcoa function, the output is what ape::pcoa would show. It is not shown because it is a very verbose output. But as you can see, very straight forward function. Just change the ordination and distance and ordinate will return the output from the supplied ordination. Let’s see one more example. # ordinate() calls the metaMDS() function from the vegan package. UCD_jac_NMDS &lt;- ordinate(phyobj_d10p25, method=&quot;NMDS&quot;, distance=&quot;jaccard&quot;) ## Square root transformation ## Wisconsin double standardization ## Run 0 stress 0.1207804 ## Run 1 stress 0.1207855 ## ... Procrustes: rmse 0.00220263 max resid 0.01280263 ## Run 2 stress 0.1237745 ## Run 3 stress 0.1141577 ## ... New best solution ## ... Procrustes: rmse 0.1020114 max resid 0.2443601 ## Run 4 stress 0.1141591 ## ... Procrustes: rmse 0.0002147099 max resid 0.001120825 ## ... Similar to previous best ## Run 5 stress 0.1141406 ## ... New best solution ## ... Procrustes: rmse 0.004400784 max resid 0.02251077 ## Run 6 stress 0.1141336 ## ... New best solution ## ... Procrustes: rmse 0.003149087 max resid 0.01686469 ## Run 7 stress 0.114135 ## ... Procrustes: rmse 0.0002331117 max resid 0.001254342 ## ... Similar to previous best ## Run 8 stress 0.1143997 ## ... Procrustes: rmse 0.0146326 max resid 0.1022156 ## Run 9 stress 0.1141317 ## ... New best solution ## ... Procrustes: rmse 0.0004295849 max resid 0.002347916 ## ... Similar to previous best ## Run 10 stress 0.1141439 ## ... Procrustes: rmse 0.00164618 max resid 0.008906703 ## ... Similar to previous best ## Run 11 stress 0.1141308 ## ... New best solution ## ... Procrustes: rmse 0.0004978797 max resid 0.002536617 ## ... Similar to previous best ## Run 12 stress 0.4043389 ## Run 13 stress 0.1143973 ## ... Procrustes: rmse 0.01470755 max resid 0.1021652 ## Run 14 stress 0.1141322 ## ... Procrustes: rmse 0.0006315912 max resid 0.003383372 ## ... Similar to previous best ## Run 15 stress 0.1245594 ## Run 16 stress 0.1141397 ## ... Procrustes: rmse 0.002163083 max resid 0.01154892 ## Run 17 stress 0.1141384 ## ... Procrustes: rmse 0.001575663 max resid 0.008491128 ## ... Similar to previous best ## Run 18 stress 0.1141651 ## ... Procrustes: rmse 0.002874055 max resid 0.01361668 ## Run 19 stress 0.1234054 ## Run 20 stress 0.114133 ## ... Procrustes: rmse 0.0008075138 max resid 0.004342552 ## ... Similar to previous best ## *** Solution reached If you recall from Chapter 3.3.2.3, NMDS iteratively computes a best fit model and stops when it reaches a stop criteria. The output here shows the NMDS algorithm searching for the best fit. Let’s look at the final output. # View output UCD_jac_NMDS ## ## Call: ## metaMDS(comm = veganifyOTU(physeq), distance = distance) ## ## global Multidimensional Scaling using monoMDS ## ## Data: wisconsin(sqrt(veganifyOTU(physeq))) ## Distance: jaccard ## ## Dimensions: 2 ## Stress: 0.1141308 ## Stress type 1, weak ties ## Two convergent solutions found after 20 tries ## Scaling: centring, PC rotation, halfchange scaling ## Species: expanded scores based on &#39;wisconsin(sqrt(veganifyOTU(physeq)))&#39; The output provides some details regarding the NMDS ordination. Firstly, the NMDS is configured for 2-dimensions and the scores (sample variance) was centered and rotated similar to PCA. The NMDS did not reach a best fit; it stopped after 20 tries and used the ‘best’ configuration out of the 20 iterations. 6.2.2 vegan Lets look at the metaMDS function in the vegan package a little closer. It has several arguments that allow extensive customization of the ordination. We are going to review a handful of the arguments within metaMDS and how they can be used to configure the NMDS. Please refer to the help page, ?metaMDS, for further details. The first argument, ‘comm’, can either take your actual data (Counts, proportional abundance, transformed counts, etc.) or a symmetric square distance/dissimilarity matrix. The second argument identifies the distance/dissimilarity index if data is loaded instead of a distance/dissimilarity matrix. I recommend inputting a distance/dissimilarity matrix because you have finer control over the input. The next argument, ‘k’, sets the number of dimensions. It is generally recommended to use the minimum number of dimensions, unless you are not finding convergence solutions. The help page recommmends increaseing k by 1 to help find convergence solutions. The next arguments, ‘try’ and ‘trymax’ set the minimum and maximum number of iterations. The ‘engine’ argument defines the MDS variant to run. Leave this alone unless you have a good reason to alter. Lastly, the ‘autotransform’ arguments defaults to TRUE and this allows metaMDS to use detect whether a tranformation is required in the supplied data. It uses a commonly used standardization in ecological sciences, the Wisconsin Double Standardization, after taking the square root of the data. The autotranform argument can be set to FALSE if you’ve already normalized your data (e.g., CLR normalized). The phyloseq NMDS was run with the Jaccard index and stated that a convergence was not found, so we will also use the Jaccard index here and alter the dimensions and max trys. Data is not normalized, so we will keep the ‘autotranform’ argument at it’s default setting. # We will use the transposed count data to ensure Samples are rows. vegan_JaccDis &lt;- vegdist(tUCDcounts, &quot;jaccard&quot;) # Displaying the function on multiple lines to emphasize the multiple arguments. UCD_jac_veganNMDS &lt;- metaMDS( comm = vegan_JaccDis, k = 3, try=10, trymax=50 ) ## Run 0 stress 0.1264836 ## Run 1 stress 0.1266075 ## ... Procrustes: rmse 0.006092033 max resid 0.03708389 ## Run 2 stress 0.1264853 ## ... Procrustes: rmse 0.0006757276 max resid 0.002130777 ## ... Similar to previous best ## Run 3 stress 0.1264785 ## ... New best solution ## ... Procrustes: rmse 0.00216278 max resid 0.0108295 ## Run 4 stress 0.1264755 ## ... New best solution ## ... Procrustes: rmse 0.0009350028 max resid 0.003242791 ## ... Similar to previous best ## Run 5 stress 0.1264815 ## ... Procrustes: rmse 0.00118572 max resid 0.004663852 ## ... Similar to previous best ## Run 6 stress 0.1265959 ## ... Procrustes: rmse 0.005236279 max resid 0.03320815 ## Run 7 stress 0.1264837 ## ... Procrustes: rmse 0.001924231 max resid 0.009461896 ## ... Similar to previous best ## Run 8 stress 0.1264779 ## ... Procrustes: rmse 0.0007789873 max resid 0.003198285 ## ... Similar to previous best ## Run 9 stress 0.1357964 ## Run 10 stress 0.1393982 ## *** Solution reached Using 3 dimensions allows us to reach the best solution with less iterations. Let’s see the output. # View output UCD_jac_veganNMDS ## ## Call: ## metaMDS(comm = vegan_JaccDis, k = 3, try = 10, trymax = 50) ## ## global Multidimensional Scaling using monoMDS ## ## Data: vegan_JaccDis ## Distance: jaccard ## ## Dimensions: 3 ## Stress: 0.1264755 ## Stress type 1, weak ties ## Two convergent solutions found after 10 tries ## Scaling: centring, PC rotation ## Species: scores missing Turns out the ‘Stress’ statistic is lower in the previous NMDS. This is likely due to the normalization that metaMDS applied to the count data prior to determining the Jaccard Indices. Still, we have good/fair stress level at &lt;0.2 (&gt; 0.3 is considered poor). I would expect some subtle differences in the plots. 6.2.3 ape The pcoa funtion is straight forward to run; all you have to input is the symmetric square distance/dissimilarity matrix. Note, the help page, ?pcoa does not recommend using proportionally scaled data in PCoA and suggests PCA for that data. We will use Bray-Curtis Dissimilarities on non-normalized counts for this example. Recall from above that the output from pcoa is extremely verbose, therefore, we will not print the output here. # Load library if necessary library(ape) # We will use the Bray Dissimilarities object that we calculated earlier. UCD_bray_apePCoA &lt;- pcoa(vegan_BrayDis) 6.3 Visualizing ordinations We use ordinations to summarize high-dimensional data into a set of new components. These components can be broken down to show the variance associated with samples, often referred to as “scores”. Plotting the “score” values can sometimes show which samples are more similar to each other and dissimilar from others. More similar scores will have smaller distances between them and cluster closer together, whereas dissimilar scores will have large distances between them and cluster farther away from each other. As with most things in R, there are several ways to plot this data. We will first describe the phyloseq workflow and then provide examples in base R. 6.3.1 phyloseq The plot_ordination function from phyloseq is probably the most user-friendly option for plotting. It integrates the Sample Metadata in the phyloseq object to aid in the visualization of sample differences, so everything is streamlined. The output of plot_ordination uses the popular plotting packge, “ggplot2”, as the graphical output, thus additional graphical options (e.g., point sizing, colors, etc) are customizeable using the ggplot2 syntax. I happen to be more versed in graphical options using base R functions, so I will refer you to ggplot2’s reference page. In addition, the phyloseq website has a tutorial page dedicated to plotting ordinations. plot_ordination requires 2 arguments at minimum: 1) a phyloseq object that created an 2) ordination from ordinate. A third argument specifies whether you want to plot ‘samples’, ‘taxa’, or a ‘biplot’. The latter of these combines samples and taxa information on the same plotting space. This works well with a small set of samples and taxa, but not with the large amount of taxa in our plot. That option may work will at the phylum or class level, but not at the more granular taxonomic levels. Let’s plot the PCoA that we created with ordinate. # Minimal example plot_ordination(phyobj_d10p25, UCD_bray_PCoA) There is no discriminant information on the plot. Ordinations are considered an un-supervised multivariate technique, meaning that these techniques display the inherent variation within the data and are not directed to maximize the variation based on a modeling construct. We can define colors for samples based on our categorical metadata. If variance is inherently explained by our categorization within these components, then we should see similar and dissimilar clusters of samples. Let’s see if there is any variation by diabetic group. We can do this by changing the ‘color’ argument to a column name from the Sample Metadata. We can obtain these names using the sample_variables function if we need the exact cases. # Sample column names sample_variables(phyobj_d10p25) ## [1] &quot;SampleID&quot; &quot;Rat&quot; &quot;GroupAbbrev&quot; &quot;Collection&quot; # Will use the &quot;GroupAbbrev&quot; column plot_ordination(phyobj_d10p25, UCD_bray_PCoA, color=&quot;GroupAbbrev&quot;) Okay, good and bad things about this plot. The default size of the points are very small and discriminating with 5 colors can be challenging. This is all customizable by adding on additional ggplot2 functions; however, we will not cover this in this workshop. The good part of this plot is you can see discrimination of experimental groups! The PD and RD groups cluster together at the top of the graph, the D3M and D6M groups drift out toward the positive values on the first axis, while the LSD group clusters together on the negative values of the first axis. Let’s talk a little bit more about how we would interpret this output. Note that each axis has a percentage associated with it, 18.8% for the Axis-1 (the x-axis) and 14% for Axis-2 (y-axis). This is the proportion of the overall variance explained by this component. Thus, the information presented here, e.g., discrimination of group clusters, on 2 components explains 32.8% (18.8% + 14%) of the overall variation in the data. We are also looking at how a particular component contains discriminant information, i.e., clusters of data that are separate from each along a particular component. For example, LSD rats are discriminated from UCD-T2DM Rat groups along the first component (Axis-1). An argument could be made that the RD and PD groups have a bit of distance from the D3M and D6M group on component 1, but overall, the distance between LSD rats and UCD-T2DM rats is much greater. Thus, we would assume that the variance associated with component 1 (18.8%) is mainly attributed to the LSD and UCD-T2DM rat discrimination. Variation associated with the cluster of PD and RD rats relative to the remaining rats is also apparent on component 2. While the distance is not as large as the LSD - UCD-T2DM discrimination on component 1, it is still quite apparent. When you consider the score values (i.e., samples) only on Axis-2, the LSD, D3M, and D6M do not differ from each other (unlike on component 1). However, you could almost run a line through component 2, just above zero, that will discriminate the PD and RD rats from the LSD, D3M and D6M rats. Thus, we can make the statement that component 2 contains variation discriminating the cluster of PD &amp; RD rats from the other groups. Now let’s see what a NMDS ordination looks like using Bray-Curtis Dissimilarities using the phyloseq workflow. # We fit NMDS with Jaccard index in the previous example # Turned the iteration messages off for the sake of brevity. UCD_bray_NMDS &lt;- ordinate(phyobj_d10p25, method=&quot;NMDS&quot;, distance=&quot;bray&quot;) # Will use the &quot;GroupAbbrev&quot; column to layer colors on scores. plot_ordination(phyobj_d10p25, UCD_bray_NMDS, color=&quot;GroupAbbrev&quot;) Very different than the PCoA. As these techniques are un-supervised and summarise the overall variance, they are subject to capture variance explained by outlying samples. This is also why PCA is sometime used to identify outliers in multivariate data. In this graph, you can see some semblance of the PCoA; for example, the LSD rats cluster together and away from the UCD-T2DM rats, and the PD/RD rats cluster indistinguishable from each other and are more dissimilar than LSD and D3M/D6M rats. However, none of this is strongly interpretable on this ordination because of the presence of outliers. Also, note that the % explained variance is not added to the axis label. This is because the NMDS algorithm iteratively fits the points until the samples most closely matches the dissimilarities in the original matrix. Thus, NMDS does not partition overall variance similar to PCA and PCoA. The score units are arbitrary and only define where the orientation of the scores. Why do we have clear outliers in the NMDS ordination and not in the PCoA ordination? Could be various reasons, but likely due to the ordinations themselves, but could also be the default normalization in the metaMDS function. Fortunately, you can pass arguments from metaMDS to ordinate as though they are the same functions. Let’s try without the transformation, 3 components, and alter the min/max iterations. # Make another NMDS UCD_bray_NMDSconfig &lt;- ordinate(phyobj_d10p25, method=&quot;NMDS&quot;, distance=&quot;bray&quot;, autotransform=FALSE, try=10, trymax=50, k=3) plot_ordination(phyobj_d10p25, UCD_bray_NMDSconfig, color=&quot;GroupAbbrev&quot;) An outlier still is present along component 2, however we now have a little better resolution among the LSD and UCD-T2DM samples along component 1. 6.3.2 Base R There may be beta-diversity workflows that are not compatible with phyloseq. For example, it is recommended to use PCA after using a CLR transformation, and PCA is not an ordination option in plot_ordination. We will use this section to describe the non-phyloseq plotting workflow. Let’s see how different normalizations influence the ordinations. We’ve already seen an example of a PCoA using Bray-Curtis Dissimilarities on untransformed data. Let’s apply various normalizations and see if the clusters are presented different. These examples includes immediate and advance coding. Examples of this workflow were shown in Chapters 2.8.4 and 2.8.6. As a reference, we will redo the PCoA using Bray-Curtis dissimilarities on non-normalized data. We have already calculated the PCoA in Section 6.2.3, so we just need to extract the scores from this object. # Most functions contain their data outputs in lists. So many times you can use $ to extract the output. # Use the str() function on the object can help identify the named element to extract. UCD_bray_pcoa_scores &lt;- UCD_bray_apePCoA$vectors %&gt;% as.data.frame() UCD_bray_pcoa_scores$SampleID &lt;- rownames(UCD_bray_pcoa_scores) # Extract Sample Metadata UCDrats_MetaData &lt;- meta(phyobj_d10p25) # Join data frames UCD_bray_pcoa_scsDF &lt;- inner_join(UCDrats_MetaData, UCD_bray_pcoa_scores, by=&quot;SampleID&quot;) # See the help page, ?plot, for more options plot( # Input data in x and y arguments x = UCD_bray_pcoa_scsDF[,&quot;Axis.1&quot;], y = UCD_bray_pcoa_scsDF[,&quot;Axis.2&quot;], # pch determines the shape of the point pch = 21, # bg denotes the point background color - assiging the colors based on the factor levels. bg = c(&quot;red&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;pink&quot;)[factor(UCD_bray_pcoa_scsDF$GroupAbbrev)], # ces denotes the size of the points cex = 1.5, # xlab and ylab provides the axis labels xlab = &quot;PCoA Component 1&quot;, ylab = &quot;PCoA Component 2&quot; ) # Add figure legend legend(x=&quot;topright&quot;, legend=c(&quot;D3M&quot;, &quot;D6M&quot;, &quot;LSD&quot;, &quot;PD&quot;, &quot;RD&quot;), pch=21, pt.bg=c(&quot;red&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;pink&quot;)) Note, that we have to calculate the % variance for each component, unlike plot_ordination. Now lets see what happens when we normalize the data prior to analysis. We will first try the log upper quartile normalization that we referenced in Chapter 3.5.2. However, we run into a couple problems, 1) Conventionally, Bray-Curtis distances are run on counts and 2) that means the negative values in the normalization are incompatible with Bray-Curtis. Instead, we will use Principal Component Analysis as suggested by the pcoa help page. # Impute zeros by shifting values + 1 UCDcounts_plus1 &lt;- UCDcounts + 1 # Identify the 75 quartile based on only the count distribution. # Values &gt; 1 counts75distribution &lt;- apply(UCDcounts_plus1, 2, function(x) quantile((x[x &gt; 1] - 1), 0.75)) # Equally these values across all rows UCD_UQadj &lt;- sweep(UCDcounts_plus1, 2, counts75distribution, &quot;/&quot;) # Apply a log transformation UCD_logUQnorm &lt;- log(UCD_UQadj) # Transpose so rows are Samples tUCD_logUQnorm &lt;- t(UCD_logUQnorm) # Will use the prcomp() function to perform PCA tUCD_logUQnorm_pca &lt;- prcomp(tUCD_logUQnorm) # Extract scores and make new column to join with metadata tUCD_logUQnorm_pca_scores &lt;- tUCD_logUQnorm_pca$x %&gt;% as.data.frame() tUCD_logUQnorm_pca_scores$SampleID &lt;- rownames(tUCD_logUQnorm_pca_scores) # Join data frames UCD_logUQnorm_pca_scsDF &lt;- inner_join(UCDrats_MetaData, tUCD_logUQnorm_pca_scores, by=&quot;SampleID&quot;) # Plot the data plot( x = UCD_logUQnorm_pca_scsDF[,&quot;PC1&quot;], y = UCD_logUQnorm_pca_scsDF[,&quot;PC2&quot;], pch = 21, bg = c(&quot;red&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;pink&quot;)[factor(UCD_logUQnorm_pca_scsDF$GroupAbbrev)], cex = 1.5, xlab = &quot;PCA Component 1&quot;, ylab = &quot;PCA Component 2&quot; ) legend(x=&quot;bottomleft&quot;, legend=c(&quot;D3M&quot;, &quot;D6M&quot;, &quot;LSD&quot;, &quot;PD&quot;, &quot;RD&quot;), pch=21, pt.bg=c(&quot;red&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;pink&quot;)) We see that there is now a clear separation between two clusters of LSD rats, but overall, the LSD rats are separated from the UCD-T2DM animals. There is also a set of PD rats more aligned with the D3M and D6M rats, which was not as well apparent in the non-normalized data. Where could this be from? Let’s show the same plot, but overlay with the Collection Year. plot( x = UCD_logUQnorm_pca_scsDF[,&quot;PC1&quot;], y = UCD_logUQnorm_pca_scsDF[,&quot;PC2&quot;], pch = 21, bg = c(&quot;blue&quot;, &quot;orange&quot;)[factor(UCD_logUQnorm_pca_scsDF$Collection)], cex = 1.5, xlab = &quot;PCA Component 1&quot;, ylab = &quot;PCA Component 2&quot; ) legend(x=&quot;bottomleft&quot;, legend=c(&quot;Y14&quot;, &quot;Y16&quot;), pch=21, pt.bg=c(&quot;blue&quot;, &quot;orange&quot;)) It is clear in this ordination that there is variance associated with a batch effect in the data. Let’s try this again, but with the CLR transformation. # load libraries if necessary # library(zCompositions) # library(rgr) # Impute zeros with Bayesian-multiplicative treatment UCDcounts_BMz &lt;- cmultRepl(UCDcounts, method=&quot;CZM&quot;, output=&quot;p-counts&quot;) # Taxa needs be rows and samples need to be columns UCD_BMz_clr &lt;- clr(UCDcounts_BMz, ifwarn=FALSE) # PCA needs samples as columns tUCD_BMz_clr &lt;- t(UCD_BMz_clr) # tUCD_BMz_clr &lt;- t(UCDcounts_BMz) # Will use the prcomp() function to perform PCA UCD_BMz_clr_pca &lt;- prcomp(tUCD_BMz_clr) # UCD_BMz_clr_pca &lt;- prcomp(scale(tUCD_BMz_clr), center=FALSE) # Extract scores and make new column to join with metadata UCD_BMz_clr_pca_scores &lt;- UCD_BMz_clr_pca$x %&gt;% as.data.frame() UCD_BMz_clr_pca_scores$SampleID &lt;- rownames(UCD_BMz_clr_pca_scores) # Join data frames UCD_BMz_clr_pca_scsDF &lt;- inner_join(UCDrats_MetaData, UCD_BMz_clr_pca_scores, by=&quot;SampleID&quot;) # Plot the data plot( x = UCD_BMz_clr_pca_scsDF[,&quot;PC1&quot;], y = UCD_BMz_clr_pca_scsDF[,&quot;PC2&quot;], pch = 21, bg = c(&quot;red&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;pink&quot;)[factor(UCD_BMz_clr_pca_scsDF$GroupAbbrev)], cex = 1.5, xlab = &quot;PCA Component 1&quot;, ylab = &quot;PCA Component 2&quot; ) legend(x=&quot;topleft&quot;, legend=c(&quot;D3M&quot;, &quot;D6M&quot;, &quot;LSD&quot;, &quot;PD&quot;, &quot;RD&quot;), pch=21, pt.bg=c(&quot;red&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;pink&quot;)) Not too different from the PCoA ordination using Bray-Curtis Dissimilarities on non-normalized data. The results are just rotated to the right. Here, we are using the covariance-matrix on CLR normalized data. Although the collection year plot is not provided for the CLR PCA, the 5 green LSD samples projecting away from the tight green LSD cluster are separated by collection year. Overall, the various ordinations we’ve assessed on the UCD-T2DM data described variation associated with the experimental design, and unfortunately, variation associated with the sub-optimal collection periods (i.e., batch effect). There is also the suggestion of outliers in the data. Further analysis may be required without these outlier samples. 6.4 PERMANOVA We’ve visualized beta-diversity, but we still need to determine whether the discrimination observed in the ordinations are statistically significant. Since the ordinations are un-supervised analyses, there are no hypothesis testing within their algorithms. Generally, the accepted method to determine whether there is a statistical difference in beta-diversity is through Permutational Multivariate Analysis of Variance (PERMANOVA). Let’s break down the components of PERMANOVA. It is related to an ANOVA in that it partitions variation in response to provided factor(s), but it occurs in multivariate data cloud relating to a dissimilarity or distance matrix. The statistical inferences (i.e., p-value) is determined via a distribution-free permutational technique. So, “PER” refers to the permutational statistical inference, “M”, refers to the fact it is Multivariate, and “ANOVA” because it is an extension of ANOVA. Since PERMANOVA is based on the ANOVA framework, study designs that require ANOVA testing can be adapted for PERMANOVA. For example, 3-way ANOVA design with main effects and all interactions can be utilized with PERMANOVA. In addition, covariate factors can also be utilized with PERMANOVA. It should be noted again, that PERMANOVA is routinely used to demonstrate whether discriminant clusters in an ordinations are statistically significant, they do not always sync together. Remember, we can only view up to 3 dimensions of an ordinations like PCoA and PCA. Sometimes 3 components only represent a small to medium amount of the variance in the data. The PERMANOVA interprets all of the variation, not just what we see in the first few components. To my knowledge, there has not been a way to run a PERMANOVA through phyloseq. If you use phyloseq you have to 1) calculate dissimilarity/distance matrix and 2) extract the Sample Metadata. The actual function we’ll use is the adonis and adonis2 function from the vegan package. Both functions use the formula syntax, but it is a little different than the formula for aov. The left side of the ~ (tilde) is the dissimilarity/distance matrix, and the right side are the factors that you want to measure. The factor(s) link to your metadata file that you supply to the ‘data’ argument. We will demonstrate a 2-way PERMANOVA using the UCD-T2DM rat data. Main effects of Group and Collection will be assessed with an interaction term. We will also start from the phyloseq object. # Calculate dissimilarity matrix phylo_BrayDis &lt;- phyloseq::distance(phyobj_d10p25, &quot;bray&quot;) # Extract sample Metadata UCDrats_MetaData &lt;- meta(phyobj_d10p25) # Assess PERMANOVA adonis(phylo_BrayDis ~ GroupAbbrev + Collection + GroupAbbrev:Collection, data=UCDrats_MetaData, permutations=500) ## ## Call: ## adonis(formula = phylo_BrayDis ~ GroupAbbrev + Collection + GroupAbbrev:Collection, data = UCDrats_MetaData, permutations = 500) ## ## Permutation: free ## Number of permutations: 500 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## GroupAbbrev 4 2.4174 0.60435 5.2189 0.26701 0.001996 ** ## Collection 1 0.3546 0.35465 3.0626 0.03917 0.001996 ** ## GroupAbbrev:Collection 3 0.9548 0.31826 2.7484 0.10546 0.001996 ** ## Residuals 46 5.3268 0.11580 0.58836 ## Total 54 9.0537 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output provides an ANOVA table. We see similar statistics that we would see from a regular ANOVA, e.g., degrees of freedom, sum of squares, F-statistic, and a P-value. Note, the F-statistic is technically a “psuedo F statistic”. These statistics can be intepreted similar to a regular ANOVA. Hence, the presence of a significant interaction terms suggest we should be assessing Group differences within Collection years. Let’s do it. # We extracted the metadata in the above chapter. d10p25CollectionYear &lt;- meta(phyobj_d10p25)$Collection # Make the logical statement using the %in% (match) operator CollectionYear14Logic &lt;- d10p25CollectionYear %in% &quot;Y14&quot; CollectionYear16Logic &lt;- d10p25CollectionYear %in% &quot;Y16&quot; # Use the prune_samples() function # First argument is the logical statement, the second is the phyloseq object phylo_year2014 &lt;- prune_samples(CollectionYear14Logic, phyobj_d10p25) phylo_year2016 &lt;- prune_samples(CollectionYear16Logic, phyobj_d10p25) # n of 2 in D6M group. Will drop these samples from analysis. phylo_year2016 &lt;- prune_samples(meta(phylo_year2016)$GroupAbbrev != &quot;D6M&quot;, phylo_year2016) # Calculate dissimilarity matrix phylo_BrayDis2014 &lt;- phyloseq::distance(phylo_year2014, &quot;bray&quot;) phylo_BrayDis2016 &lt;- phyloseq::distance(phylo_year2016, &quot;bray&quot;) # Extract sample Metadata UCDrats2014_MetaData &lt;- meta(phylo_year2014) UCDrats2016_MetaData &lt;- meta(phylo_year2016) # Assess PERMANOVA year 2014 adonis(phylo_BrayDis2014 ~ GroupAbbrev, data=UCDrats2014_MetaData, permutations=500) ## ## Call: ## adonis(formula = phylo_BrayDis2014 ~ GroupAbbrev, data = UCDrats2014_MetaData, permutations = 500) ## ## Permutation: free ## Number of permutations: 500 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## GroupAbbrev 3 1.4137 0.47123 3.4071 0.36218 0.001996 ** ## Residuals 18 2.4895 0.13831 0.63782 ## Total 21 3.9032 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 adonis(phylo_BrayDis2016 ~ GroupAbbrev, data=UCDrats2016_MetaData, permutations=500) ## ## Call: ## adonis(formula = phylo_BrayDis2016 ~ GroupAbbrev, data = UCDrats2016_MetaData, permutations = 500) ## ## Permutation: free ## Number of permutations: 500 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## GroupAbbrev 3 1.3506 0.45020 4.6813 0.34217 0.001996 ** ## Residuals 27 2.5966 0.09617 0.65783 ## Total 30 3.9472 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The group effect is significant in both years. But what groups differ from each other. We saw that the PD and RD groups were very similar in the ordinations, so it those groups should not differ between each other. The coding to assess pairwise differences becomes more advanced, but can be done. First, we need to determine all the pairwise combinations and then use those in a sapply function. This examples includes immediate and advance coding. Examples of this workflow were shown in Chapters 2.8.4 and 2.8.6. # Identify pairwise comparisons year2016_combinations &lt;- combn(unique(meta(phylo_year2016)$GroupAbbrev),2) # Use sapply() to extract data from phyloseq object and then run PERMANOVA year2016_permanovaPvals &lt;- sapply(1:ncol(year2016_combinations), function(x) { # Extract combinations combo &lt;- year2016_combinations[,x] # Extract metadata Year16group &lt;- meta(phylo_year2016)$GroupAbbrev # Make logical statement based on pairwise combinations Year16subset &lt;- Year16group %in% combo # Subset phyloseq object phylo_subset &lt;- prune_samples(Year16subset, phylo_year2016) # Calculate dissimilarity matrix BrayDis_subset &lt;- phyloseq::distance(phylo_subset, &quot;bray&quot;) # Extract sample metadata subsetMetaData &lt;- meta(phylo_subset) # Run PERMANOVA permanova &lt;- adonis(BrayDis_subset ~ GroupAbbrev, data=subsetMetaData, permutations=500) # Isolate p-value from permanova object, round to the 5th digit, and return. round(permanova$aov.tab[1,6], 5) }) # Make vector of pairwise combinations paircomp &lt;- apply(year2016_combinations, 2, paste, collapse=&quot; - &quot;) # Make data frame with pairwise comparison, p-value, and FDR correction. paircompDF &lt;- data.frame(Comparison = paircomp, Pvalue = year2016_permanovaPvals, FDR = p.adjust(year2016_permanovaPvals, &quot;fdr&quot;)) # Printing data frame as a table in the text... Comparison Pvalue FDR LSD - RD 0.00200 0.004000 LSD - PD 0.00200 0.004000 LSD - D3M 0.00399 0.005985 RD - PD 0.32535 0.325350 RD - D3M 0.00200 0.004000 PD - D3M 0.00599 0.007188 Note, the comparison between PD and RD rats was not significant. All other comparisons were significant, which sync with the visualizations from the PCoA ordination in Section 6.3.1. 6.5 Conclusion We did not cover how to identify which taxa are driving the sample discrimination in the ordinations in this chapter. There are ways to visualize this, but it does no good to plot 400 taxa along with your samples. These analyses describe the global microbial differences between samples. The next chapter will describe how to identify differences in individual taxa, which puts these global changes in context. Also, feel good here! The tools used to visualize and analyze beta-diversity are typically thought of as advanced statistics. Anytime you’re dealing with hundreds to thousands of variable, you will have to use some more advanced tools. It shouldn’t make complete sense over a single reading, especially if this is your first introduction to microbial sequencing analysis and R. So keep at it and know that you’ve made it pretty far already! "],
["differential-abundance.html", "Chapter 7 Differential Abundance 7.1 Rank analysis 7.2 DESeq2 7.3 metagenomeSeq 7.4 ALDEx2 7.5 Conclusion", " Chapter 7 Differential Abundance The next step in assessing differences in microbial sequencing data is to determine which taxa differ between experimental groups. Be warned, this area is still under active research and debate. I would not be surprised if some of the approaches covered in this chapter will fall out of favor after several years. Similar to beta-diversity, results can be variable depending on the normalization and statistical approach. Some approaches are also more conservative than others. Recently, I’ve adopted a “survival analysis” approach and identify taxa that intersect among several different approaches. I would argue that the phyloseq environment makes alpha- and beta-diversity assessment fairly straight-forward in R. Unfortunately, differential abundance analysis occurs entirely outside of the phyloseq environment. There are also different packages that we’ll introduce in this chapter that require modification of the the data. Some are directly related to RNA-seq analysis and build an object similar to the phyloseq object, but these new objects are typically unrelated to microbial data. We will navigate these approaches with care, but in depth detail of these methods are beyond the scope of this class. Unfortunately, there is no way to avoid immediate and advance coding in this chapter. Some of the statistical approaches require running a single test on all taxa, so we will have to use functions that can loop similar code (e.g., the apply family of functions). Other approaches have methods embedded in functions that do not rely on loops, but have additional steps and complex functions. Please refer to Chapter 2 to help with some of these concepts. Links to additional help will be provided throughout. 7.1 Rank analysis Early studies of microbial sequencing data used non-parametric rank-order statistics for group comparisons, e.g., Mann-Whitney U Test and Kruskal-Wallis 1-way analysis of variance by rank. This approach was adopted due to the fact that count data and proportional abundances are rarely normally distributed. However, some have criticized this approach due to the fact that the underlying data is compositional and should be handled by compositional data analysis approaches. Furthermore, the high abundance of zeros in low abundant taxa inflate the number of ties in the rank and limit the power to detect differences in low abundant taxa. Still, it is widely used by many investigators and can be more conservative then other methods we’ll cover. We will use the Mann-Whitney U test to determine which taxa differ between LSD and pre-diabetic (PD) UCD-T2DM rats under various normalizations. Since this is 16S rRNA data and we have poor resolution at Species level, we will evaluate group differences at Genus level for the ease of interpretation. # Aggregate to Genus level phyobj_d10p25_G &lt;- tax_glom(phyobj_d10p25, &quot;Genus&quot;) # Subset phyloseq object phyloG_LSDPD &lt;- prune_samples(meta(phyobj_d10p25_G)$GroupAbbrev %in% c(&quot;LSD&quot;, &quot;PD&quot;), phyobj_d10p25_G) # Extract Sample Meta Data and make new ID column with rownames LSDPD_Gmeta &lt;- meta(phyloG_LSDPD) %&gt;% as.data.frame() LSDPD_Gmeta$SampleID &lt;- rownames(LSDPD_Gmeta) # Exract OTU table LSDPD_Gcounts &lt;- abundances(phyloG_LSDPD) # Need to join with meta data, so samples need to be rows. # transpose data and make new ID column with rownames tLSDPD_Gcounts &lt;- t(LSDPD_Gcounts) %&gt;% as.data.frame() tLSDPD_Gcounts$SampleID &lt;- rownames(tLSDPD_Gcounts) # Join count and meta data LSDPD_Gcountmeta &lt;- full_join(LSDPD_Gmeta, tLSDPD_Gcounts, by=&quot;SampleID&quot;) # Reshape data to tall format # The &#39;-&#39; prefix denotes columns that should not have data reshaped into the new Taxa column LSDPD_G_tall &lt;- LSDPD_Gcountmeta %&gt;% gather(Taxa, counts, -SampleID, -Rat, -GroupAbbrev, -Collection) # Identify the unique taxa now listed in the &quot;Taxa&quot; column LSDPD_G_uniqueOTU &lt;- unique(LSDPD_G_tall$Taxa) # Use sapply() to loop the Mann-Whitney U test across all unique taxa LSDPD_MW_P &lt;- sapply(LSDPD_G_uniqueOTU, function(x) { # Subset data within function DAT &lt;- LSDPD_G_tall[LSDPD_G_tall$Taxa %in% x,] # Run Mann Whitney U on subsetted data MWU &lt;- wilcox.test(counts ~ GroupAbbrev, data=DAT) # Extract P.value and return MWU$p.value }) # Make new data frame containing taxa identifier, p value, and FDR. LSDPD_MW_P_df &lt;- data.frame(Taxa=names(LSDPD_MW_P), P=round(LSDPD_MW_P, 5), FDR=round(p.adjust(LSDPD_MW_P, &quot;fdr&quot;), 5)) # Exract Taxa table, no extractor function, so need to manually extract. LSDPD_Gtaxa &lt;- tax_table(phyloG_LSDPD)@.Data %&gt;% as.data.frame() # Make Taxa column of row names for unique identifier, to join with p value data frame LSDPD_Gtaxa$Taxa &lt;- rownames(LSDPD_Gtaxa) # Join P value and taxa data frames LSDPD_MW_results &lt;- full_join(LSDPD_Gtaxa, LSDPD_MW_P_df, by=&quot;Taxa&quot;) # Subset significant results LSDPD_MW_Sigresults &lt;- LSDPD_MW_results[LSDPD_MW_results$FDR &lt; 0.05,] # Dispaly with trimmed columns LSDPD_MW_Sigresults[,c(&quot;Phylum&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;FDR&quot;)] Phylum Family Genus FDR p__Firmicutes f__Streptococcaceae g__Streptococcus 0.00125 p__Proteobacteria f__Desulfovibrionaceae g__ 0.02517 p__Proteobacteria f__Pasteurellaceae g__Aggregatibacter 0.00443 p__Proteobacteria f__Desulfovibrionaceae g__Desulfovibrio 0.00921 p__Firmicutes f__Lactobacillaceae g__Lactobacillus 0.00125 p__Firmicutes f__Peptococcaceae g__rc4-4 0.00313 p__Elusimicrobia f__Elusimicrobiaceae g__ 0.00608 p__Firmicutes f__Lachnospiraceae g__Roseburia 0.00022 p__Proteobacteria f__Enterobacteriaceae g__ 0.00022 p__Firmicutes f__Veillonellaceae g__Phascolarctobacterium 0.00035 p__Firmicutes f__Ruminococcaceae g__Oscillospira 0.01489 p__Bacteroidetes f__S24-7 g__ 0.01489 p__Actinobacteria f__Bifidobacteriaceae g__Bifidobacterium 0.01553 p__Cyanobacteria f__ g__ 0.00022 p__Firmicutes f__Erysipelotrichaceae g__Allobaculum 0.00125 p__Firmicutes f__[Mogibacteriaceae] g__ 0.01286 p__Firmicutes f__Dehalobacteriaceae g__Dehalobacterium 0.00117 p__Verrucomicrobia f__Verrucomicrobiaceae g__Akkermansia 0.00022 p__Firmicutes f__Lachnospiraceae g__Dorea 0.03324 Almost 20 genera are altered between the LSD and PD rats. However, we do not have any information regarding the dispersion of the data. Let’s calculate the median and IQR since we used a non-parametric method. # Can calculate this from LSDPD_G_tall object using tidyverse # Note the use of the pipe, %&gt;%, the function sequence will terminate after the last pipe. LSDPD_G_MEDIAN &lt;- LSDPD_G_tall %&gt;% # Define columns to get groups. # In our case, we want medians by Taxa and GroupAbbrev group_by(Taxa, GroupAbbrev) %&gt;% # Summarize defines the functions that we want to group by # Use the column name in which you want the function to run on summarise(MEDIAN = median(counts), IQR = IQR(counts)) # View object LSDPD_G_MEDIAN ## # A tibble: 76 x 4 ## # Groups: Taxa [38] ## Taxa GroupAbbrev MEDIAN IQR ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1076587 LSD 10 7.25 ## 2 1076587 PD 17 12.5 ## 3 1076969 LSD 11 13.2 ## 4 1076969 PD 60 50.5 ## 5 1107027 LSD 490 226. ## 6 1107027 PD 2498 2871 ## 7 1108422 LSD 7046. 1594. ## 8 1108422 PD 5911 2291 ## 9 1108599 LSD 2351 3064. ## 10 1108599 PD 4039 1948. ## # ... with 66 more rows We could stop here, but it would be better to organize the Mann-Whitney results with the median/IQR data. With a bit of data wrangling, we can do it. # Will use tidyverse workflow to do this. LSDPD_G_MEDIAN_wide &lt;- LSDPD_G_MEDIAN %&gt;% # Use gather() to combine MEDIAN and IQR columns into a single column gather(STATS, value, -Taxa, -GroupAbbrev) %&gt;% # Use mutate() to make new columns that combines GroupAbbrev and STATS mutate(GroupStat = paste(GroupAbbrev, STATS, sep=&quot;_&quot;)) %&gt;% # Use dplyr::select() to remove GroupAbbrev and STATS columns dplyr::select(-GroupAbbrev, -STATS) %&gt;% # Use spread() to distribute value by GroupStat spread(GroupStat, value) # Join with LSDPD_MW_results LSDPD_MW_Final &lt;- full_join(LSDPD_MW_results, LSDPD_G_MEDIAN_wide, by=&quot;Taxa&quot;) # Display significant taxa with trimmed columns LSDPD_MW_Final[LSDPD_MW_Final$FDR &lt; 0.05, c(&quot;Genus&quot;, &quot;FDR&quot;,&quot;LSD_MEDIAN&quot;,&quot;LSD_IQR&quot;,&quot;PD_MEDIAN&quot;,&quot;PD_IQR&quot;)] Genus FDR LSD_MEDIAN LSD_IQR PD_MEDIAN PD_IQR g__Streptococcus 0.00125 11.0 13.25 60 50.5 g__ 0.02517 2351.0 3063.75 4039 1947.5 g__Aggregatibacter 0.00443 0.5 1.25 11 18.0 g__Desulfovibrio 0.00921 193.0 183.00 582 455.0 g__Lactobacillus 0.00125 490.0 225.50 2498 2871.0 g__rc4-4 0.00313 6.0 9.75 49 55.0 g__ 0.00608 34.0 41.25 5 13.5 g__Roseburia 0.00022 0.0 2.50 57 69.5 g__ 0.00022 0.5 1.25 62 97.5 g__Phascolarctobacterium 0.00035 0.0 0.00 153 294.0 g__Oscillospira 0.01489 1318.5 275.50 1898 648.5 g__ 0.01489 7045.5 1593.75 5911 2291.0 g__Bifidobacterium 0.01553 43.0 60.00 3 19.5 g__ 0.00022 520.5 557.75 12 17.5 g__Allobaculum 0.00125 376.0 849.25 0 0.0 g__ 0.01286 6.5 5.25 14 11.0 g__Dehalobacterium 0.00117 17.0 12.75 0 1.5 g__Akkermansia 0.00022 3645.0 4157.75 0 29.0 g__Dorea 0.03324 10.0 7.25 17 12.5 The table only shows the relevant columns for the sake of brevity in the web-book. There are several genera that are only identified by “g__“; these are undefined genera that likely were defined only at a higher taxonomy level. You will see the full taxonomy if you do not filter columns in LSDPD_MW_Final. Major results from this analysis suggest that LSD rats have lower abundances of Lactobacillus and greater abundances of Akkermansia relative to UCD-T2DM rats that have yet to develop diabetes. Let’s follow the same workflow, but with proportional abundances. We can start after we’ve extracted out of the phyloseq object. # Convert counts to relative abundance with sweep() function # Samples must add to 1, so sum by column and sweep sums by columns LSDPD_Gprop &lt;- sweep(LSDPD_Gcounts, 2, colSums(LSDPD_Gcounts), &quot;/&quot;) # Need to join with meta data, so samples need to be rows. # transpose data and make new ID column with rownames tLSDPD_Gprop &lt;- t(LSDPD_Gprop) %&gt;% as.data.frame() tLSDPD_Gprop$SampleID &lt;- rownames(tLSDPD_Gprop) # Join count and meta data LSDPD_Gpropmeta &lt;- full_join(LSDPD_Gmeta, tLSDPD_Gprop, by=&quot;SampleID&quot;) # Reshape data to tall format # The &#39;-&#39; prefix denotes columns that should not have data reshaped into the new Taxa column LSDPD_Gprop_tall &lt;- LSDPD_Gpropmeta %&gt;% gather(Taxa, prop, -SampleID, -Rat, -GroupAbbrev, -Collection) # Turn to percentages for rounding and easier interpretation LSDPD_Gprop_tall$prop &lt;- round(LSDPD_Gprop_tall$prop * 100, 3) # Identify the unique taxa now listed in the &quot;Taxa&quot; column LSDPD_G_uniqueOTU &lt;- unique(LSDPD_Gprop_tall$Taxa) # Use sapply() to loop the Mann-Whitney U test across all unique taxa LSDPD_propMW_P &lt;- sapply(LSDPD_G_uniqueOTU, function(x) { # Subset data within function DAT &lt;- LSDPD_Gprop_tall[LSDPD_Gprop_tall$Taxa %in% x,] # Run Mann Whitney U on subsetted data MWU &lt;- wilcox.test(prop ~ GroupAbbrev, data=DAT) # Extract P.value and return MWU$p.value }) # Make new data frame containing taxa identifier, p value, and FDR. LSDPD_propMW_P_df &lt;- data.frame(Taxa=names(LSDPD_propMW_P), P=round(LSDPD_propMW_P, 5), FDR=round(p.adjust(LSDPD_propMW_P, &quot;fdr&quot;), 5)) # Join P value and taxa data frames # Taxa table extracted from phyloseq object in coding above LSDPD_propMW_results &lt;- full_join(LSDPD_Gtaxa, LSDPD_propMW_P_df, by=&quot;Taxa&quot;) # Can calculate this from LSDPD_Gprop_tall object using tidyverse # Note the use of the pipe, %&gt;%, the function sequence will terminate after the last pipe. LSDPD_Gprop_MEDIANwide &lt;- LSDPD_Gprop_tall %&gt;% # Define columns to get groups. # In our case, we want medians by Taxa and GroupAbbrev group_by(Taxa, GroupAbbrev) %&gt;% # Summarize defines the functions that we want to group by # Use the column name in which you want the function to run on summarise(MEDIAN = median(prop), IQR = IQR(prop)) %&gt;% # We can continue this pipe sequence # Use gather() to combine MEDIAN and IQR columns into a single column gather(STATS, value, -Taxa, -GroupAbbrev) %&gt;% # Use mutate() to make new columns that combines GroupAbbrev and STATS mutate(GroupStat = paste(GroupAbbrev, STATS, sep=&quot;_&quot;)) %&gt;% # Use dplyr::select() to remove GroupAbbrev and STATS columns dplyr::select(-GroupAbbrev, -STATS) %&gt;% # Use spread() to distribute value by GroupStat spread(GroupStat, value) # Join with LSDPD_MW_results LSDPD_propMW_Final &lt;- full_join(LSDPD_propMW_results, LSDPD_Gprop_MEDIANwide, by=&quot;Taxa&quot;) # Display significant taxa with trimmed columns LSDPD_propMW_Final[LSDPD_propMW_Final$FDR &lt; 0.05, c(&quot;Genus&quot;, &quot;FDR&quot;,&quot;LSD_MEDIAN&quot;,&quot;LSD_IQR&quot;,&quot;PD_MEDIAN&quot;,&quot;PD_IQR&quot;)] Genus FDR LSD_MEDIAN LSD_IQR PD_MEDIAN PD_IQR g__Streptococcus 0.00124 0.0325 0.05100 0.165 0.1670 g__ 0.00750 6.8890 7.60725 11.352 6.1205 g__Aggregatibacter 0.00317 0.0015 0.00350 0.034 0.0535 g__Desulfovibrio 0.01055 0.6730 0.49175 1.637 1.5210 g__Lactobacillus 0.00124 1.8015 0.93000 8.189 7.0875 g__rc4-4 0.00126 0.0235 0.02925 0.122 0.1565 g__ 0.00783 0.1110 0.11075 0.014 0.0420 g__Roseburia 0.00033 0.0000 0.00625 0.168 0.2025 g__ 0.00032 0.0010 0.00425 0.154 0.2555 g__Phascolarctobacterium 0.00043 0.0000 0.00000 0.465 0.6720 g__Oscillospira 0.01401 3.8140 1.09550 5.337 2.6060 g__ 0.00366 21.1155 4.72475 16.309 3.9465 g__ 0.03131 0.0440 0.03250 0.073 0.0495 g__Bifidobacterium 0.01055 0.1135 0.15025 0.008 0.0515 g__ 0.00043 1.4385 1.55475 0.039 0.0595 g__Allobaculum 0.00124 1.4850 2.11250 0.000 0.0000 g__ 0.03042 0.0205 0.02000 0.044 0.0390 g__Dehalobacterium 0.00126 0.0455 0.04350 0.000 0.0035 g__Akkermansia 0.00032 11.0660 9.14775 0.000 0.0905 g__Dorea 0.04405 0.0290 0.02125 0.052 0.0440 Good agreement with Mann-Whitney U between count and proportional data. 19 taxa were significant between both results. Lets try this workflow one more time, but with CLR normalized data, also referred to as ANCOM ( A nalysis of Co mposition of M icrobes). # load libraries if necessary # library(zCompositions) # library(rgr) # Impute zeros with Bayesian-multiplicative treatment LSDPD_Gcounts_BMz &lt;- cmultRepl(LSDPD_Gcounts, method=&quot;CZM&quot;, output=&quot;p-counts&quot;) # Taxa needs be rows and samples need to be columns LSDPD_G_BMz_clr &lt;- clr(LSDPD_Gcounts_BMz, ifwarn=FALSE) # Need to join with meta data, so samples need to be rows. # transpose data and make new ID column with rownames tLSDPD_Gclr &lt;- t(LSDPD_G_BMz_clr) %&gt;% as.data.frame() tLSDPD_Gclr$SampleID &lt;- rownames(tLSDPD_Gclr) # Join count and meta data LSDPD_Gclrmeta &lt;- full_join(LSDPD_Gmeta, tLSDPD_Gclr, by=&quot;SampleID&quot;) # Reshape data to tall format # The &#39;-&#39; prefix denotes columns that should not have data reshaped into the new Taxa column LSDPD_Gclr_tall &lt;- LSDPD_Gclrmeta %&gt;% gather(Taxa, clr, -SampleID, -Rat, -GroupAbbrev, -Collection) # Identify the unique taxa now listed in the &quot;Taxa&quot; column LSDPD_G_uniqueOTU &lt;- unique(LSDPD_Gclr_tall$Taxa) # Use sapply() to loop the Mann-Whitney U test across all unique taxa LSDPD_clrMW_P &lt;- sapply(LSDPD_G_uniqueOTU, function(x) { # Subset data within function DAT &lt;- LSDPD_Gclr_tall[LSDPD_Gclr_tall$Taxa %in% x,] # Run Mann Whitney U on subsetted data MWU &lt;- wilcox.test(clr ~ GroupAbbrev, data=DAT) # Extract P.value and return MWU$p.value }) # Make new data frame containing taxa identifier, p value, and FDR. LSDPD_clrMW_P_DF &lt;- data.frame(Taxa=names(LSDPD_clrMW_P), P=round(LSDPD_clrMW_P, 5), FDR=round(p.adjust(LSDPD_clrMW_P, &quot;fdr&quot;), 5)) # Join P value and taxa data frames # Taxa table extracted from phyloseq object in coding above LSDPD_clrMW_results &lt;- full_join(LSDPD_Gtaxa, LSDPD_clrMW_P_DF, by=&quot;Taxa&quot;) # Can calculate this from LSDPD_Gclr_tall object using tidyverse # Note the use of the pipe, %&gt;%, the function sequence will terminate after the last pipe. LSDPD_Gclr_MEDIANwide &lt;- LSDPD_Gclr_tall %&gt;% # Define columns to get groups. # In our case, we want medians by Taxa and GroupAbbrev group_by(Taxa, GroupAbbrev) %&gt;% # Summarize defines the functions that we want to group by # Use the column name in which you want the function to run on summarise(MEDIAN = median(clr), IQR = IQR(clr)) %&gt;% # We can continue this pipe sequence # Use gather() to combine MEDIAN and IQR columns into a single column gather(STATS, value, -Taxa, -GroupAbbrev) %&gt;% # Use mutate() to make new columns that combines GroupAbbrev and STATS mutate(GroupStat = paste(GroupAbbrev, STATS, sep=&quot;_&quot;)) %&gt;% # Use dplyr::select() to remove GroupAbbrev and STATS columns dplyr::select(-GroupAbbrev, -STATS) %&gt;% # Use spread() to distribute value by GroupStat spread(GroupStat, value) # Join with LSDPD_MW_results LSDPD_clrMW_Final &lt;- full_join(LSDPD_clrMW_results, LSDPD_Gclr_MEDIANwide, by=&quot;Taxa&quot;) # Display significant taxa with trimmed columns LSDPD_clrMW_Final[LSDPD_clrMW_Final$FDR &lt; 0.05, c(&quot;Genus&quot;, &quot;FDR&quot;,&quot;LSD_MEDIAN&quot;,&quot;LSD_IQR&quot;,&quot;PD_MEDIAN&quot;,&quot;PD_IQR&quot;)] Genus FDR LSD_MEDIAN LSD_IQR PD_MEDIAN PD_IQR g__Streptococcus 0.00125 -0.7558356 1.1081013 0.9447631 0.9102334 g__ 0.02517 0.4499693 4.0722199 0.9927526 0.5260083 g__Aggregatibacter 0.00978 -1.1110675 1.9363557 1.8469538 1.9459101 g__Desulfovibrio 0.00978 -0.5589153 0.7567401 0.5487594 0.9361988 g__Lactobacillus 0.00125 -0.7599474 0.4449609 0.8689117 1.2621105 g__rc4-4 0.00313 -0.8836745 1.7650386 1.2304717 1.4045579 g__ 0.00664 1.4055247 1.1887166 -0.5113979 2.3614766 g__Roseburia 0.00025 -2.8509159 2.1183904 2.3144205 1.7659838 g__ 0.00025 -2.5916857 1.2960636 2.0968371 1.4137854 g__Phascolarctobacterium 0.00037 -2.7583940 0.0000000 3.3945608 2.0744459 g__Oscillospira 0.01489 -0.1677343 0.2113098 0.1985330 0.3642676 g__ 0.01489 0.1279404 0.2244325 -0.0473339 0.4109117 g__Bifidobacterium 0.01582 1.4595001 1.4359913 -1.1987422 4.0928271 g__ 0.00025 2.1205849 0.8884355 -1.6452917 1.4540357 g__Allobaculum 0.00125 3.5006451 2.3172681 -3.5496547 0.0000000 g__ 0.01286 -0.4941540 0.7169747 0.3004636 0.7419373 g__Dehalobacterium 0.00125 2.2705771 0.8381018 -1.6704695 1.9721102 g__Akkermansia 0.00025 4.2872392 1.0519912 -5.0103960 3.8115149 g__Dorea 0.03324 -0.2509362 0.6619915 0.2847172 0.6718674 Again, excellent agreement between the raw count data and CLR normalized data when using the Mann-Whitney U test. However, the CLR normalization is difficult to interpret in a table. Remember, each measurement in a CLR normalization is a log ratio between the measurement and the mean taxa. 7.2 DESeq2 DESeq2 is a Bioconductor based R package that was developed for RNA-seq count data analysis that uses a Negative Binomial Wald Test to compare dichtonomous outcomes. This workflow has been championed by the authors of phyloseq and they have demonstrated how to transfer a phyloseq object into a DESeq2 object for differential abundance testing. Unlike the non-parametric tests, DESeq2 can integrate covariates into the fitted model it uses. So that is a big plus, especially for those working with human clinical samples. I will refer you to the DESeq2 references manual for those needing this functionality. As DESeq2 requires dichtonomous outcomes, we will use the LSD vs PD data from above. # load library library(DESeq2) # phyloseq authors made a function that does all of the heavy lifting to get a phyloseq object into a DESeqDataSet object. # Must be in DESeqDataSet for DESeq2 to work # The phyloseq_to_deseq2() requires you to identify the column in the Sample Metadata that is to be compared. LSDPD_G_deseq2 = phyloseq_to_deseq2(phyloG_LSDPD, ~ GroupAbbrev) # Run the Wald test as implemented in DESeq2 LSDPD_G_deseq2Wald = DESeq(LSDPD_G_deseq2, test=&quot;Wald&quot;, fitType=&quot;parametric&quot;) # The results() function creates a DESeq2 table of the results that we can coerce into a data frame LSDPD_G_deseq2Waldres = results(LSDPD_G_deseq2Wald, cooksCutoff = FALSE) %&gt;% as.data.frame() # Make a Taxa column to match the column we made in the taxa table data frame LSDPD_G_deseq2Waldres$Taxa &lt;- rownames(LSDPD_G_deseq2Waldres) # Join DESeq2 results and taxa data frames # Taxa table extracted from phyloseq object in coding above LSDPD_DESeq2_results &lt;- full_join(LSDPD_Gtaxa, LSDPD_G_deseq2Waldres, by=&quot;Taxa&quot;) # View abbreviated table LSDPD_DESeq2_results[LSDPD_DESeq2_results$padj &lt; 0.05,c(&quot;Genus&quot;,&quot;baseMean&quot;,&quot;log2FoldChange&quot;,&quot;padj&quot;)] Genus baseMean log2FoldChange padj g__Streptococcus 41.12756 1.8675796 0.0002830 g__Aggregatibacter 11.34275 4.3260630 0.0000109 g__Desulfovibrio 417.33716 0.8751891 0.0204434 g__Lactobacillus 1672.30019 2.0444127 0.0000011 g__rc4-4 31.01152 2.1681175 0.0004158 g__ 30.20378 -2.7793369 0.0000939 g__Roseburia 31.27082 4.8934834 0.0000000 g__ 50.77997 6.0008067 0.0000000 g__Phascolarctobacterium 89.75957 9.5404025 0.0000000 g__ 6713.40232 -0.7788920 0.0000011 g__Bifidobacterium 32.01014 -2.4745211 0.0034136 g__ 412.12676 -4.1402375 0.0000001 g__Dehalobacterium 10.34479 -2.6138157 0.0187999 g__Akkermansia 2390.90621 -7.7503791 0.0000000 g__Bacteroides 695.71336 -1.4301040 0.0002003 g__Parabacteroides 178.03731 -1.0518262 0.0134140 Again, this is an abbreviated output to maintain the formating in the webbook. The DESeq2 output provides a baseMean and log2FoldChange columns that require a little explaining because they are not as straight-forward as they seem. The results for these columns are actually normalized baseMeans and log2foldchanges, so they will not sync with similar calculations on the raw count. There are instances were the direction of the log2 fold change has been reversed between DESeq2’s log fold change calculations and the log2 fold change using raw count data. Typically this will only occur in low abundant taxa. Overall, 14 of the taxa match between this analysis and the Mann-Whitney U test on raw counts. So, actually fairly close. Surprisingly more conservative. My experience with DESeq2 is that it tends to be much more liberal. 7.3 metagenomeSeq metagenomeSeq is another Bioconductor based R package, but it was developed specifically for 16S rRNA sequencing data. It can also be extended to shotgun metagenomics sequencing as the properties of 16S and shotgun metagonomics are similar. metagenomeSeq uses a cumulative sum scaling approach to normalize its data followed by either a Zero-inflated Log-Normal or Gaussian mixture model. Further details of the statistical approach is detailed here. Like DESeq2, metagenomeSeq allows for the addition of covariates, so it is more amendable to clinical studies as opposed to the non-parametric approach. Still, metagenomeSeq is sensitive to low abundant taxa. In fact, the authors recommend restricting the analysis to those with a minimum number of positive samples (page 15 of metagenomeSeq’s manual). Most metagenomeSeq tutorials cover pairwise comparisons, but comparisons of &gt; 2 groups can be conducted. However, it uses a workflow from another Bioconductor package, limma. I will refer you metagenomeSeq’s manual for further information. Let’s use metagenomeSeq to see which taxa are differential regulated between LSD and PD rats. Like phyloseq and DESeq2, metagenomeSeq requires its own object that packages all of the data together. For this workflow, we need the count, Sample Metadata, and taxonomy data. While phyloseq has a convenience function, phyloseq_to_metagenomeSeq, that will transition a phyloseq object to a metagenomeSeq object, it currently has a bug that will not allow us to use it with our current phyloseq object. Therefore, we will create it manually. # Load library library(metagenomeSeq) # We will start with the data already extracted from the phyloseq object # Need to convert Sample Metadata and Taxonomy data into objects required for metagenomeSeq LSDPD_G_MGSmeta = AnnotatedDataFrame(LSDPD_Gmeta) LSDPD_G_MGStaxa = AnnotatedDataFrame(LSDPD_Gtaxa) # Create metagenomeSeq object with count data, metadata, and taxonomy data LSDPD_G_MGobj = newMRexperiment(LSDPD_Gcounts, phenoData=LSDPD_G_MGSmeta,featureData=LSDPD_G_MGStaxa) ## Zero-inflated Log-Normal mixture model # Normalize data with cumulative sum scaling LSDPD_G_MGobj &lt;- cumNorm(LSDPD_G_MGobj, p = cumNormStatFast(obj=LSDPD_G_MGobj)) ## Default value being used. # Designate model. Only need to set the right side of the tilde (~). # Setting 1 prior to the variables sets the slope. # Needs metadata from metagenomeSeq object. pData() is extractor function for metagenomeSeq object LSDPD_G_MGobj_mod &lt;- model.matrix(~1 + GroupAbbrev, data = pData(LSDPD_G_MGobj)) # Run the zero-inflated log-normal model. LSDPD_G_MGobj_res = fitFeatureModel(LSDPD_G_MGobj, LSDPD_G_MGobj_mod) ## Warning: Partial NA coefficients for 2 probe(s) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred # Extract results LSDPD_G_MGobj_resDF &lt;- MRcoefs(LSDPD_G_MGobj_res, number=nrow(LSDPD_G_MGobj)) # Set column to bind with taxonomy data LSDPD_G_MGobj_resDF$Taxa &lt;- rownames(LSDPD_G_MGobj_resDF) # Coerce Taxa column in taxonomy data to match labels in metagenomeSeq output table. LSDPD_Gtaxa$Taxa &lt;- gsub(&quot;X&quot;, &quot;&quot;, make.names(LSDPD_Gtaxa$Taxa)) # Join taxomony data and metagenomeSeq output LSDPD_G_MGobj_Final &lt;- inner_join(LSDPD_Gtaxa, LSDPD_G_MGobj_resDF, by=&quot;Taxa&quot;) # Remove taxa that did not compute LSDPD_G_MGobj_Final_noNA &lt;- LSDPD_G_MGobj_Final[!(LSDPD_G_MGobj_Final$pvalues %in% NA),] # View abbreviated table LSDPD_G_MGobj_Final_noNA[LSDPD_G_MGobj_Final_noNA$adjPvalues &lt; 0.05,c(&quot;Genus&quot;,&quot;logFC&quot;, &quot;adjPvalues&quot;)] Genus logFC adjPvalues g__Aggregatibacter 2.340147 0.0001672 g__Roseburia 3.354060 0.0000034 g__ 3.984866 0.0000000 g__Phascolarctobacterium 4.147489 0.0000000 g__ -3.600788 0.0101187 g__Turicibacter -1.677393 0.0486975 Much different output. Only 6 taxa were found to be differential altered between LSD and pre-diabetic UCD-T2DM rats. Lactobacillus and Akkermansia are also no longer statistically significant. Let’s look at the raw counts. # Have to identify by unique numerical ID (OTU); Lactobacillus: 1107027 and and Akkermansia: 363731 # Arrange by GroupAbbrev LSDPD_Gcountmeta[,c(&quot;GroupAbbrev&quot;, &quot;1107027&quot;, &quot;363731&quot;)] %&gt;% arrange(GroupAbbrev) ## GroupAbbrev 1107027 363731 ## 1 LSD 691 69 ## 2 LSD 453 2804 ## 3 LSD 628 2281 ## 4 LSD 196 11396 ## 5 LSD 309 5434 ## 6 LSD 487 2252 ## 7 LSD 470 8036 ## 8 LSD 493 2124 ## 9 LSD 180 6054 ## 10 LSD 918 1073 ## 11 LSD 662 7349 ## 12 LSD 636 4486 ## 13 PD 8071 54 ## 14 PD 4818 171 ## 15 PD 317 4 ## 16 PD 3005 1 ## 17 PD 1701 104 ## 18 PD 4146 0 ## 19 PD 430 0 ## 20 PD 4803 0 ## 21 PD 1864 0 ## 22 PD 1495 0 ## 23 PD 3974 107 ## 24 PD 3512 0 ## 25 PD 2498 0 ## 26 PD 813 0 ## 27 PD 883 0 Looks pretty clear that these raw counts differ by group. Let’s see the normalized counts that metagenomeSeq used in it’s model. # Extract counts LSDPD_G_MGnormcounts &lt;- MRcounts(LSDPD_G_MGobj, norm = T) # transpose and coerce into data frame tLSDPD_G_MGnormcounts &lt;- t(LSDPD_G_MGnormcounts) %&gt;% as.data.frame() # Make SampleID column to join with Sample Metadata tLSDPD_G_MGnormcounts$SampleID &lt;- rownames(tLSDPD_G_MGnormcounts) # Join with Sample Metadata tLSDPD_G_MGnormcountsmeta &lt;- inner_join(LSDPD_Gmeta, tLSDPD_G_MGnormcounts, by=&quot;SampleID&quot;) # View with arrangement by GroupAbbrev tLSDPD_G_MGnormcountsmeta[,c(&quot;GroupAbbrev&quot;, &quot;1107027&quot;, &quot;363731&quot;)] %&gt;% arrange(GroupAbbrev) ## GroupAbbrev 1107027 363731 ## 1 LSD 1997.1098 199.421965 ## 2 LSD 955.6962 5915.611814 ## 3 LSD 1897.2810 6891.238671 ## 4 LSD 203.5306 11833.852544 ## 5 LSD 248.7923 4375.201288 ## 6 LSD 571.5962 2643.192488 ## 7 LSD 586.0349 10019.950125 ## 8 LSD 1229.4264 5296.758105 ## 9 LSD 184.6154 6209.230769 ## 10 LSD 2165.0943 2530.660377 ## 11 LSD 1287.9377 14297.665370 ## 12 LSD 1115.7895 7870.175439 ## 13 PD 12670.3297 84.772370 ## 14 PD 3476.1905 123.376623 ## 15 PD 557.1178 7.029877 ## 16 PD 3607.4430 1.200480 ## 17 PD 2308.0054 141.112619 ## 18 PD 5914.4080 0.000000 ## 19 PD 578.7349 0.000000 ## 20 PD 8829.0441 0.000000 ## 21 PD 3153.9763 0.000000 ## 22 PD 5436.3636 0.000000 ## 23 PD 5384.8238 144.986450 ## 24 PD 5337.3860 0.000000 ## 25 PD 3558.4046 0.000000 ## 26 PD 2079.2839 0.000000 ## 27 PD 2574.3440 0.000000 Still visually looks like there could be differences between LSD and PD groups in these two taxa. I do not have an explanation as to why metagenomeSeq did not find Lactobacillus significant. Akkermansia was not computed, likely due to the high abundance of zeros in the PD group. This really highlights why there is still so much work being done in this area, i.e., differential abundance analysis. 7.4 ALDEx2 ALDEx stands for “A NOVA- L ike D ifferential Ex pression Analysis”. It uses a Dirichlet-multinomial model to infer abundances from counts data and then calculates sample variation using standard statistical tools. Unlike most of the methods we have looked at, ALDEx2 takes into consideration the compositional nature of the microbial sequencing count data. My understanding of ALDEx2 is that it handles the zero counts in a unique way. Essentially it estimates many possible imputations for zeros and then tests each of these permutations for significance, and then takes the average value for each significance test as the final determination. So, very similar to the CLR-Mann Whitney U example, but instead of doing this a single time, it runs the test many times with different combinations of the zero-imputations. As the CLR transformation is supposed to tranform the data into an Euclidean space, more traditional statistical approaches, e.g., t-test, Mann Whitney U test, Kruskal Wallis test, and general linear models can be conducted with this approach. Therefore, covariates can be included as well. Let’s look at ALDEx2 approach in our data. # Load library library(ALDEx2) # Calculate Monte Carlo samples of the Dirichlet distribution for each sample. # Each calculation uses CLR transformation LSDPD_GaldexMCcalc &lt;- aldex.clr(LSDPD_Gcounts, LSDPD_Gmeta$GroupAbbrev, mc.samples=130, denom=&quot;all&quot;, verbose=FALSE) # Assess t-test among all Monte Carlo samples and then make Taxa column to merge with Taxonomy table LSDPD_Galdex_ttest &lt;- aldex.ttest(LSDPD_GaldexMCcalc, LSDPD_Gmeta$GroupAbbrev, paired.test=FALSE) LSDPD_Galdex_ttest$Taxa &lt;- rownames(LSDPD_Galdex_ttest) # Merge with taxonomy table LSDPD_Galdex_Final &lt;- full_join(LSDPD_Gtaxa, LSDPD_Galdex_ttest, by=&quot;Taxa&quot;) # Assess effect sizes for all Monte Carlo samples and then make Taxa column to merge with Taxonomy table LSDPD_Galdex_esize &lt;- aldex.effect(LSDPD_GaldexMCcalc, LSDPD_Gmeta$GroupAbbrev, verbose=FALSE) LSDPD_Galdex_esize$Taxa &lt;- rownames(LSDPD_Galdex_esize) # Merge with LSDPD_Galdex_Final object LSDPD_Galdex_Final &lt;- full_join(LSDPD_Galdex_Final, LSDPD_Galdex_esize, by=&quot;Taxa&quot;) # View abbreviated table. &quot;BH&quot; refers to Benjamini and Hochberg FDR correction. LSDPD_Galdex_Final[LSDPD_Galdex_Final$we.eBH &lt; 0.05,c(&quot;Genus&quot;, &quot;we.eBH&quot;, &quot;effect&quot;)] Genus we.eBH effect g__Streptococcus 0.0294584 0.8288661 g__ 0.0354442 0.6195333 g__Aggregatibacter 0.0342545 0.8019802 g__Lactobacillus 0.0039331 0.9621998 g__rc4-4 0.0164894 0.8242482 g__ 0.0066873 -0.9180533 g__Roseburia 0.0006821 1.5346999 g__ 0.0001580 1.8077430 g__Phascolarctobacterium 0.0023025 1.5754577 g__ 0.0187273 -0.8020735 g__Bifidobacterium 0.0119089 -0.7928918 g__ 0.0000111 -2.1068590 g__Allobaculum 0.0000717 -1.5501139 g__Dehalobacterium 0.0014370 -1.1355466 g__Akkermansia 0.0000032 -2.3143187 Overall, 15 taxa were significant in the ALDEx2 workflow and Mann-Whitney U test on non-normalized data. So, still consistent results from all tests except metagenomeSeq (although the authors of ALDEx2 would probably suggest that the compositional nature of the data make all other approaches inadmissible). The authors of ALDEx2 suggest that differences with an absolute effect size &gt; 1 have stronger evidence for true differences. Based on this criteria, only 7 genera are significant. Lactobacillus just misses the cutoff while Akkermansia is highly significant. 7.5 Conclusion As you can see, there are many ways to determine differential abundances. There are several other approaches that we did not cover, e.g., LEfSe, Balances, etc. As I stated in the intro to this chapter, this field is still under constant evolution and results between each method may differ based on the underlying statistical principles, normalizations, and multiple comparison corrections. Our example showed fairly consistent results among most approaches; however, this is not always the case. "]
]
